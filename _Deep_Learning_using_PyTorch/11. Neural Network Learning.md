<details>
<summary><h1>1. Gradient Decent</h1></summary>

## 1.1 Gradient Decent's direction and value

Assume loss function is L(w1,w2,b), use this L and derive with w1, w2, b seperately. Then the gradient expression is [∂L/∂w1, ∂L/∂w2, ∂L/∂b]T

## 1.2 Move the point (iterate one time)

w(t+1)=w(t)-η*∂L/∂w
η is steps or learning rate

following codes are 3-D loss function
```python
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from mpl_toolkits import mplot3d
import numpy as np

w1=np.arange(-10,10,0.05)
w2=np.arange(-10,10,0.05)
w1, w2=np.meshgrid(w1,w2)
lossfn=(2-w1-w2)**2+(4-3*w1-w2)**2

def plot_3D(elev=45, azim=50, X=w1, y=w2):
  fig,ax=plt.subplots(1,1,constrained_layout=True, figsize(8,8))
  ax=plt.subplot(projection="3d")
  ax.plot_surface(w1,w2,lossfn,cmap='rainbow',alpha=0.7)
  ax.view_init(elev=elev, azim=azim)
  ax.set_xlabel("w1", fontsize=20)
  ax.set_ylabel("w2", fontsize=20)
  ax.set_zlabel("lossfn", fontsize=20)

from ipywidgets import interact, fixed
interact(plot_3D, elev=[0,15,30], azip=(-180,180), X=fixed(w1),y=fixed(w2))
plt.show()
```

</details>



<details>
<summary><h1>2. Step and Direction: back propagation</h1></summary>

3 classifications, 500 data samples, 20 features, 3 layers, 1st layer 13 neurons, 2nd layer 8 neurons
1st layer's activation function is relu, 2nd layer's activation function is sigmoid
```python
import torch
import torch.nn as nn
from torch.nn import functional as F

torch.manual_seed(420)
X=torch.rand((500,20), dtype=torch.float32)
y=torch.randint(low=0,high=3,size=(500,),dtype=torch.float32)

input_=X.shape[1]
output_=len(y.unique())

class Module(nn.Module):
  def __init__(self, in_features=40, out_features=2):
    super(Module,self).__init__()
    self.linear1=nn.Linear(in_features,13,bias=False)
    self.linear2=nn.Linear(13,8,bias=False)
    self.output=nn.Linear(8,out_features, bias=True)

  def forward(self,x):
    sigma1=torch.relu(self.linear1(x))
    sigma2=torch.sigmoid(self.linear2(sigma1))
    zhat=self.output(sigma2)
    return zhat

torch.manual_seed(420)
net=Model(in_features=input_, out_features=output_)
zhat=net.forward(x)

criterion=nn.CrossEntropyLoss()
loss=criterion(zhat, y.long())
loss

net.linear1.weight.grad #not back-propagation yet, therefore no gradients yet
loss.backward(retain_graph=True)
net.linear1.weight.grad # output gradients

```

</details>



<details>
<summary><h1>3. torch.optim</h1></summary>

continue the above codes, w(t+1)=w(t)-step*grad
```python
lr=10
w=net.linear1.weight.data
dw=net.linear1.weight.grad

v=torch.zeros(dw.shape[0], dw.shape[1])
v=gamma*v - lr*dw
w=w+v
```
use torch existing libraries to implement
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import functional as F

torch.manual_seed(420)
X=torch.rand((500,20), dtype=torch.float32)
y=torch.randint(low=0,high=3,size=(500,),dtype=torch.float32)

lr=0.1
gamma=0.9

input_=X.shape[1]
output_=len(y.unique())

class Module(nn.Module):
  def __init__(self, in_features=40, out_features=2):
    super(Module,self).__init__()
    self.linear1=nn.Linear(in_features,13,bias=False)
    self.linear2=nn.Linear(13,8,bias=False)
    self.output=nn.Linear(8,out_features, bias=True)

  def forward(self,x):
    sigma1=torch.relu(self.linear1(x))
    sigma2=torch.sigmoid(self.linear2(sigma1))
    zhat=self.output(sigma2)
    return zhat

torch.manual_seed(420)
net=Model(in_features=input_, out_features=output_)
zhat=net.forward(x)

criterion=nn.CrossEntropyLoss()
opt=optim.SGD(net.parameters(),
              lr=lr,
              momentum=gamma
            )

#forward propagation
#calculate loss function
#back propagation, calculate gradient
#update weights and momentum
#clear gradient

zhat=net.forward(x)
loss=criterion(zhat, y.reshape(500).long())
loss.backward()
opt.step() #move one step, update w, update momentum v
opt.zero_grad()


```
> [!IMPORTANT]
> Based on the above codes, we can conclude that we need to determine following steps to implement gradient decent with PyTorch modules:
> 1. Import library
> 2. Determin data samples, hyperparameters (lr, gamma)
> 3. Determine neural network architecture Model and its parameters
> 4. Determine neural network class, forward propagation
> 5. Determine loss function, calculate loss function
> 6. Backpropagation, calculate gradient
> 7. Determin optimization algorithms, and update weights and momentum
> 8. Clear gradient

</details>



<details>
<summary><h1>4. batch_size and epochs</h1></summary>

SGD: use one sample
mini-batch SGD: use mini batch
GD: use all data

batch_size and epoches:  assume there are totally m data samples, batch_size is N, therefore the iteration number of one epoch is m/N.
In machine learning, an epoch is a complete pass of the training data through the algorithm.
Let's explain Epoch with an example. Consider a dataset that has 200 samples. These samples take 1000 epochs or 1000 turns for the dataset to pass through the model. It has a batch size of 5. This means that the model weights are updated when each of the 40 batches containing five samples passes through. Hence the model will be updated 40 times.

One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches. For example, as above, an epoch that has one batch is called the batch gradient descent learning algorithm.

You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within this for-loop is another nested for-loop that iterates over each batch of samples, where one batch has the specified “batch size” number of samples.

If we have epoch=60, that means NN will learn the complete data sample for 60 times.

for epoch in number_of_epochs:
  for batch in number_of_batches:

Assume 1000 samples, batchsize=10, then in order to complete one training, we need 100 iterations and 1 epoch, and number of batch=1000/10=100.

for epoch in 1:
  for batch in 1000(total sample)/10(batchsize):

> [!IMPORTANT]
> for epoch in 1:
>   for iteration in number_of_batches:

## 4.3 TensorDataset and DataLoader

TensorDataset is used to pack features and labels into one object

DataLoader is used to split small batch

```python
import torch
from torch.utils.data import TensorDataset
from torch.tuils.data import DataLoader

a=torch.randn(500,2,3) #3d   #1st dimention must be the same
b=torch.randn(500,3,4,5) #4d
b=torch.randn(500,1) #2d

data=TensorDataset(b,c)
for x in DataLoader(data):
  print(x)


bs=120
dataset=DataLoader(data
          ,batch_size=bs
          ,shuffle=True
          ,drop_last=False
        )

for i in dataset:
  print(i)

```

</details>



<details>
<summary><h1>5. Implement NN with Fashion-MINST </h1></summary>



</details>
