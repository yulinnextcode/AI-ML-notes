<details>
<summary><h1>1. Gradient Decent</h1></summary>

## 1.1 Gradient Decent's direction and value

Assume loss function is L(w1,w2,b), use this L and derive with w1, w2, b seperately. Then the gradient expression is [∂L/∂w1, ∂L/∂w2, ∂L/∂b]T

## 1.2 Move the point (iterate one time)

w(t+1)=w(t)-η*∂L/∂w
η is steps or learning rate

following codes are 3-D loss function
```python
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from mpl_toolkits import mplot3d
import numpy as np

w1=np.arange(-10,10,0.05)
w2=np.arange(-10,10,0.05)
w1, w2=np.meshgrid(w1,w2)
lossfn=(2-w1-w2)**2+(4-3*w1-w2)**2

def plot_3D(elev=45, azim=50, X=w1, y=w2):
  fig,ax=plt.subplots(1,1,constrained_layout=True, figsize(8,8))
  ax=plt.subplot(projection="3d")
  ax.plot_surface(w1,w2,lossfn,cmap='rainbow',alpha=0.7)
  ax.view_init(elev=elev, azim=azim)
  ax.set_xlabel("w1", fontsize=20)
  ax.set_ylabel("w2", fontsize=20)
  ax.set_zlabel("lossfn", fontsize=20)

from ipywidgets import interact, fixed
interact(plot_3D, elev=[0,15,30], azip=(-180,180), X=fixed(w1),y=fixed(w2))
plt.show()
```


</details>



<details>
<summary><h1>2. Step and Direction: back propagation</h1></summary>

For **binary classification single layer** neural network, the calculation diagram is as follows

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Binary_classification_single_layer_neural_network.png)

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Loss_function_derive_with_w.png)

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Single_layer_derivation.png)

For **binary classification two layers** neural network, the calculation diagram is as follows

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Binary_classification_two_layers_neural_network.png)

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Two_layers_derivation.png)

Before chain derivation, it is very hard to calculate the derivation...

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Two_layers_derivation_1.png)

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Two_layers_derivation_2.png)

After the chain derivation, the backpropagation can be shown as follows:

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Two_layers_derivation_1_back.png)

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Two_layers_derivation_2_back.png)

What does the following code mean?
```python
import torch

x=torch.tensor(1., requires_grad=True)   #requires_grad means allow calculate derivation with X
y=x**2

torch.autograd.grad(y,x)   # means for y=x**2, the gradient result of x=1
```

```python
import torch

x=torch.tensor(1,requires_grad=True, dtype=torch.float32)   # define x value

z=x**2  # define a loss function

y=torch.tensor(2, requires_grad=True, dtype=torch.float32)   # define y value

sigma=torch.sigmoid(z)

loss=-(y*torch.log(sigma)+(1-y)*log(1-sigma))

torch.autograd.grad(loss,x)

torch.autograd.grad(loss,y)

```

REQUIREMENTS: 3 classifications, 500 data samples, 20 features, 3 layers, 1st layer 13 neurons, 2nd layer 8 neurons
1st layer's activation function is relu, 2nd layer's activation function is sigmoid
```python
import torch
import torch.nn as nn
from torch.nn import functional as F

torch.manual_seed(420)
X=torch.rand((500,20), dtype=torch.float32)    # since we are not going to calculate gradient with x and y, therefore we don't need set requires_grad for x and y, to improve efficiency
y=torch.randint(low=0,high=3,size=(500,),dtype=torch.float32)    # since we are not going to calculate gradient with x and y, therefore we don't need set requires_grad for x and y, to improve efficiency

input_=X.shape[1]
output_=len(y.unique())

class Module(nn.Module):
  def __init__(self, in_features=40, out_features=2):
    super(Module,self).__init__()
    self.linear1=nn.Linear(in_features,13,bias=False)
    self.linear2=nn.Linear(13,8,bias=False)
    self.output=nn.Linear(8,out_features, bias=True)

  def forward(self,x):
    sigma1=torch.relu(self.linear1(x))
    sigma2=torch.sigmoid(self.linear2(sigma1))
    zhat=self.output(sigma2)
    return zhat

torch.manual_seed(420)
net=Model(in_features=input_, out_features=output_)
zhat=net.forward(x)

criterion=nn.CrossEntropyLoss()
loss=criterion(zhat, y.long())
loss

net.linear1.weight.grad   # not back-propagation yet, therefore no gradients yet
loss.backward(retain_graph=True)    # run back-propagation
net.linear1.weight.grad   # output gradients

```
> For three classification problems, loss function use logsoftmax + NLLLoss or CrossEntropyLoss
> 
> For binary classification problems, loss function use BCE or BCEWithLogitsLoss


> v(t)=gamma*v(t-1)-lr*dw
> 
> w(t+1)=w(t)+v(t)

</details>

<details>
<summary><h1>3. torch.optim</h1></summary>

continue the above codes, w(t+1)=w(t)-step*grad
```python
lr=10
w=net.linear1.weight.data
dw=net.linear1.weight.grad

v=torch.zeros(dw.shape[0], dw.shape[1])
v=gamma*v - lr*dw
w=w+v
```
use torch existing libraries to implement
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import functional as F

torch.manual_seed(420)
X=torch.rand((500,20), dtype=torch.float32)
y=torch.randint(low=0,high=3,size=(500,),dtype=torch.float32)

lr=0.1
gamma=0.9

input_=X.shape[1]
output_=len(y.unique())

class Module(nn.Module):
  def __init__(self, in_features=40, out_features=2):
    super(Module,self).__init__()
    self.linear1=nn.Linear(in_features,13,bias=False)
    self.linear2=nn.Linear(13,8,bias=False)
    self.output=nn.Linear(8,out_features, bias=True)

  def forward(self,x):
    sigma1=torch.relu(self.linear1(x))
    sigma2=torch.sigmoid(self.linear2(sigma1))
    zhat=self.output(sigma2)
    return zhat

torch.manual_seed(420)
net=Model(in_features=input_, out_features=output_)
zhat=net.forward(x)

criterion=nn.CrossEntropyLoss()
opt=optim.SGD(net.parameters(),
              lr=lr,
              momentum=gamma
            )

#forward propagation
#calculate loss function
#back propagation, calculate gradient
#update weights and momentum
#clear gradient

zhat=net.forward(x)
loss=criterion(zhat, y.reshape(500).long())
loss.backward()
opt.step() #move one step, update w, update momentum v
opt.zero_grad()


```
> Based on the above codes, we can conclude that we need to determine following steps to implement gradient decent with PyTorch modules:
> 1. Import library
> 2. Determin data samples, hyperparameters (lr, gamma)
> 3. Determine neural network architecture Model and its parameters
> 4. Determine neural network class, forward propagation
> 5. Determine loss function, calculate loss function
> 6. Backpropagation, calculate gradient
> 7. Determin optimization algorithms, and update weights and momentum
> 8. Clear gradient


> How to move one step?
> 1. Move forward
> 2. Calculate current iteration's loss function
> 3. Move back-propagation
> 4. Update weights and moments
> 5. Clear gradient. (Clear all gradients' results based on previous point)

</details>



<details>
<summary><h1>4. batch_size and epochs</h1></summary>

> SGD: use one sample. Stochastic Gradient Descent (SGD) is a simplification of Gradient Descent (GD) that reduces computational time by computing the gradient on a randomly chosen partition of the dataset instead of the whole dataset. SGD converges more quickly and requires less memory to store the cost function gradients, but the updates it produces can be characterized as noisy. 

> 
> mini-batch SGD: use mini batch. Mini-batch Gradient Descent (MBGD) is a cross between GD and SGD that splits the dataset into small subsets (batches) and computes the gradients for each batch. MBGD converges faster and produces more precise results than SGD, and provides more stable updates compared to SGD. MBGD is often the preferred choice as it brings together the strengths of both BGD and SGD.
> 
> GD: use all data

batch_size and epoches:  assume there are totally m data samples, batch_size is N, therefore the iteration number of one epoch is m/N.
In machine learning, an epoch is a complete pass of the training data through the algorithm.
Let's explain Epoch with an example. Consider a dataset that has 200 samples. These samples take 1000 epochs or 1000 turns for the dataset to pass through the model. It has a batch size of 5. This means that the model weights are updated when each of the 40 batches containing five samples passes through. Hence the model will be updated 40 times.

One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches. For example, as above, an epoch that has one batch is called the batch gradient descent learning algorithm.

You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within this for-loop is another nested for-loop that iterates over each batch of samples, where one batch has the specified “batch size” number of samples.

If we have epoch=60, that means NN will learn the complete data sample for 60 times.

for epoch in number_of_epochs:
  for batch in number_of_batches:

Assume 1000 samples, batchsize=10, then in order to complete one training, we need 100 iterations and 1 epoch, and number of batch=1000/10=100.

for epoch in 1:
  for batch in 1000(total sample)/10(batchsize):

> [!IMPORTANT]
> for epoch in 1:
>   for iteration in number_of_batches:

## 4.3 TensorDataset and DataLoader

TensorDataset is used to pack features and labels into one object

DataLoader is used to split small batch

```python
import torch
from torch.utils.data import TensorDataset
from torch.tuils.data import DataLoader

a=torch.randn(500,2,3) #3d   #1st dimention must be the same
b=torch.randn(500,3,4,5) #4d
b=torch.randn(500,1) #2d
# As long as the 1st dimensions are the same, then they can be combined

data=TensorDataset(b,c)
for x in DataLoader(data):
  print(x)


bs=120
dataset=DataLoader(data
          ,batch_size=bs
          ,shuffle=True   #Before split into small batch, please randomnize dataset
          ,drop_last=False  #whether or not to discard the last batch
        )

for i in dataset:
  print(i)

```

How to use dataset and tensordataloader for reading sklearn data
```python
from sklearn.datasets import load_breast_cancer as LBC

data=LBC()

X=torch.tensor(data.data, dtype=torch.float32)
y=torch.tensor(data.target, dtype=torch.float32)

data=TensorDataset(X,y)
batchdata=DataLoader(data, batch_size=5, shuffle=True)

for x,y in batchdata:
  print(x)

```
How to use dataset and tensordataloader for reading csv data
```python
import pandas as pd
import numpy as np

data=pd.read_csv(r"C:\test.csv")

X=torch.tensor(np.array(data.iloc[:,:-1]), dtype=torch.float32)
y=torch.tensor(np.array(data.iloc[:,-1]), dtype=torch.float32)

data=TensorDataset(X,y)
batchdata=DataLoader(data, batch_size=1000, shuffle=True)

for x,y in batchdata:
  print(x)
```

>for x,y in batchdata:
>
>x is used for forward propagation, y is used for loss function calculation 
</details>



<details>
<summary><h1>5. Implement NN with Fashion-MINST </h1></summary>

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/torchvision_torchtext_torchaudio.png)

```python
import torchvision
import torchvision.transforms as transforms  # module to process data

mnist=torchvision.datasets.FashionMNIST(root=""
                        ,download=True
                        ,train=True   # data is for training or not?
                        ,transform=transforms.ToTensor()
                        )
mnist.data.shape     #torch.size([60000,28,28])  (sample_size, H-height, W-width, C-color)
mnist.targets.unique()

import matplotlib.pyplot as plt
plt.imshow(mnist[0][0].view(28,28).numpy());

# Modeling procedure includes two parts: external (build NN and loss function) and internal (gradient descent)
# Step 1: define hyper-parameters: step lr, moment value gamma, epochs, batch_size
# Step 2: Import data and split into batches
# Step 3: Define neural network architecture
# Step 4: Define loss function
# Step 5: Define optimization method
# Step 6: Execute optimization method on epoches and batch:
#       6.1: Forward propagation, calculate loss function
#       6.2: Backward propagation, calculate loss function's derivaties with each w
#       6.3: Update current weights
#       6.4: Clear current iteration's gradients
#       6.5: Monitor model's process and results, loss and accuracy
# Step 7: Output results

```
> [!IMPORTANT]
> Modeling procedure includes two parts: external (build NN and loss function) and internal (gradient descent)
> Step 1: define hyper-parameters: step lr, moment value gamma, epochs, batch_size
> Step 2: Import data and split into batches
> Step 3: Define neural network architecture
> Step 4: Define loss function
> Step 5: Define optimization method
> Step 6: Execute optimization method on epoches and batch:
>       6.1: Forward propagation, calculate loss function
>       6.2: Backward propagation, calculate loss function's derivaties with each w
>       6.3: Update current weights
>       6.4: Clear current iteration's gradients
>       6.5: Monitor model's process and results, loss and accuracy
> Step 7: Output results


```python
import torch
from torch import nn
from torch import optim
from torch.nn import functional as F
from torch.utils.data import DataLoader, TensorDataset
import torchvision
import torchvision.transforms as transforms

lr=0.15
gamma=0.8
epochs=5
bs=128

mnist=torchvision.datasets.FashionMNIST(root=""
                        ,download=True
                        ,train=True   # data is for training or not?
                        ,transform=transforms.ToTensor()
                        )
batchdata = DataLoader(mnist, batch_size=bs, shuffle=True)
for x,y in batchdata:
  print(x.shape) #torch.size([128,1,28,28])
  print(y.shape) #torch.size([128])
  break

input_=mnist.data[0].numel()   #total elements, for input layer's number of neurons (flatten all features, so taht this can be feed to neural network)
output_len(mnist.targets.unique())  # for output layer's number of neurons

#define NN class
class Model(nn.Module):
  def __init__(self, in_features=10, out_features=2):
    super().__init__()
    self.linear1=nn.Linear(in_features, 128, bias=False)
    self.output=nn.Linear(128, out_features, bias=False)

  def forward(self, x):
    x=x.view(-1,28*28)
    sigma1=torch.relu(self.linear1(x))
    sigma2=F.log_softmax(self.output(sigma1),dim=1)
    return sigma2

#define loss function and optimal method
def fit(net, batchdata, lr=0.01, epochs=5, gamma=0):
  creterion=nn.NLLLoss()
  opt=optim.SGD(net.parameters(),lr=lr,momentum=gamma)
  correct=0
  samples=0
  for epoch in range(epochs):
    for batch_idx, (x,y) in enumerate(batchdata):   # get batch index, feature tensor-x, feature tensor's corresponding label-y
      y=y.view(x.shape[0])
      sigma=net.forward()  #move forward propagation
      loss=criterion(sigma,y)
      loss.backward()
      opt.step()
      opt.zero_grad()

      #calculate accuracy
      yhat=torch.max(sigma, 1)[1]   #extract torch.max
      correct += torch.sum(yhat==y)
      samples=samples+x.shape[0]

    if (batch_idx+1)%125==0 or batch_idx==len(batchdata)-1:
      print("Epoch{}:[{}/{}({:.0f}%)], Loss:{:.6f}, Accuracy:{:.3f}".format(epoch+1, samples, epochs*len(batchdata.dataset), 100*samples/(epochs*len(batchdata.dataset)),loss.data.item(),float(100*correct/samples)))


#train the model and evaluate model
torch.manual_seed(1412)
net = Model(in_features=input_, out_features=output_)

fit(net,batchdata,lr=lr,epochs=ephcos, gamma=gamma)

```
> [!TIP]
> view(-1,) # ask pytorch to calculate the dimensions. for example x=torch.randn(30,40) x=x.view(-1,20)  then x shape is x.shape is torch.size([60,20])


> [!TIP]
> list=["A","B","C"]
> for x in enumerate(list):
>     print(x)
> this will output following:
> (0, 'A')
> (1, 'B')
> (2, 'C')



</details>
