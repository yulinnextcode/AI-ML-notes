<details>
<summary><h1>1. AutoGrad and dynamic computational graph</h1></summary>
## 1.1 Differentiable
```python
import numpy as np
import torch

x=torch.tensor(1, requires_grad = True)
y=x**2

y.grad_fn   #store from x to y relationship
y.required_grad   #True, differentiable

## 1.2 Tensor calculation graph

```

</details>


<details>
<summary><h1>2. Back-propagation and Gradient calculation</h1></summary>
## 2.1 Backpropagation procedure
```python
import numpy as np
import torch

x=torch.tensor(1, requires_grad = True)
y=x**2

z=y+1

x.grad   # will output nothing

z.backward()
x.grad  # will output tensor(2.), because z=x**2+1
```
```python
import numpy as np
import torch

x=torch.tensor(1, requires_grad = True)
y=x**2

z=y+1

x.grad   # will output nothing

y.backward()    # back-propagation from middle node
x.grad  # will output tensor(2.), because z=x**2+1
```

> [!IMPORTANT]
> Each calculation graph can only do one back-propagation

## 2.2 back-propagation notes
> [!IMPORTANT]
> The back-propagation results are different between starting from middle node and leaf node
> store middle node gradient as follows

```python
x=torch.tensor(1, requires_grad=True)
y=x**2
y.retain_grad()   # store middle note gradient
z=y**2

z.backward()
y.grad  # tensor(2.)
x.grad  # tensor(4.)
```

## 2.3 stop calculation graph
```python
x=torch.tensor(1, requires_grad=True)
y=x**2

with torch.no_grad():  # will not calculation graph trace
  z=y**2

z.requires_grad  # False
```

or use detach() method

```python
x=torch.tensor(1, requires_grad=True)
y=x**2

y1=y.detach()  # detach() method

y   # tensor(1., grad_fn=)
y1  # tensor(1.)
z   # tensor(1.)
```

## 2.4 recognize leaf note
```python
x.is_leaf   # True
y.is_leaf   # False
```

</details>

<details>
<summary><h1>3. Gradient decent</h1></summary>

## 3.1 Limitation of least squared method

Least squared method requires the feature matrix must be invertible.

</details>

## 3.2 Core of gradient decent method

**Gradient decent method is not a single algorithm, it is an algorithm cluster. Gradient decent method includs gradient decent, stochastic gradient decent, mini-batch gradient decent, ect.**

## 3.3 Gradient decent's direction and step

### 3.3.1 derivatives and gradient

### 3.3.2 gradient and direction
```python
a=torch.tensor(0, requires_grad=True)
b=torch.tensor(0, requires_grad=True)

s0=torch.pow((2+a+b), 2) + torch.pow((4+3*a+b), 2)

s0.backward()
a.grad, b.grad   #(tensor(-28.0), tensor(-12.0)), move following gradient's opposite direction

```

<details>
<summary><h1>4. Gradient decent mathmatical expression</h1></summary>
## 4.1 Gradient decent algebra expression

## 4.2 Gradient decent matrix expression


</details>


<details>
<summary><h1>5. Gradient decent manually implementation</h1></summary>

```python
weights = torch.tensors(2,1,requires_grad=True)

X=torch.tensor([[1,1],[3,1]], requires_grad=True)
y=torch.tensor([2,4], requires_grad=True).reshape(2,1)

eps=torch.tensor(0.01, requires_grad=True)

for k in range(10):
  grad=torch.mm(X.t(), (torch.mm(X, weights)y))/2
  weights=weights+eps*grad

```
</details>
