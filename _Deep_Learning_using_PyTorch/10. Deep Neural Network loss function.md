
<details>
<summary><h1>1. Optimization method</h1></summary>



</details>


<details>
<summary><h1>2. SSE and MSE</h1></summary>

For regression neural network, most commonly used loss function is SSE or MSE

```python
from torch.nn import MSELoss

yhat=torch.randn(size=(50,),dtype=torch.float32)
y=torch.randn(size=(50,),dtype=torch.float32)

criterion=MSELoss(reduction="sum")  # by default, resuction is mean
loss=criterion(yhat,y)

```

</details>


<details>
<summary><h1>3. Binary classification cross entropy loss function</h1></summary>

binary cross entropy loss is derived by maximum likelihood estimation.

![Python_File_Operation](https://miro.medium.com/v2/resize:fit:640/format:webp/1*rdBw0E-My8Gu3f_BOB6GMA.png)

```python
import torch

m = 3000
torch.random.manual.seed(420)
X=torch.rand((m,4),dtype=torch.float32)
w=torch.rand((4,1),dtype=torch.float32)
z=torch.randint(low=0,high=2,size=(m,1),dtype=torch.float32)

zhat=torch.mm(X,w)

sigma=torch.sigmoid(zhat)

sigma.shape

loss=-(y*torch.log(sigma)+(1-y)*torch.log(1-sigma))

total_loss=torch.sum(loss)
avg_loss=total_loss/m
```
> [!IMPORTANT]
> torch.sum() is much faster than sum()

we have existing class to do binary cross entropy loss
- Method 1: nn module's class. class BCEWithLogitsLoss, class BCELoss. BCEWithLogitsLoss requires zhat and real label; BCELoss requires sigman and real label.

```python
import torch.nn as nn

criterion=nn.BCELoss(reduction="mean")
loss=criterion(sigma.y)

criterion2=nn.BCEWithLogitLoss(reduction="sum")
loss2=criterion2(zhat,y)
```

- Method 2: functional library's functions. function F.binary_cross_entropy_with_logits, function F.binary_cross_entropy

</details>


<details>
<summary><h1>4. Multi classification cross entropy loss function</h1></summary>


```python
import torch

m = 3000
torch.random.manual.seed(420)
X=torch.rand((m,4),dtype=torch.float32)
w=torch.rand((4,3),dtype=torch.float32)
z=torch.randint(low=0,high=3,size=(m,),dtype=torch.float32)

zhat=torch.mm(X,w)
logsm=nn.LogSoftmax(dim=1)

logsigma=logsm(zhat)

criterion=nn.NLLLoss()

criterion=(logsigma,y.long())
```

use Cross Entropy Loss Class
```python
import torch

criterion=nn.CrossEntropyLoss(reduction="mean/sum/None")
criterion(zhat,y.long())

criterion=(logsigma,y.long())
```

> [!IMPORTANT]
> Summary:
> Regression use SSE, and use MSELoss
> Binary cross entropy loss use BCELoss and BCEWithLogitsLoss
> Multiple cross entropy loss use nn.LogSoftmax + nn.NLLLoss() to implement cross entropy loss, or use CrossEntropyLoss directly



</details>
















