<details>
<summary><h1>1. Learning rate optimization manual implementation</h1></summary>

如果模型学习率设置太大，虽然前期收敛速度较快，但容易出现收敛过程不稳定、收敛结果不佳、或者神经元活性失效等问题；而如果学习率设置太小，虽然收敛过程将相对平稳，并且能够有效规避神经元活性坏死的问题，但容易出现收敛速度慢、收敛结果不佳等问题。

## 1.1 Learning rates effects on model training

```python
torch.manual_seed(420)

features, labels = tensorGenReg(w=[2,-1,3,1,2], bias=False, deg=2)

train_loader, test_loader = split_loader(features, labels, batch_size=50)

torch.manual_seed(24)

num_epochs=20

tanh_model1=net_class2(act_fun=torch.tanh, in_features=5, BN_model='pre')
tanh_model2=net_class2(act_fun=torch.tanh, in_features=5, BN_model='pre')
tanh_model3=net_class2(act_fun=torch.tanh, in_features=5, BN_model='pre')
tanh_model4=net_class2(act_fun=torch.tanh, in_features=5, BN_model='pre')

train_l1, test_l1 = model_train_test(tanh_model1,
                                     train_loader,
                                     test_loader,
                                     num_epochs=num_epochs,
                                     criterion=nn.MSELoss(),
                                     optimizer=optim.SGD,
                                     lr=0.03,
                                     cla=False,
                                     eva=mse_cal)

train_l2, test_l2 = model_train_test(tanh_model2,
                                     train_loader,
                                     test_loader,
                                     num_epochs=num_epochs,
                                     criterion=nn.MSELoss(),
                                     optimizer=optim.SGD,
                                     lr=0.01,
                                     cla=False,
                                     eva=mse_cal)

train_l3, test_l3 = model_train_test(tanh_model3,
                                     train_loader,
                                     test_loader,
                                     num_epochs=num_epochs,
                                     criterion=nn.MSELoss(),
                                     optimizer=optim.SGD,
                                     lr=0.005,
                                     cla=False,
                                     eva=mse_cal)

train_l4, test_l4 = model_train_test(tanh_model4,
                                     train_loader,
                                     test_loader,
                                     num_epochs=num_epochs,
                                     criterion=nn.MSELoss(),
                                     optimizer=optim.SGD,
                                     lr=0.001,
                                     cla=False,
                                     eva=mse_cal)

plt.plot(list(range(num_epochs)), train_l1, label='train_mse_0.03')
plt.plot(list(range(num_epochs)), train_l1, label='train_mse_0.01')
plt.plot(list(range(num_epochs)), train_l1, label='train_mse_0.005')
plt.plot(list(range(num_epochs)), train_l1, label='train_mse_0.001')
plt.legend(loc=1)
```


## 1.2 Learning rates scheduler and manual implementation

先让学习率取得较大数值，从而能够让模型再最开始能够以较快速度收敛，然后在经过一段时间迭代之后，将学习率调小，从而恩公偶让收敛过程穿过损失函数的隘口，抵达更小的解。

```python
torch.manual_seed(24)

tanh_model = net_class2(act_fun=torch.tanh, in_features=5, BN_model='pre')

train_mse=[]
test_mse=[]

while input("Do you want to continue the iteration? [y/n]") == "y":
  epochs=init(input("Number of epochs:"))
  lr=float(input("Update learning rate: "))
  train_l, test_l = fit_rec(net=tanh_model,
                            criterion=nn.MSELoss(),
                            optimizer=optim.SGD(tanh_model.parameters(), lr=lr),
                            train_data=train_loader,
                            test_data=test_loader,
                            epochs=epochs,
                            cla=False,
                            eva=mse_cal)
  train_mse.extend(train_l)
  test_mse.extend(test_l)

plt.plot(train_mse, label='train_mse')
plt.legend(loc=1)

```
比较通用的学习率调度的策略分为以下五类：
- 幂调度
- 指数调度
- 分段恒定调度
- 性能调度
- 周期调度

</details>


<details>
<summary><h1>2. Implement Learning rate scheduler with PyTorch</h1></summary>

```python
from torch.optim import lr_scheduler
```

## 2.1 optimizor and state_dict
```python
torch.manual_seed(420)

features, labels = tensorGenReg(w=[2,-1,3,1,2], bias=False, deg=2)

train_loader, test_loader = split_loader(features, labels, batch_size=50)

torch.manual_seed(24)

tanh_model1=net_class2(act_fun=torch.tanh, in_features=5, BN_model='pre')

optimizer=torch.optim.SGD(tanh_model1.parameters(), lr=0.05)

optimizer.state_dict()

t1=tanh_model1.state_dict()
t1
# or
torch.save(tanh_model1.state_dict(),'tanh1.pt')

torch.load('tanh1.pt')
tanh_model1.load_state_dict(torch.load('tanh1.pt'))
```
## 2.2 LambdaLR practices

```python
lr_lambda = lambda epoch: 0.5 ** epoch

torch.manual_seed(24)

tanh_model1=net_class2(act_fun=torch.tanh, in_features=5, BN_model='pre')

optimizer=torch.optim.SGD(tanh_model1.parameters(), lr=0.05)

optimizer.state_dict()

scheduler=lr_scheduler.LambdaLR(optimizer, lr_lambda)

optimizer.state_dict()

for X,y in train_loader:
  yhat=tanh_model1.forward(X)
  loss=criterion(yhat,y)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
scheduler.step()
```

## 2.3 LambdaLR learning rate scheduler experiment
```python
def fit_rec_sc(net,
               criterion,
               optimizer,
               train_data,
               test_data,
               scheduler,
               epochs=3,
               cla=False,
               eva=mse_cal)
  train_l=[]
  test_l=[]
  for epoch in range(epochs):
    net.train()
    for X,y in train_data:
      if cla==True:
        y=y.flatten().long()
      yhat=net.forward(X)
      loss=criterion(yhat,y)
      optimizer.zero_gard()
      loss.backward()
      optimizer.step()
    scheduler.step()
    net.eval()
    train_l.append(eva(train_data, net).detach())
    test_l.append(eva(test_data, net).detach())
  return train_l, test_l

lr_lmabda=lambda epoch: 0.95**epoch

torch.manual_seed(24)

tanh_model1=net_class2(act_fun=torch.tanh, in_features=5, BN_model='pre')

optimizer=torch.optim.SGD(tanh_model1.parameters(), lr=0.05)

scheduler=lr_scheduler.LmabdaLR(optimizer, lr_lambda)

train_l, test_l = fit_rec_sc(net=tanh_model1,
                             criterion=nn.MSELoss(),
                             optimizer=optimizer,
                             train_data=train_loader,
                             test_data=test_loader,
                             scheduler=scheduler,
                             epochs=60,
                             cla=Flase,
                             eva=mse_cal)

plt.plot(train_l, label='train_mse')
plt.xlabel('epochs')
plt.ylabel('MSE')
plt.legend(loc=1)

optimizer.state_dict()

```

</details>












