<details>
<summary><h1>1. Setup environment and introduction</h1></summary>

install opencv library

## 1.1 Computer vision and deep computer vision introduction

Deep computer vision is the interaction between computer vision and deep learning.



</details>



<details>
<summary><h1>2. Images basic operation</h1></summary>

install opencv library

## Convolution and CNN

Deep computer vision is the interaction between computer vision and deep learning.

```python
import numpy as np
import matplotlib.pyplot as plt
import cv2

img=cv2.imread('D:/cv/peacock/blue-peacock.jpg')
#OpenCV在读取图像时会默认图像通道顺序是BGR，而不是RGB。所以读之后要转换一下
img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

img.shape

plt.figure(dpi=150) #画布
plt.imshow(img)
plt.axis('off')

img.dtype     #dtype('uint8')
img=img*1.0   #dtype('float')

b=np.array([280, -3, 250])
np.clip(b,0,255)   #array([255,0,250])

# increase brightness
img_=np.clip(img+100/255,0,1)
plt.figure(dpi=100)
plt.imshow(img_)
plt.axis('off')

# decrease brightness
img_=np.clip(img-100/255,0,1)
plt.figure(dpi=100)
plt.imshow(img_)
plt.axis('off')

# 更鲜艳
img_=np.clip(img*2,0,1)
plt.figure(dpi=100)
plt.imshow(img_)
plt.axis('off')

# 更阴暗
img_=np.clip(img*0.5,0,1)
plt.figure(dpi=100)
plt.imshow(img_)
plt.axis('off')
```

</details>



<details>
<summary><h1>3. Convolutional operation and edge detection</h1></summary>
  
```python
import numpy as np
import cv2
from matplotlib import pyplot as plt

img=cv2.imread('D:\cv\edge detection.png')

img=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

plt.figure(dpi=200)
plt.imshow(img, cmap="gray")
plt.axis('off')

laplacian=cv2.Laplacian(img, cv2.CV_64F, ksize=5) #laplacian operator

sobelx=cv2.Sobel(img, cv2.CV_64F,1,0,ksize=5) #sobel operator
sobely=cv2.Sobel(img, cv2.CV_64F,0,1,ksize=5)

plt.figure(dpi=300)
plt.subplot(2,2,1),plt.imshow(img,cmap='gray')
plt.title('Original'),plt.axis('off')
plt.subplot(2,2,2),plt.imshow(laplacian,cmap='gray')
plt.title('Laplician'),plt.axis('off')
plt.subplot(2,2,3),plt.imshow(soblex,cmap='gray')
plt.title(Sobel X),plt.axis('off')
plt.subplot(2,2,4),plt.imshow(sobley,cmap='gray')
plt.title(Sobel Y),plt.axis('off')

```

filter or convolution kernel
receiptive field
feature map


</details>



<details>
<summary><h1>4. Convolutional and deep learning</h1></summary>

卷积与深度学习碰撞所带来的变革是革命性的。通过学习的方式改进卷积核，再通过深层网络不断提纯特征，以及大幅度降低参数量，卷积神经网络的作用已经不言而喻。


</details>

<details>
<summary><h1>5. Implement CNN using PyTorch</h1></summary>

## 5.1 Convolutional kernel, input channel and feature map


![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Convolution_layer.png)

conbolutional layer is under nn.Module

Conv1d is for time series data and Conv2d is for images, Conv3d is for frames.

```python
CLASS torch.nn.Conv2d (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_model='zeros')

```

- kernel_size: size of convontional kernel
- out_channels: 扫描次数
- in_channels: 输入的通道数

在一次扫描中，我们输入了一张拥有三个通道的彩色图像。对于这张图，拥有同样尺寸，但不同具体数值的三个卷积核会分别在三个通道上进行扫描，得出三个相应的“新通道”。且该新通道的尺寸也是一致的。
得出三个新通道之后，我们将对应位置的元素相加，形成一张新图，这就是卷积层输入的三彩色图像的第一个特征图。
需要注意的是，当feature maps被输入到下一个卷积层时，也是被当作通道来处理的。feature map其实也可以时一种通道。当feature map进入到下一个卷积层时，新卷积层上对所有feature map完成之后，也会将它们的扫描结果加和成一个新feature map。

Conclusions
- 卷积层的输入是图像时，一次扫描会扫描所有通道的值再加和成一张特征图。
- 当卷积层的输入是上层的特征图时，特征图会被当做通道对待，一次扫描会扫描所有输入的特征图，加和成新的feature map
- 无论在那一层，生成的feature map数量都等于这一层的扫描次数，也就是等于out_channels的值。下一层卷积的in_channels就等于上一层卷积的out_channels。

```python
import torch
from torch import nn

data=torch.ones(size=(10,3,28,28))   #(samples, c,w,h)

conv1=nn.Conv2d(kernel_size=3,
                in_channels=3,    #输入图像的通道数 或者 上一层传入的特征图的数目
                out_channels=6,   #所有RGB通道会被合并，被扫描6次
)

conv2=nn.Conv2d(kernel_size=3,
                in_channels=6,
                out_channels=4
)

conv1(data).shape

conv2(conv1(data)).shape

```


## 5.2 Feature map size: stride, padding, padding_mode

H_out = H_in -KH + 1
W_out = W_in - KW + 1

stride,卷积操作中的“步长”或者步幅

H_out = (H_in - KH) / S[0] + 1

W_out = (W_in - KW) / S[1] + 1


- stride
- padding
- padding_mode: "zeros", "circular"
  
H_out = (H_in - KH) / S[0] + 1

W_out = (W_in - KW) / S[1] + 1

如果你的图像尺寸比较小，你希望机娘避免未扫描的像素被丢弃，那可以如下设置：
- 卷积核尺寸控制在5X5以下，并且kernel_size > stride
- 令2*padding > stride

**H_out = (H_in + 2P - KH) / S[0] + 1**

**W_out = (W_in + 2P - KW) / S[1] + 1**


```python
#当我们不调整conv2d中的参数时，p默认为0，S默认为1

data = torch.ones(size=(10,3,28,28))

conv1 = nn.Conv2d(3,6,3) # in_channels, out_channels, kernel_size
conv2 = nn.Conv2d(6,10,3)
conv3 = nn.Conv2d(10,16,5,stride=2, padding=1)
conv4 = nn.Conv2d(16,3,5,stride=3, padding=2)

# conv1 (28+0-3)/1+1=26 (10,6,26,26)
# conv2 (26+0-3)/1+1=24 (10,10,24,24)
# conv3 (24+2-5)/2+1=11.5 (10,16,11,11)
# conv4 (11+4-5)/3+1=4.33 (10,3,4,4)

```

</details>







