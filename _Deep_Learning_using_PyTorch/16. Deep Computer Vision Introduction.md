<details>
<summary><h1>1. Setup environment and introduction</h1></summary>

install opencv library

## 1.1 Computer vision and deep computer vision introduction

Deep computer vision is the interaction between computer vision and deep learning.



</details>



<details>
<summary><h1>2. Images basic operation</h1></summary>

install opencv library

## Convolution and CNN

Deep computer vision is the interaction between computer vision and deep learning.

```python
import numpy as np
import matplotlib.pyplot as plt
import cv2

img=cv2.imread('D:/cv/peacock/blue-peacock.jpg')
#OpenCV在读取图像时会默认图像通道顺序是BGR，而不是RGB。所以读之后要转换一下
img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

img.shape

plt.figure(dpi=150) #画布
plt.imshow(img)
plt.axis('off')

img.dtype     #dtype('uint8')
img=img*1.0   #dtype('float')

b=np.array([280, -3, 250])
np.clip(b,0,255)   #array([255,0,250])

# increase brightness
img_=np.clip(img+100/255,0,1)
plt.figure(dpi=100)
plt.imshow(img_)
plt.axis('off')

# decrease brightness
img_=np.clip(img-100/255,0,1)
plt.figure(dpi=100)
plt.imshow(img_)
plt.axis('off')

# 更鲜艳
img_=np.clip(img*2,0,1)
plt.figure(dpi=100)
plt.imshow(img_)
plt.axis('off')

# 更阴暗
img_=np.clip(img*0.5,0,1)
plt.figure(dpi=100)
plt.imshow(img_)
plt.axis('off')
```

</details>



<details>
<summary><h1>3. Convolutional operation and edge detection</h1></summary>
  
```python
import numpy as np
import cv2
from matplotlib import pyplot as plt

img=cv2.imread('D:\cv\edge detection.png')

img=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

plt.figure(dpi=200)
plt.imshow(img, cmap="gray")
plt.axis('off')

laplacian=cv2.Laplacian(img, cv2.CV_64F, ksize=5) #laplacian operator

sobelx=cv2.Sobel(img, cv2.CV_64F,1,0,ksize=5) #sobel operator
sobely=cv2.Sobel(img, cv2.CV_64F,0,1,ksize=5)

plt.figure(dpi=300)
plt.subplot(2,2,1),plt.imshow(img,cmap='gray')
plt.title('Original'),plt.axis('off')
plt.subplot(2,2,2),plt.imshow(laplacian,cmap='gray')
plt.title('Laplician'),plt.axis('off')
plt.subplot(2,2,3),plt.imshow(soblex,cmap='gray')
plt.title(Sobel X),plt.axis('off')
plt.subplot(2,2,4),plt.imshow(sobley,cmap='gray')
plt.title(Sobel Y),plt.axis('off')

```

filter or convolution kernel
receiptive field
feature map


</details>



<details>
<summary><h1>4. Convolutional and deep learning</h1></summary>

卷积与深度学习碰撞所带来的变革是革命性的。通过学习的方式改进卷积核，再通过深层网络不断提纯特征，以及大幅度降低参数量，卷积神经网络的作用已经不言而喻。


</details>

<details>
<summary><h1>5. Implement CNN using PyTorch</h1></summary>

## 5.1 Convolutional kernel, input channel and feature map


![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/Convolution_layer.png)

conbolutional layer is under nn.Module

Conv1d is for time series data and Conv2d is for images, Conv3d is for frames.

```python
CLASS torch.nn.Conv2d (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_model='zeros')

```

- kernel_size: size of convontional kernel
- out_channels: 扫描次数
- in_channels: 输入的通道数

在一次扫描中，我们输入了一张拥有三个通道的彩色图像。对于这张图，拥有同样尺寸，但不同具体数值的三个卷积核会分别在三个通道上进行扫描，得出三个相应的“新通道”。且该新通道的尺寸也是一致的。
得出三个新通道之后，我们将对应位置的元素相加，形成一张新图，这就是卷积层输入的三彩色图像的第一个特征图。
需要注意的是，当feature maps被输入到下一个卷积层时，也是被当作通道来处理的。feature map其实也可以时一种通道。当feature map进入到下一个卷积层时，新卷积层上对所有feature map完成之后，也会将它们的扫描结果加和成一个新feature map。

Conclusions
- 卷积层的输入是图像时，一次扫描会扫描所有通道的值再加和成一张特征图。
- 当卷积层的输入是上层的特征图时，特征图会被当做通道对待，一次扫描会扫描所有输入的特征图，加和成新的feature map
- 无论在那一层，生成的feature map数量都等于这一层的扫描次数，也就是等于out_channels的值。下一层卷积的in_channels就等于上一层卷积的out_channels。

```python
import torch
from torch import nn

data=torch.ones(size=(10,3,28,28))   #(samples, c,w,h)

conv1=nn.Conv2d(kernel_size=3,
                in_channels=3,    #输入图像的通道数 或者 上一层传入的特征图的数目
                out_channels=6,   #所有RGB通道会被合并，被扫描6次
)

conv2=nn.Conv2d(kernel_size=3,
                in_channels=6,
                out_channels=4
)

conv1(data).shape

conv2(conv1(data)).shape

```


## 5.2 Feature map size: stride, padding, padding_mode

H_out = H_in -KH + 1
W_out = W_in - KW + 1

stride,卷积操作中的“步长”或者步幅

H_out = (H_in - KH) / S[0] + 1

W_out = (W_in - KW) / S[1] + 1


- stride
- padding
- padding_mode: "zeros", "circular"
  
H_out = (H_in - KH) / S[0] + 1

W_out = (W_in - KW) / S[1] + 1

如果你的图像尺寸比较小，你希望机娘避免未扫描的像素被丢弃，那可以如下设置：
- 卷积核尺寸控制在5X5以下，并且kernel_size > stride
- 令2*padding > stride

**H_out = (H_in + 2P - KH) / S[0] + 1**

**W_out = (W_in + 2P - KW) / S[1] + 1**


```python
#当我们不调整conv2d中的参数时，p默认为0，S默认为1

data = torch.ones(size=(10,3,28,28))

conv1 = nn.Conv2d(3,6,3) # in_channels, out_channels, kernel_size
conv2 = nn.Conv2d(6,10,3)
conv3 = nn.Conv2d(10,16,5,stride=2, padding=1)
conv4 = nn.Conv2d(16,3,5,stride=3, padding=2)

# conv1 (28+0-3)/1+1=26 (10,6,26,26)
# conv2 (26+0-3)/1+1=24 (10,10,24,24)
# conv3 (24+2-5)/2+1=11.5 (10,16,11,11)
# conv4 (11+4-5)/3+1=4.33 (10,3,4,4)

```

## 5.3 pooling layer, BN layer and Dropout layer

pooling layer经常在卷积层后面。他只有尺寸没有值。步长默认等于核尺寸。
```python
CLASS torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)
CLASS torch.nn.AdaptiveAvgPool2d(output_size)
```

Dropout2d

BatchNorm2d

```python
conv1 = nn.Conv2d(3,32,5,padding=2)
bn1 =  nn.BatchNorm2d(32)

bn1(conv1(data))

```
</details>


<details>
<summary><h1>6. Reconstruct classical network</h1></summary>

## 6.1 Reconstruct LeNet-5

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/LeNet-5.png)

```python

import torch
from torch import nn
from torch.nn import functional as F

data=torch.ones(size=(10,1,32,32))

class Model(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1=nn.Conv2d(1,6,5)
    self.pool1=nn.AvgPool2d(kernel_size=2,stride=2)
    self.conv2=nn.Conv2d(6,16,5)
    self.pool2=nn.AvgPool2d(kernel_size=2,stride=2)
    self.fc1=nn.Linear(5*5*16,120)
    self.fc2=nn.Linear(120,84)

  def forwards(self,x):
    x=F.tanh(self.conv1(x))
    x=self.pool1(x)
    x=F.tanh(self.conv2(x))
    x=self.pool2(x)
    x=x.view(-1,5*5*16)   # -1是占位符号
    x=F.tanh(self.fc1(x))
    output=F.softmax(self.fc2(x), dim=1)

net=Model()

net(data)

# pip install torchinfo
from torchinfo import summary
net=Model()
summary(net, input_size=(10,1,32,32))



```

## 6.2 Reconstruct AlexNet


ImageNet Large Scale Visual Recognition Challenge

在AlexNet出现之前，最好成绩一直由手工提取特征+支持向量机的算法获得。

![Python_File_Operation](/_Deep_Learning_using_PyTorch/imgs/AlexNet.png)

# AlexNet: (Cov+Pool) + (Cov+Pool) + (Cov*3+Pool) + (Linear*3)

```python

import torch

from torch import nn
from torch.nn import functional as F

data=torch.ones(size=(10,3,227,227))   

class Model(nn.Module):
  def __init__(self):
    super().__init__()

    # 为了处理较大的原始图片，先使用11x11的卷积核和较大的步长哎快速降低特征图的尺寸。同时，使用比较多的通道数，来你不降低尺寸造成的数据损失。
    self.conv1=nn.Conv2d(3, 96, kernel_size=11, stride=4)
    self.pool1=nn.MaxPool2d(kernel_size=3, stride=2)   #overlap pooling

    # 已经将特征图尺寸缩小到27x27，计算量可控，可以开始进行特征提取了。
    # 卷积核、步长恢复到业界常用的大小，进一步扩大通道来提取数据
    self.conv2=nn.Conv2d(96, 256, kernel_size=5, padding=2)
    self.pool2=nn.MaxPool2d(kernel_size=3, stride=2)

    # 疯狂提取特征，连续用多个卷积层
    self.conv3=nn.Conv2d(256, 384, kernel_size=3, padding=1) #kernel 5, padding 2; kernel 3, padding 1 可以维持特征图大小
    self.conv4=nn.Conv2d(384, 384, kernel_size=3, padding=1)
    self.conv5=nn.Conv2d(384, 256, kernel_size=3, padding=1)
    self.pool3=nn.MaxPool2d(kernel_size=3, stride=2)
    
    # 进入全连接层，进行信息汇总
    self.fc1=nn.Linear(6*6*256,4096)
    self.fc2=nn.Linear(4096,4096)
    self.fc3=nn.Linear(4096,1000)

  def forward(self, x):
    x=F.relu(self.conv1(x))
    x=self.pool1(x)

    x=F.relu(self.conv2(x))
    x=self.pool2(x)

    x=F.relu(self.conv3(x))
    x=F.relu(self.conv4(x))
    x=F.relu(self.conv5(x))
    x=self.pool3(x)

    x=x.view(-1, 6*6*256) #将数据拉平

    x=F.dropout(x,p=0.5)
    x=F.relu(F.dropout(self.fc1(x),p=0.5))
    x=F.relu(self.fc2(x))
    out=F.softmax(self.fc3(x), dim=1)

net=Model()

net(data)

from torchinfo import summary

summary(net, input_size=(10,3,227,227))
```

















</details>
