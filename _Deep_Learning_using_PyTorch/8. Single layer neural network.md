
<details>
<summary><h1>1. PyTorch single layer regression network</h1></summary>

Use torch.tensor to implement single layer regression netral network forard propagation
```python
import torch

X=torch.tensor([[1,0,0],[1,1,0],[1,0,1],[1,1,1]],dtype=torch.float32)
w=torch.tensor([-0.2],[0.15],[0.15], dtype=torch.float32)

def LinearR(X,w):
  zhat=torch.mv(X,w)
  return zhat

LinearR(X,w)
```


Use torch.nn.Linear to implement single layer regression netral network forard propagation
```python
import torch

X=torch.tensor([[0,0],[1,0],[0,1],[1,1]],dtype=torch.float32)

torch.random.manual_seed(420)
output=torch.nn.Linear(2,1, bias=True)  # random generate w and b
output.weight
output.bias
zhat=output(X)
```
</details>


<details>
<summary><h1>2. Tensors guide to avoid pitfalls</h1></summary>

> [!NOTE]
> torch.tensor : will infer data type from the data. then decide based on your input data type.
> torch.Tensor : creates a tensor with the default data type, which is float32
> torch.mv(X,w)
> torch.tensor([1,1,1], dtype=torch.float32)

</details>


<details>
<summary><h1>3. PyTorch single layer classification network</h1></summary>

Use tensor to implement single layer classification netral network forard propagation

```python

import torch

X=torch.tensor([[1,0,0],[1,1,0],[1,0,1],[1,1,1]], dtype=torch.flost32)

andgate = torch.tensor([[0],[0],[0],[1]], dtype=torch.float32)
w=torch.tensor([-0.2,0.15,0.15],dtype=torch.float32)

def LogisticR(X,w):
  zhat=torch.mv(X,w)
  sigma=1/(1+torch.exp(-zhat))
  andhat=torch.tensor([int(x) for x in sigma>=0.5], dtype=torch.float32)
  return sigma, andhat

sigma, andhat=LogisticR(X,w)
```

sign function: sign, ReLU, tanh
 


Use torch.nn.Linear to implement single layer classification netral network forard propagation
```python
import torch
from torch.nn import functional as F

X=torch.tensor([[0,0],[1,0],[0,1],[1,1]],dtype=torch.float32)

torch.random.manual_seed(420)
dense=torch.nn.Linear(2,1, bias=True)  # random generate w and b
output.weight
output.bias
zhat=dense(X)
sigma=F.sigmoid(zhat)
y=[int(x) for x in sigma >=0.5]
```

</details>

> [!IMPORTANT]
> We can only use softmax(), sign() and tanh() by torch.sign() and torch.tanh(), torch.softmax(), however, we can use torch.nn.functional.relu() and torch.nn.functional.sigmoid()
> Implement with tensor, we can use feature matrix by X=torch.tensor([[1,0,0],[1,1,0],[1,0,1],[1,1,1]], dtype=torch.flost32), which means we have to consider b term
> Implement with nn.Linear, we can use feature matrix by X=torch.tensor([[0,0],[1,0],[0,1],[1,1]],dtype=torch.float32), which means we dont need consider b term, because that will be considered by dense=torch.nn.Linear(2,1, bias=True)


<details>
<summary><h1>4. Multi-classification neural network</h1></summary>

```python

import torch

s=torch.tensor([[[1,2,3,5],[3,4,4,5],[5,6,4,5]],[[5,6,4,5],[7,8,4,5],[9,1,2,3]],[[5,6,4,5],[7,8,4,5],[9,1,2,3]],dtype=torch.float32)

s.shape  #torch.size([2,3,4])

torch.softmax(s,dim=0) # calculate by the 1st dimention, which has 2 numbers (means sum 2 numbers equals 1)
torch.softmax(s,dim=1) # calculate by the 2nd dimention, which has 3 numbers (means sum 3 numbers equals 1)
torch.softmax(s,dim=2) # calculate by the 3rd dimention, which has 4 numbers (means sum 4 numbers equals 1)
```

</details>


















