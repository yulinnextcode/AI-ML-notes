<details>
<summary><h1>1. Exclusive-OR gate problem</h1></summary>

```python
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5,3))
plt.scatter(X[:,1],X[:,2],c=andgate,cmap="rainbow")
plt.style.use('seaborn-whitegrid')
sns.set_stype("white")

sns.title("AND GATE", fontsize=16)

plt.grid(alpha=.4,axis="y")
plt.gca().spines["top"].set_alpha(.0)
plt.gca().spines["right"].set_alpha(.0)

import numpy as np
x=np.arange(-1,3,0.5)
plt.plot(x,(0.23-0.15*x)/0.15,color="k",linesype="--")
```
implement and gate with tensor
```python
import torch
X=torch.tensor([[1,0,0],[1,1,0],[1,0,1],[1,1,1]],dtype=torch.float32)
andgate=torch.tensor([0,0,0,1],dtype=torch.float32)

def AND(X):
  w=torch.tensor([-0.2,0.15,0.15],dtype=torch.float32)
  zhat=torch.mv(X,w)
  andhat=torch.tensor([int(x) for x in zhat>=0],dtype=torch.float32)
  return andhat

andhat=AND(x)
```
</details>

<details>
<summary><h1>2. Black box: deep neural network is unexplainable</h1></summary>

</details>

<details>
<summary><h1>3. Explore deep neural network</h1></summary>

</details>

<details>
<summary><h1>4. Activation function</h1></summary>

Activation function: identity function, sign, sigmoid, ReLU, tanh, softmax.
Softmax and identity function almost not used in hidden layers; Sign and sigmoid are almost not used in output layers; ReLU and Sigmoid can be used both.

</details>

<details>
<summary><h1>5. Implement deep neural network from scratch</h1></summary>

Assume we have 500 sample data, 20 features, need to calssify into 3 categories. We want to implement a 3-layer neural network: 1st layer 13 neurons, 2nd layer 8 neurons, 3rd layer is output layer. Activation function of 1st layer is relu, 2nd layer is sigmoid.
```python
import torch
import torch.nn.as nn
from torch.nn import functional as F

torch.random.manual_seed(420)
X=torch.rand((500,20), dtype=torch.float32)
y=torch.randint(low=0, high=3, size=(500,1),dtype=torch.float32)

class Model(nn.Module):
  def __init__(self,in_features=10, out_features=2):
    super(Model,self).__init__()    # copy parent's __init__() to child's __init__(), which means inherit nn.Module's entire class
    self.linear1=nn.Linear(in_features,13,bias=True)
    self.linear2=nn.Linear(13,8,bias=True)
    self.output=nn.Linear(8,out_features,bias=True)

  def forward(self,x):
    z1=self.linear1(x)
    sigma1=torch.relu(z1)
    z2=self.linear2(sigma1)
    sigma2=self.sigmoid(z2)
    z3=self.output(sigma2)
    sigma3=F.softmax(z3,dim=1)

input_=X.shape[1]
output_=len(y.unique())

torch.random.manual_seed(420)
net=Model(in_features=input_, out_features=output_)

net.forward(X).shape # [500,3]
net.forward(X)

net.linear1.weight
net.linear1.bias
```


> [!IMPORTANT]
> When we use class Model(nn.Module) to inherint from nn.Module, we can only inherit its methods and attributes outside from __init__.
> 
> If we want to inherit its methods and attributes inside the __init__ function, we need to use super(Model,self).__init__()
> 
> Assume we have codes: class FooChild(FooParent), and super(FooChild, self).__init__(); The meaning is that use super method, find FooChild's parent class, which is FooParent, then copy to self's __init__(), which is FooChild's __init__()


Let's see what is defined in nn.Module's init function:
```python
def __init__(self):
  torch._C_._log_api_usage_once("python.nn_module")

  self.training=True
  self._parameters=OrderedDict()
  self._buffers=OrderedDict()
  self._non_persistent_buffers_set=set()
  self._backward_hooks=OrderedDict()
  self._forward_hooks=OrderedDict()
  self._forward_pre_hooks=OrderedDict()
  self._state_dick_hooks=OrderedDict()
  self._load_state_dict_pre_hooks=OrderedDict()
  self._modules=OrederedDict()
```

```python
net.cuda()
net.cpu()
net.apply() # apply same operations to init functions' all objects
```
</details>


















