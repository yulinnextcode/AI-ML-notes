We have implement linear regression, logistic regression and softmax regression with neural network in previous lessons.

If the model doesn't have activation function, then the model equals to linear regression;
If the model applies sigmoid activation function at the output layer for binary classification, then the model equals to Logistic regression;
For multiple classification, applies softmax activation function at the output layer, then the model equals to Softmax regression;



<details>
<summary><h1>1. Functions to create dateset for deep learning modeling experiments</h1></summary>

```python
import random

import matplotlib as mpl
import matplotlib.pyplot as plt

import numpy as np

import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import Dataset, TensorDataset, DataLoader
```

## 1.1 Manually create datasets for regression

```python
num_inputs = 2
num_examples = 1000

torch.manual_seed(420)

w_true = torch.tensor([2,-1]).reshape(2,1)
b_true = torch.tensor(1.0)

features = torch.randn(num_examples, num_inputs)
labels_true = torch.mm(features, w_true) + b_true
labels = labels_true + torch.randn(size=labels_true.shape)*0.01

plt.subplot(121)
plt.scatter(features[:,0], labels)
plt.subplot(122)
plt.scatter(features[:,1], labels)

torch.manual_seed(420)
labels1 = labels_true + torch.randn(size = labels_true.shape) * 2

# small disturb
plt.subplot(221)
plt.scatter(features[:,0], labels)
plt.subplot(222)
plt.plot(features[:,1], labels, 'ro')

# large disturb
plt.subplot(221)
plt.scatter(features[:,0], labels1)
plt.subplot(222)
plt.plot(features[:,1], labels1, 'yo')

```
> [!TIP]
> plot vs scatter
> For ploting large points, scatter is faster than plot
> For distingushing points, plot is better than scatter

```python
torch.manual_seed(420)

w_true = torch.tensor([2,-1]).reshape(2,1)
b_true = torch.tensor(1.0)

features = torch.randn(num_examples, num_inputs)
labels_true = torch.pow(features, 2) * w_true + b_true    #y=x^2+1
labels = labels_true + torch.randn(size=labels_true.shape)*0.01

plt.scatter(features, labels)
```

## 1.2 Functions to create datasets for regression

```python
def tensorGenReg(num_examples=1000, w=[2,-1,1], bias=True, delta=0.01, deg=1)
  if bias==True:
    num_inputs=len(w)-1
    features_true=torch.randn(num_examples, num_inputs)
    w_true=torch.tensor(w[:-1]).reshape(-1,1).float()
    b_true=torch.tensor(w[-1]).float()
    if num_inputs == 1:
      labels_true = torch.pow(features_true, deg) * w_true + b_true
    else:
      labels_true = torch.mm(torch.pow(features_true, deg), w_true) + b_true
    features = torch.cat((features_true, torch.ones(len(features_true), 1)), 1)
    labels = labels_true + torch.randn(size = labels_true.shape) * delta
  else:
    num_inputs = len(w)
    features = torch.randn(num_examples, num_inputs)
    w_true = torch.tensor(w).reshape(-1,1).float()
    if num_inputs == 1:
      labels_true = torch.pow(features_true, deg) * w_true + b_true
    else:
      labels_true = torch.mm(torch.pow(features_true, deg), w_true) + b_true
    labels = labels_true + torch.randn(size = labels_true.shape) * delta
  return features, labels
```
> [!TIP]
> Above function can not create cross product items
```python
torch.manual_seed(420)

f, l = tensorGenReg(delta=1)

plt.subplot(221)
plt.scatter(f[:,0], 1)
plt.subplot(222)
plt.scatter(f[:,1], 1)
```
```python
torch.manual_seed(420)

f, l = tensorGenReg(delta=2)

plt.subplot(221)
plt.scatter(f[:,0], 1)
plt.subplot(222)
plt.scatter(f[:,1], 1)
```
2nd order relationship
```python
torch.manual_seed(420)

f, l = tensorGenReg(deg=2)

plt.subplot(221)
plt.scatter(f[:,0], 1)
plt.subplot(222)
plt.scatter(f[:,1], 1)
```
## 1.3 Manually create datasets for classification
> [!TIP]
> 
> torch.randn(4,2) returns standard normalization distribution
> 
> torch.normal(4,2,size=(10,2)) returns normalization distribution, which average is 4 and standard variance is 2
```python

torch.manual_seed(420)

data0=torch.normal(4,2,size=(num_examples, num_inputs))
data1=torch.normal(-2,2,size=(num_examples, num_inputs))
data2=torch.normal(-6,2,size=(num_examples, num_inputs))

label0=torch.zeros(500)
label1=torch.ones(500)
label2=torch.full_like(label1,2)

features=torch.cat((data0, data1, data2)).float()
labels=torch.cat((label0, label1, label2)).long().reshape(-1,1)

plt.scatter(features[:,0], features[:,1], c=labels)

```


## 1.4 Functions to create datasets for classification
```python
def tensorGenCla(num_examples=500, num_inputs=2, num_class=3, deg_dispersion=[4,2], bias=False):
  cluster_1 = torch.empty(num_examples, 1)
  mean_ = deg_dispersion[0]
  std_ = deg_dispersion[1]
  lf = []
  l1 = []
  k = mean_ + (num_class+1) / 2

  for i in range(num_class):
    data_temp = torch.normal(i*mean_-k, std_, size=(num_examples, num_inputs))
    lf.append(data_temp)
    labels_temp = torch.full_like(cluster_l, i)
    l1.append(labels_temp)

  features = torch.cat(lf).float()
  labels = torch.cat(l1).long()

  if bias == True:
    features = torch.cat((features, torch.ones(len(features), 1)), 1)

  return features, labels

f, l = tensorGenCla(deg_dispersion = [6,2])
f1, l1 = tensorGenCla(deg_dispersion = [6,4])

plt.subplot(121)
plt.scatter(f[:,0], f[:,1], c=1)
plt.subplot(122)
plt.scatter(f1[:,0], f1[:,1], c=l1)

```

## 1.5 Split batch dataset functions

```python

l=list(range(5))
random.shuffle(l)

def data_iter(batch_size, features, labels):
  num_examples = len(features)
  indices = list(range(num_examples))
  random.shuffle(indices)
  l=[]
  for i in range(0, num_examples, batch_size):
    j=torch.tensor(indices[i:min(i+batch_size, num_examples)])
    l.append([torch.index_select(features, 0, j), torch.index_select(labels, 0, j)])
  return l

torch.manual_seed(420)
features, labels = tensorGenCla()
```

## 1.6 Python modules

</details>



<details>
<summary><h1>2. Visulization tool TensorBoard installation and use for PyTorch deep learning</h1></summary>

## 1.1 TensorBoard installation

pip install tensorboardX

## 1.2 SummaryWriter class

```python

writer = SummaryWriter(log_dir='test')

writer.log_dir

for i in range(10):
  writer.add_scalar('mul', i*i, i)
```

</details>


<details>
<summary><h1>3. Linear regression modeling using deep learning</h1></summary>
  
## 1.1 Deep learning modeling implementation in 4 steps

- Step 1. Model selection
- Step 2. Determine target function
- Step 3. Select optimization method
- Step 4. Model training

## 1.2 PyTorch's derivable tensor in-place operation

```python
import random

import matplotlib as mpl
import matplotlib.pyplot as plt

import numpyy as np

import torch

from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import Dataset, TensorDataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

from torchLearning import *


torch.manual_seed(420)

features, labels = tensorGenReg()

#Step 1. w1x2 + w2x2 + b
def linreg(X,w):
  return torch.mm(X, w)

#Step 2. MSE
def squared_loss(h_hat, y):
  num_ = y.numel()
  sse = torch.sum((h_hat,reshape(-1,1) - y.reshape(-1,1)) ** 2)
  return sse / num_

# Step 3. Determine optimization method
def sgd(params, lr):
  params.data -= lr*parames.grad
  params.grad.zero_()

# Step 4. Model training
torch.manual_seed(420)
```

## 1.3 Linear regression manual implementation with deep learning model

```python
writer=SummaryWriter(log_dir='reg_loss')

batch_size = 10
num_epochs = 3
lr=0.03
w = torch.zeros(3,1,requires_grad = True)

net = linreg
loss = squared_loss

for epoch in range(num_eopchs):
  for X, y in data_iter(batch_size, features, labels):
    l=loss(net(X,w),y)
    l.backward()
    sgd(w, lr)
  train_l = loss(net(features, w), labels)
  print('epoch %d, loss %f' % (spech+1, train_l))
  writer.add_scalar('mul', train_l, epoch)

```

## 1.4 Linear regression auto implementation (use libraries) with deep learning model

```python

batch_size=10
lr=0.03
num_epochs=3

torch.manual_seed(420)

features, labels = tensorGenReg()
features = features[:, :-1]
data = TensorDataset(features, labels)
batchData = DataLoader(data, batch_size = batch_size, shuffle=True)

# Step 1. Define model
class LR(nn.Module):
  def __init__(self, in_features=2, out_features=1):
    super(LR, self).__init__()
    self.linear=nn.Linear(in_features, out_features)

  def forward(self, x):
    out=self.linear(x)
    return out

LR_model=LR()

# Step 2. Define loss function
criterion = nn.MSELoss()

# Step 3. Define optimization method
optimizer = optim.SGD(LR_model.parameters(), lr=0.03)

# Step 4. Model training
def fit(net, criterion, optimizer, batchdata, epochs):
  for epoch in range(epochs):
    for X,y in batchdata:
      yhat = net.forward(x)
      loss=criterion(yhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.setp()
    writer.add_scalar('loss', loss, global_step=epoch)

# execute
torch.manual_seed(420)

fit(net=LR_model, criterion=cterion, optimizer=optimizer,batchdata=batchdata, epochs=num_eopchs)

list(LR_model.parameters())

criterion(LR_model(features), labels)

writer.add_graph(LR_model, (features,))
```


</details>


<details>
<summary><h1>4. Logistic regression modeling using deep learning</h1></summary>

```python
import random

import matplotlib as mpl
import matplotlib.pyplot as plt

import numpy as np

import torch
from torch import nn,optim
import torch.nn.functional as F
from torch.utils.data mport Dataset, TensorDataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

from torchLearning import *

from IPython.core.interactiveshell import InteractiveShell
```

## 1.1 Logistic regression manual implementation with deep learning model

```python
# Generate dataset
torch.manual_seed(420)

features, labels = tensorGenCla(num_class=2, bias=True)

plt.scatter(features[:,0], features[:,1], c=labels)

# Step 1. model selection
def sigmoid(z):
  return 1/(1+torch.exp(-z))

def logistic(X,w):
  return sigmoid(torch.mm(X,w))

def cal(sigma, p=0.5):
  return((sigma>=p).float())

def accuracy(y_hat,y):
  acc_bool=cal(y_hat).flatten() == y.flatten()
  acc = torch.mean(acc_bool.float())
  return(acc)

# Step 2. Define loss function
def cross_entropy(sigma,y):
  return(-(1/y.numel())*torch.sum((1-y)*torch.log(1-sigma)+y*torch.log(sigma)))

# Step 3. Define optimization method
def sgd(params, lr):
  params.data -= lr*parames.grad
  params.grad.zero_()

# Step 4. Model training
batch_size=10
lr=0.03
num_epochs=3
w = torch.ones(3,1,requires_grad=True)

net=logistic
loss=cross_entropy

for epoch in range(num_eopchs):
  for X,y in data_iter(batch_size, features, labels):
    l=loss(net(X,w),y)
    l.backward()
    sgd(w, lr)
  train_acc = accuracy(cal(net(features,w)), labels)
  print('epoch %d, accuracy %f' % (epoch+1, train_acc))

```


## 1.2 Logistic regression auto implementation with deep learning model
```python
torch.manual_seed(420)

features, labels = tensorGenCla(num_class=2)
labels=labels.float()
data=TensorDataset(features, labels)
batchData=DataLoader(data, batch_size=batch_size, shuffle=True)

# Step 1. Model selection
class logisticR(nn.Module):
  def __init__(self, in_features=2, out_features=1):
    super(logisticR, self).__init__()
    self.linear=nn.Linear(in_features, out_features)

  def forward(self,x):
    out=self.linear(x)
    return out

logic_model=logisticR()

# Step 2 criterion = nn.BCEWithLogitsLoss()

# Step 3. Define optimization method
optimizer = optim.SGD(logic_model.parameters(), lr=lr)

# Step 4. Model training
def fit(net, criterion, optimizer, batchdata, epochs):
  for epoch in range(epochs):
    for X,y in batchdata:
      zhat=net.forward(X)
      loss=criterion(zhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.setp()

torch.manual_seed(420)

fit(net=logic_model, criterion=criterion, optimizer=optimizer, batchdata=batchdata,epochs=num_epochs)

logic_model

list(logic_model.parameters())

criterion(logic_model(features), labels)

def acc_zhat(zhat,y):
  sigma=sigmoid(zhat)
  return accuracy(sigma,y)

acc_zhat(logic_model(features), labels)
```
## 1.3 Model fine-tuning

</details>


<details>
<summary><h1>5. Softmax regression modeling using deep learning</h1></summary>

```python
import random

import matplitlib as mpl
import matplotlib.pyplot as plt

import numpy as np

import torch
from torch import nn,optim
import torch.nn.functional as F
from torch.utils.data import Dataset,TensorDataset,DataLoader
from torch.tuils.tensorboard import SummaryWriter

from torchLearning import *

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

```

## 1.1 Softmax regression and max regression

```python
# create dataset
torch.manual_seed(420)

features, labels = tensorGenCla(bias=True, deg_dispersion=[6,2])
plt.scatter(features[:,0], features[:,1], c=labels)

# Step 1 model selection
def softmax(X,w):
  m=torch.exp(torch.mm(X,w))
  sp=torch.sum(m,1).reshape(-1,1)
  return m/sp

# Step 2 Target function
f=torch.tensor([[0.6,0.2,0.2],[0.3,0.4,0.3]])
l=torch.tensor([0,1])

def m_cross_entropy(soft_z,y):
  y=y.long()
  prob_real=torch.gather(soft_z, 1, y)
  return (-(1/y.numel()) * torch.log(prob_real).sum())


# Step 3 Determine optimization method
def m_accuracy(soft_z,y):
  acc_bool = torch.argmax(soft_z,1).flatten() == y.flatten()
  acc=torch.mean(acc_bool.float())
  return (acc)

# Step model training
torch.manual_seed(420)

features, labels = tensorGenCla(bias=True, deg_dispersion=[6,2])
plt.scatter(features[:,0], features[:,1], c=labels)

torch.manual_seed(420)

batch_size=10
lr=0.03
num_epochs=3
w=torch.randn(3,3,requires_grad=True)

net=softmax
loss=m_cross_entropy

for epoch in range(num_epochs):
  for X,y in data_iter(batch_size, features, labels):
    l=loss(net(X,w),y)
    l.backward()
    sgd(w, lr)
  train_acc = m_accuracy(net(features, w), labels)
  print('epoch %d, acc %f' % (epoch +1, train_acc))

# fine tune model

```


## 1.2 Softmax regression manual implementation
```python
batch_size=10
lr=0.03
num_epochs=3

# prepare dataset
torch.manual_seed(420)

features, labels = tensorGenCla(deg_dispersion = [6,2])
labels = labels.float()
data = TensorDataset(features, labels)
batchData = DataLoader(data, batch_size=batch_size, shuffle=True)

# Step 1 define model
class softmaxR(nn.Module):
  def __init__(self, in_features=2, out_features=3, bias=False):
    super(softmaxR, self).__init__()
    self.linear=nn.Linear(in_features, out_features)

  def forward(self, x):
    out=self.linear(x)
    return out

softmax_model = softmaxR()

# Step 2 define loss function
criterion = nn.CrossEntropyLoss()

# Step 3. Define optimization method
optimizer=optim.SGD(softmax_model.parameters(), lr=lr)

# Step 4. Model training
def fit(net, criterion, optimizer, batcdata, epochs):
  for epoch in range(epochs):
    for X,y in batchdata:
      zaht=net.forward(X)
      y=y.flatten().long()
      loss=criterion(zhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

fit(net=softmax_model, criterion=criterion, optimizer=optimizer, batchdata=batchdata, epochs=num_epochs)

softmax_model

criterion(softmax_model(features), labels.flatten().long())

m_accuracy(F.softmax(softmax_model(features),1),labels)

F.softmax(softmax_model(features), 1)
```
## 1.3 Model stability test

## 1.4 Softmax regression auto implementation

## 1.5 Run PyTorch deep learning models with GPU

</details>



