

<details>
<summary><h1>1. Deep Learning Modeling Goals and Performance Evaluation Theory</h1></summary>


## 1.1 Setup package automatic load in Jupyter initialization process
- Find .ipython/profile_default/startup. If not there, create it.
- create start.py file inside that folder
- input following codes:
  
```python
import random

import time

import math

import matplotlib as mpl
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import numpy as np

import pandas as pd

import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import Dataset, TensorDataset, DataLoader

from torchLearning import *

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
```

- restart ipy to check

## 1.2 Machine Learning Goals and Model Evaluation Methods


## 1.3 Manually implement splitting traing and test dataset

```python

def data_split(features, labels, rate=0.7):
  num_examples=len(features)
  indices=list(range(num_examples))
  random.shuffle(indices)
  num_train=int(num_examples*rate)
  indices_train=torch.tensor(indices[:num_train])
  indices_test=torch.tensor(indices[num_train:])
  Xtrain=features[indices_train]
  ytrain=labels[indices_train]
  Xtest=features[indices_test]
  ytest=labels[indices_test]
  return Xtrain, Xtest, ytrain, ytest

torch.manual_seed(420)
features, labels=tensorGenReg()

Xtrain, Xtest, ytrain, ytest=data_split(features, labels)

batch_size=10
lr=0.03
num_epochs=5
w=torch.zeros(3,1, requires_grad=True)

net=linreg
loss=MSE_loss

for epoch in range(num_epochs):
  for X,y in data_iter(batch_size, Xtrain, ytrain):
    l=loss(net(X,w),y)
    l.backward()
    sgd(w,lr)

MSE_loss(torch.mm(Xtrain, w), ytrain)
MSE_loss(torch.mm(Xtest, w), ytest)

```

## 1.4 Dataset and DataLoader introduction and dataset split function
Considering that deep learning usually process large dataset, therefore it's not possible that create a new set and store the data. PyTorch usually store the data by mapping the indexing.

```python
from torch.utils.data import random_split

t=torch.arange(12).reshape(4,3)

random_split(t, [2,2])

```

```python
class LBCDataset(Dataset):
  def __init__(self,data):
    self.features=data.data
    self.labels=data.target
    self.lens=len(data.data)

  def __getitem__(self, index):
    return self.features[index,:], self.labels[index]

  sef __len__(self):
    return self.lens

data=LBC()
LBC_data=LBCDataset(data)

LBC_data.lens

LBC_data.__getitem__[2]

LBC_data.features[2]

LBC_data.labels[2]

LBC_train, LBC_test = random_split(LBC_data, [num_train, num_test])

LBC_train.dataset == LBC_data

LBC_train.indices[:10]

```
Although data processing using PyTorch seems more complicated, it is very necessary to use PyTorch to pre-process the data, especially for the large scale, non-structural data.
We have to use DataLoader function to transfer the data from normal condition to "can be modeled" condition. The "can be modeled" condition means that after DataLoader function, data not included original data information, but also included process method information, such as number of batches, batch_size, shuffle, num_worker, etc.

```python
train_loader=DataLoader(LBC_train, batch_size=10, shuffle=True)
test_loader=DataLoader(LBC_test, batch_size=10, shuffle=False)

train_loader.dataset
```

The above codes can be summarized as follows, you can image that the encapsulation process is a mapped storage as follows
![Data Encapsulation](/_Deep_Learning_using_PyTorch/imgs/Data encapsulation.png)











  
</details>


