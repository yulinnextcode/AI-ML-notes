

<details>
<summary><h1>1. Deep Learning Modeling Goals and Performance Evaluation Theory</h1></summary>


## 1.1 Setup package automatic load in Jupyter initialization process
- Find .ipython/profile_default/startup. If not there, create it.
- create start.py file inside that folder
- input following codes:
  
```python
import random

import time

import math

import matplotlib as mpl
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import numpy as np

import pandas as pd

import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import Dataset, TensorDataset, DataLoader

from torchLearning import *

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
```

- restart ipy to check

## 1.2 Machine Learning Goals and Model Evaluation Methods


## 1.3 Manually implement splitting traing and test dataset

```python

def data_split(features, labels, rate=0.7):
  num_examples=len(features)
  indices=list(range(num_examples))
  random.shuffle(indices)
  num_train=int(num_examples*rate)
  indices_train=torch.tensor(indices[:num_train])
  indices_test=torch.tensor(indices[num_train:])
  Xtrain=features[indices_train]
  ytrain=labels[indices_train]
  Xtest=features[indices_test]
  ytest=labels[indices_test]
  return Xtrain, Xtest, ytrain, ytest

torch.manual_seed(420)
features, labels=tensorGenReg()

Xtrain, Xtest, ytrain, ytest=data_split(features, labels)

batch_size=10
lr=0.03
num_epochs=5
w=torch.zeros(3,1, requires_grad=True)

net=linreg
loss=MSE_loss

for epoch in range(num_epochs):
  for X,y in data_iter(batch_size, Xtrain, ytrain):
    l=loss(net(X,w),y)
    l.backward()
    sgd(w,lr)

MSE_loss(torch.mm(Xtrain, w), ytrain)
MSE_loss(torch.mm(Xtest, w), ytest)

```

## 1.4 Dataset and DataLoader introduction and dataset split function

### 1.4.1 Dataset and DataLoader instructions

Considering that deep learning usually process large dataset, therefore it's not possible that create a new set and store the data. PyTorch usually store the data by mapping the indexing.

```python
from torch.utils.data import random_split

t=torch.arange(12).reshape(4,3)

random_split(t, [2,2])

```

```python
class LBCDataset(Dataset):
  def __init__(self,data):
    self.features=data.data
    self.labels=data.target
    self.lens=len(data.data)

  def __getitem__(self, index):
    return self.features[index,:], self.labels[index]

  sef __len__(self):
    return self.lens

data=LBC()
LBC_data=LBCDataset(data)

LBC_data.lens

LBC_data.__getitem__[2]

LBC_data.features[2]

LBC_data.labels[2]

LBC_train, LBC_test = random_split(LBC_data, [num_train, num_test])

LBC_train.dataset == LBC_data

LBC_train.indices[:10]

```
Although data processing using PyTorch seems more complicated, it is very necessary to use PyTorch to pre-process the data, especially for the large scale, non-structural data.
We have to use DataLoader function to transfer the data from normal condition to "can be modeled" condition. The "can be modeled" condition means that after DataLoader function, data not included original data information, but also included process method information, such as number of batches, batch_size, shuffle, num_worker, etc.

```python
train_loader=DataLoader(LBC_train, batch_size=10, shuffle=True)
test_loader=DataLoader(LBC_test, batch_size=10, shuffle=False)

train_loader.dataset
```

The above codes can be summarized as follows, you can image that the encapsulation process is a mapped storage as follows

![Data Encapsulation](/_Deep_Learning_using_PyTorch/imgs/Data_encapsulation.png)

The above figure can be codes as follows
```python
class LBCDataset(Dataset):
  def __init__(self,data):
    self.features=data.data
    self.labels=data.target
    self.lens=len(data.data)

  def __getitem__(self, index):
    return self.features[index,:], self.labels[index]

  sef __len__(self):
    return self.lens

data=LBC()
LBC_data=LBCDataset(data)   #encapsulation

LBC_train, LBC_test = random_split(LBC_data, [num_train, num_test])   #split

train_loader=DataLoader(LBC_train, batch_size=10, shuffle=True)    #load
test_loader=DataLoader(LBC_test, batch_size=10, shuffle=False)
```

The reason that we use process data with above figure (**class->encapsulation->load->modeling** rather than directly modeling with data) is that
- This process fit not only structural data but also non-structural data
- A lot of functions such as random_split can be used after data encapsulation

> [!TIP]
> A lot of references use scikit-learn's train_test_split function for PyTroch deep learning modeling to split the data. This is very convenient, but this method is not suitable for spliting large dataset and it will use huge computational resource. Because after this split it will output real entity objects, which use a lot of space if the original dataset is large.
> Best practice is that we use PyTroch's function and class. If there are not any, we'd better manually create class and functions to process tensors. Last will be the using scikit-learn functions
> **PyTorch original functions and class > Manually create functions based on tensors > Scikit-Learn function**

  
### 1.4.2 Modeling process and evaluation

```python
features, labels = tensorGenReg()
features = features[:,:-1]

class GenData(Dataset):
  def __init__(self, features, labels):
    self.features=features
    self.labels=labels
    self.lens=len(features)

  def __getitem__(self, index):
    return self.features[index,:],self.labels[index]

  def __len__(self):
    return self.lens

data=GenData(features, labels)

num_train=int(data.lens*0.7)
num_test=data.lens-num_train
data_train, data_test=random_split(data, [num_train, num_test])

train_loader=DataLoader(data_train, batch_size=10, shuffle=True)
test_loader=DataLoader(data_test, batch_size=10, shuffle=True)

batch_size=10
lr=0.03
num_epochs=3

# define model -> instanlize model -> loss function -> optimization method -> train model
class LR(nn.Module):
  def __init__(self, in_features=2, out_features=1):
    super(LR, self).__init__()
    self.linear=nn.Linear(in_features, out_features)

  def forward(self, x):
    out=self.linear(x)
    return out

LR_model=LR()

criterion=nn.MSELoss()

optimizer=optim.SGD(LR_model.parameters(), lr=0.03)

def fit(net, criterion, optimizer, batchdata, epochs=3):
  for epoch in range(epochs):
    for X,y in batchdata:
      yhat=net.forward(X)
      loss=criterion(yhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

fit(net=LR_model,
    criterion=criterion,
    optimizer=optimizer,
    batchdata=train_loader,
    epochs=num_epochs)

LR_model

list(LR_model.parameters())

F.mes_loss(LR_model(data[data_train.indices][0], data[data_train.indices][1])) # calculate mse with train dataset

F.mes_loss(LR_model(data[data_train.indices][0], data[data_train.indices][1])) # calculate mse with test dataset

```
Remember we used to use Tensor's functions to do the same thing, but muct easier. As following shows using tensordata function,
```Python
torch.manual_seed(420)

features, labels=tensorGenCla(num_class=2)
labels=labels.float()
data=TensorDataset(features, labels)
batchData=DataLoader(data, batch_size=batch_size, shuffle=True)
```


## 1.5 More functions

- Data encapsulation, data split and load functions
```python
def split_loader(features, labels, batch_size=10, rate=0.7):
  data=GenData(features, labels)
  num_train=int(data.lens*0.7)
  num_test=data.lens-num_train
  data_train, data_test=random_split(data, [num_train, num_test])
  train_loader=DataLoader(data_train, batch_size=batch_size, shuffle=True)
  test_loader=DataLoader(data_test, batch_size=batch_size, shuffle=False)
  return(train_loader, test_loader)
```

Test the above function
```python
torch.manual_seed(420)

features, labels=tensorGenReg()

feature = features[:,:-1]

train_loader, test_loader=split_loader(features, labels)

train_loader.dataset[0]
```

- Model training function
```python
def fit(net, criterion, optimizer, batchdata, epochs=3, cla=False):
  for epoch in range(epochs):
    for X,y in batchdata:
      if cla==True:
        y=y.flatten().long()
      yhat=net.forward(X)
      loss=criterion(yhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
```

- MSE calculation function
```python
def mse_cal(data_loader, net):
  data=data_loader.dataset
  X=data[:][0]
  y=data[:][1]
  yhat=net(X)
  return F.mse_loss(yhat,y)
```

- Accuracy calculation function
```python
def accuracy_cal(data_loader, net):
  data=data_loader.dataset
  X=data[:][0]
  y=data[:][1]
  zhat=net(X)
  soft_z=F.softmax(zhat,1)
  acc_bool=torch.argmax(soft_z,1).flatten()==y.flatten()
  acc=torch.mean(acc_bool.float())
  return acc
```
Use the following code to test function
```python

torch.manual_seed(420)
features, labels=tensorGenCla()
train_loader, test_loader=split_loader(features, labels)

class softmaxR(nn.Module):
  def __init__(self, in_features=2, out_features=3, bias=False):
    super(softmaxR, self).__init__()
    self.linear=nn.Linear(in_features, out_features)

  def forward(self, x):
    out=self.linear(x)
    return out

softmax_model=softmaxR()

criterion=nn.CrossEntropyLoss()

optimizer=optim.SGD(softmax_model.parameters(), lr=lr)

fit(net=softmax_model,
    criterion=criterion,
    optimizer=optimizer,
    batchdata=train_loader,
    eopchs=num_epochs,
    cla=True)

m_accuracy

accuracy_cal(train_loader, softmax_model)

accuracy_cal(test_loader, softmax_model)

```
</details>



<details>
<summary><h1>2. </h1></summary>






























</details>
