

<details>
<summary><h1>1. Deep Learning Modeling Goals and Performance Evaluation Theory</h1></summary>


## 1.1 Setup package automatic load in Jupyter initialization process
- Find .ipython/profile_default/startup. If not there, create it.
- create start.py file inside that folder
- input following codes:
  
```python
import random

import time

import math

import matplotlib as mpl
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import numpy as np

import pandas as pd

import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import Dataset, TensorDataset, DataLoader

from torchLearning import *

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
```

- restart ipy to check

## 1.2 Machine Learning Goals and Model Evaluation Methods


## 1.3 Manually implement splitting traing and test dataset

```python

def data_split(features, labels, rate=0.7):
  num_examples=len(features)
  indices=list(range(num_examples))
  random.shuffle(indices)
  num_train=int(num_examples*rate)
  indices_train=torch.tensor(indices[:num_train])
  indices_test=torch.tensor(indices[num_train:])
  Xtrain=features[indices_train]
  ytrain=labels[indices_train]
  Xtest=features[indices_test]
  ytest=labels[indices_test]
  return Xtrain, Xtest, ytrain, ytest

torch.manual_seed(420)
features, labels=tensorGenReg()

Xtrain, Xtest, ytrain, ytest=data_split(features, labels)

batch_size=10
lr=0.03
num_epochs=5
w=torch.zeros(3,1, requires_grad=True)

net=linreg
loss=MSE_loss

for epoch in range(num_epochs):
  for X,y in data_iter(batch_size, Xtrain, ytrain):
    l=loss(net(X,w),y)
    l.backward()
    sgd(w,lr)

MSE_loss(torch.mm(Xtrain, w), ytrain)
MSE_loss(torch.mm(Xtest, w), ytest)

```

## 1.4 Dataset and DataLoader introduction and dataset split function

### 1.4.1 Dataset and DataLoader instructions

Considering that deep learning usually process large dataset, therefore it's not possible that create a new set and store the data. PyTorch usually store the data by mapping the indexing.

```python
from torch.utils.data import random_split

t=torch.arange(12).reshape(4,3)

random_split(t, [2,2])

```

```python
class LBCDataset(Dataset):
  def __init__(self,data):
    self.features=data.data
    self.labels=data.target
    self.lens=len(data.data)

  def __getitem__(self, index):
    return self.features[index,:], self.labels[index]

  sef __len__(self):
    return self.lens

data=LBC()
LBC_data=LBCDataset(data)

LBC_data.lens

LBC_data.__getitem__[2]

LBC_data.features[2]

LBC_data.labels[2]

LBC_train, LBC_test = random_split(LBC_data, [num_train, num_test])

LBC_train.dataset == LBC_data

LBC_train.indices[:10]

```
Although data processing using PyTorch seems more complicated, it is very necessary to use PyTorch to pre-process the data, especially for the large scale, non-structural data.
We have to use DataLoader function to transfer the data from normal condition to "can be modeled" condition. The "can be modeled" condition means that after DataLoader function, data not included original data information, but also included process method information, such as number of batches, batch_size, shuffle, num_worker, etc.

```python
train_loader=DataLoader(LBC_train, batch_size=10, shuffle=True)
test_loader=DataLoader(LBC_test, batch_size=10, shuffle=False)

train_loader.dataset
```

The above codes can be summarized as follows, you can image that the encapsulation process is a mapped storage as follows

![Data Encapsulation](/_Deep_Learning_using_PyTorch/imgs/Data_encapsulation.png)

The above figure can be codes as follows
```python
class LBCDataset(Dataset):
  def __init__(self,data):
    self.features=data.data
    self.labels=data.target
    self.lens=len(data.data)

  def __getitem__(self, index):
    return self.features[index,:], self.labels[index]

  sef __len__(self):
    return self.lens

data=LBC()
LBC_data=LBCDataset(data)   #encapsulation

LBC_train, LBC_test = random_split(LBC_data, [num_train, num_test])   #split

train_loader=DataLoader(LBC_train, batch_size=10, shuffle=True)    #load
test_loader=DataLoader(LBC_test, batch_size=10, shuffle=False)
```

The reason that we use process data with above figure (**class->encapsulation->load->modeling** rather than directly modeling with data) is that
- This process fit not only structural data but also non-structural data
- A lot of functions such as random_split can be used after data encapsulation

> [!TIP]
> A lot of references use scikit-learn's train_test_split function for PyTroch deep learning modeling to split the data. This is very convenient, but this method is not suitable for spliting large dataset and it will use huge computational resource. Because after this split it will output real entity objects, which use a lot of space if the original dataset is large.
> Best practice is that we use PyTroch's function and class. If there are not any, we'd better manually create class and functions to process tensors. Last will be the using scikit-learn functions
> **PyTorch original functions and class > Manually create functions based on tensors > Scikit-Learn function**

  
### 1.4.2 Modeling process and evaluation

```python
features, labels = tensorGenReg()
features = features[:,:-1]

class GenData(Dataset):
  def __init__(self, features, labels):
    self.features=features
    self.labels=labels
    self.lens=len(features)

  def __getitem__(self, index):
    return self.features[index,:],self.labels[index]

  def __len__(self):
    return self.lens

data=GenData(features, labels)

num_train=int(data.lens*0.7)
num_test=data.lens-num_train
data_train, data_test=random_split(data, [num_train, num_test])

train_loader=DataLoader(data_train, batch_size=10, shuffle=True)
test_loader=DataLoader(data_test, batch_size=10, shuffle=True)

batch_size=10
lr=0.03
num_epochs=3

# define model -> instanlize model -> loss function -> optimization method -> train model
class LR(nn.Module):
  def __init__(self, in_features=2, out_features=1):
    super(LR, self).__init__()
    self.linear=nn.Linear(in_features, out_features)

  def forward(self, x):
    out=self.linear(x)
    return out

LR_model=LR()

criterion=nn.MSELoss()

optimizer=optim.SGD(LR_model.parameters(), lr=0.03)

def fit(net, criterion, optimizer, batchdata, epochs=3):
  for epoch in range(epochs):
    for X,y in batchdata:
      yhat=net.forward(X)
      loss=criterion(yhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

fit(net=LR_model,
    criterion=criterion,
    optimizer=optimizer,
    batchdata=train_loader,
    epochs=num_epochs)

LR_model

list(LR_model.parameters())

F.mes_loss(LR_model(data[data_train.indices][0], data[data_train.indices][1])) # calculate mse with train dataset

F.mes_loss(LR_model(data[data_train.indices][0], data[data_train.indices][1])) # calculate mse with test dataset

```
Remember we used to use Tensor's functions to do the same thing, but muct easier. As following shows using tensordata function,
```Python
torch.manual_seed(420)

features, labels=tensorGenCla(num_class=2)
labels=labels.float()
data=TensorDataset(features, labels)
batchData=DataLoader(data, batch_size=batch_size, shuffle=True)
```


## 1.5 More functions

- Data encapsulation, data split and load functions
```python
def split_loader(features, labels, batch_size=10, rate=0.7):
  data=GenData(features, labels)
  num_train=int(data.lens*0.7)
  num_test=data.lens-num_train
  data_train, data_test=random_split(data, [num_train, num_test])
  train_loader=DataLoader(data_train, batch_size=batch_size, shuffle=True)
  test_loader=DataLoader(data_test, batch_size=batch_size, shuffle=False)
  return(train_loader, test_loader)
```

Test the above function
```python
torch.manual_seed(420)

features, labels=tensorGenReg()

feature = features[:,:-1]

train_loader, test_loader=split_loader(features, labels)

train_loader.dataset[0]
```

- Model training function
```python
def fit(net, criterion, optimizer, batchdata, epochs=3, cla=False):
  for epoch in range(epochs):
    for X,y in batchdata:
      if cla==True:
        y=y.flatten().long()
      yhat=net.forward(X)
      loss=criterion(yhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
```

- MSE calculation function
```python
def mse_cal(data_loader, net):
  data=data_loader.dataset
  X=data[:][0]
  y=data[:][1]
  yhat=net(X)
  return F.mse_loss(yhat,y)
```

- Accuracy calculation function
```python
def accuracy_cal(data_loader, net):
  data=data_loader.dataset
  X=data[:][0]
  y=data[:][1]
  zhat=net(X)
  soft_z=F.softmax(zhat,1)
  acc_bool=torch.argmax(soft_z,1).flatten()==y.flatten()
  acc=torch.mean(acc_bool.float())
  return acc
```
Use the following code to test function
```python

torch.manual_seed(420)
features, labels=tensorGenCla()
train_loader, test_loader=split_loader(features, labels)

class softmaxR(nn.Module):
  def __init__(self, in_features=2, out_features=3, bias=False):
    super(softmaxR, self).__init__()
    self.linear=nn.Linear(in_features, out_features)

  def forward(self, x):
    out=self.linear(x)
    return out

softmax_model=softmaxR()

criterion=nn.CrossEntropyLoss()

optimizer=optim.SGD(softmax_model.parameters(), lr=lr)

fit(net=softmax_model,
    criterion=criterion,
    optimizer=optimizer,
    batchdata=train_loader,
    eopchs=num_epochs,
    cla=True)

m_accuracy

accuracy_cal(train_loader, softmax_model)

accuracy_cal(test_loader, softmax_model)

```
</details>



<details>
<summary><h1>2. Model goodness of fit and under-fit model structure adjust</h1></summary>
## 2.1 Model goodness of fit

> [!TIP]
> Strictly Speaking
> We divide dataset into train, validate and test.
> Train dataset is used to train model
> Test dataset is used to test model
> Validation dataset is also used to train model, but validation is not train model parameters, but model hyper parameters. 

> [!TIP]
> Un-Strictly Speaking
> Validation and test are mixed used

Underfit and Overfit

## 2.2 Model underfit examples
Use linear regression to fit high order equation, then model underfit will happen.

- Create dataset

```python
torch.manual_seed(420)

features, labels = tensorGenReg(w=[2,-1], bias=False, deg=2)

plt.subplot(121)
plt.scatter(features[:,0], labels)
plt.subplot(122)
plt.scatter(features[:,1], labels)

train_loader, test_loader = split_loader(features, labels)
```

- Train model
```python
class LR_class(nn.Module):
  def __init__(self, in_features=2, out_features=1):
    super(LR_class, self).__init__()
    self.linear=nn.Linear(in_features, out_features)

  def forward(self, x):
    out=self.linear(x)
    return out

torch.manual_seed(420)

LR=LR_class()

train_l=[]
test_l=[]

num_epochs=20

for epochs in range(num_epochs):
  fit(net=LR,
      criterion=nn.MSELoss(),
      optimizer=optim.SGD(LR.parameters(), lr=0.03),
      batchdata=train_loader,
      epochs=epochs)
  train_l.append(mse_cal(train_loader, LR).detach().numpy())
  test_l.append(mse_cal(test_loader, LR).detach().numpy())

plt.plot(list(range(num_epochs)), train_l, label='train_mse')

plt.plot(list(range(num_epochs)), test_l, label='test_mse')

plt.legend(loc=1)
```
Based on the above figure, we can conclude that model is under fitted

Put the following codes in a py file
```python
def model_train_test(model,
                    train_data,
                    test_data,
                    num_epochs=20,
                    criterion=nn.MSELoss(),
                    optimizer=optim.SGD,
                    lr=0.03,
                    cla=False,
                    eva=mse_cal)
  train_l=[]
  test_l=[]

  for epochs in range(num_eopchs):
    fit(net=model,
        criterion=criterion,
        optimizer=optimizer(model.parameters(), lr=lr),
        batchdata=train_data,
        epochs=epochs,
        cla=cla)
    train_l.append(eva(train_data, model).detach())
    test_l.append(eva(test_data, model).detach())

``` 
Test the above codes
```python
torch.manual_seed(420)

LR=LR_class()

train_l, test_l=model_train_test(LR,
                                train_loader,
                                test_loader,
                                num_eopchs=20,
                                criterion=nn.MSELoss(),
                                optimizer=optim.SGD,
                                lr=0.03,
                                cla=False,
                                eva=mse_cal)

plt.plot(list(range(num_epochs)), train_l, label='train_mse')

plt.plot(list(range(num_epochs)), test_l, label='test_mse')

plt.legend(loc=1)
```

Add one more hidden layer but not add activation function with following codes
```python
class LR_class1(nn.Module):
  def __init__(self, in_features=2, n_hidden=4, out_features=1):
    super(LR_class1, self).__init__()
    self.linear1=nn.Linear(in_features, n_hidden)
    self.linear2=nn.Linear(n_hidden, out_features)

  def forward(self, x):
    z1=self.linear1(x)
    out=self.linear2(z1)
    return out
```
## 2.3 Activation function comparision

sigmoid, tanh, relu

Add **sigmoid** activation function but not add layer number with following codes
```python
class Sigmoid_class1(nn.Module):
  def __init__(self, in_features=2, n_hidden=4, out_features=1):
    super(Sigmoid_class1, self).__init__()
    self.linear1=nn.Linear(in_features, n_hidden, bias=bias)
    self.linear2=nn.Linear(n_hidden, out_features, bias=bias)

  def forward(self, x):
    z1=self.linear1(x)
    p1=torch.sigmoid(z1)
    out=self.linear2(p1)
    return out
```

Add **tanh** activation function but not add layer number with following codes
```python
class tanh_class1(nn.Module):
  def __init__(self, in_features=2, n_hidden=4, out_features=1):
    super(tanh_class1, self).__init__()
    self.linear1=nn.Linear(in_features, n_hidden, bias=bias)
    self.linear2=nn.Linear(n_hidden, out_features, bias=bias)

  def forward(self, x):
    z1=self.linear1(x)
    p1=torch.tanh(z1)
    out=self.linear2(p1)
    return out
```

Add **ReLU** activation function but not add layer number with following codes
```python
class ReLU_class1(nn.Module):
  def __init__(self, in_features=2, n_hidden=4, out_features=1):
    super(ReLU_class1, self).__init__()
    self.linear1=nn.Linear(in_features, n_hidden, bias=bias)
    self.linear2=nn.Linear(n_hidden, out_features, bias=bias)

  def forward(self, x):
    z1=self.linear1(x)
    p1=torch.relu(z1)
    out=self.linear2(p1)
    return out
```
- Instantlize models
```python
torch.manual_seed(420)

LR1=LR_class1()

sigmoid_model1=Sigmoid_class1()
tanh_model1=tanh_class1()
relu_model1=ReLU_class1()

model_1=[LR1, sigmoid_model1, tanh_model1, relu_model1]
name_1=['LR1', 'sigmoid_model1', 'tanh_model1', 'relu_model1']
```

- Define core parameters
```python
num_epochs=30
lr=0.03
```
- Store MSE for train and test dataset
```python
mse_train=torch.zeros(len(model_l), num_epochs)
mse_test=torch.zeros(len(model_l), num_epochs)
```

- Train model

```python
for epochs in range(num_epochs):
  for i, model in enumerate(model_1):
    fit(net=model,
        criterion=nn.MSELoss(),
        optimizer=optim.SGD(model.parameters(), lr=lr),
        batchdata=train_loader,
        epochs=epochs)

    mse_train[i][epochs]=mse_cal(train_loader, model).detach()

    mse_test[i][epochs]=mse_cal(test_loader, model).detach()
```
- Plot to compare results
```python

for i, name in enumerate(name_l):
  plt.plot(list(range(num_epochs)), mse_train[i], label=name)
plt.legend(loc=1)
plt.title('mse_train')

```

> [!IMPORTANT]
> Overlay (over stack) ReLU activation function may cause model not useable.
> Overlay (over stack) Sigmoid activation function may cause model not useable.
> Overlay (over stack) tanh activation function may cause model not useable.

## 2.5 Neural Network Structure Optimization 

- parameters vs hyper-parameters

- How to select neural network structures (gridsearch, AutoML, etc.)
  - Number of layers.
    - <3
    - [3,6]
    - >6
  - Number of nuerons each layer

- Avoid to mixly use activation functions

## 2.6 Randomly create loss function phenomena

- Is the loss function not changed during the training process? Like walking down from mountain top to down?
- Why loss function is decreasing but model evaluation criteria is fluctuating?

**If we use SGD or mini batch GD, in other words as long as we don't use batch GD, then the mountain will be changed for each iteration step.**

'[BGD vs SGD vs MBGD](https://zhuanlan.zhihu.com/p/25765735).


</details>



<details>
<summary><h1>3. Gradient Nonstationarity and Glorot Condition</h1></summary>

[Gradient Nonstationarity and Glorot Condition](https://blog.csdn.net/Grateful_Dead424/article/details/123235167).

[Gradient disappear and explosion](https://zhuanlan.zhihu.com/p/498525410).

## 3.3 Zero-Centered Data and Glorot condition
In order to solve the gradient nonstationarity, there are totally five methods:
- Parameter initialization method
- Data normalization method
- Derived activation method
- Adjust learning rate method
- Gradient descent optimization method

</details>


<details>
<summary><h1>4. Dead ReLU Problem and learning rate optimization</h1></summary>

nn.Sequential can only do linear structure model.
We can use class method to define model. Also, we can use nn.Sequential method to define model.

Use nn.init method

</details>

<details>
<summary><h1>5. Xavier method and kaiming method (HE initialization)</h1></summary>

These two methods are mainly about **parameters initialization method**. Xavier method is for tanh and Sigmoid activation method, and kaiming method (HE initialization) is for ReLU activation function.



</details>
