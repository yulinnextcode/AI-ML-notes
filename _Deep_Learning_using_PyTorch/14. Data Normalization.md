<details>
<summary><h1>1. Data Normalization and Batch Normalization</h1></summary>

Data normalization is an optional method for traditional machine learning. For example, decition tree algorithms do not adopt data normalization before modeling. However, data normalization is a general and very popular procedure for deep learning algorithms.

## 1.1 Traditional machine learning normalization methods

0-1 standarization (Max-Min Normalization) and Z-Score standarization

```python
torch.manual_seed(420)

features, labels = tensorGenReg(w=[2,-1], bias=False, deg=2)
features_norm = z_ScoreNormalization(features)

train_loader, test_loader = split_loader(features, labels)
train_loader_norm, test_loader=split_loader(features_norm, labels)

torch.manual_seed(24)
lr=0.03
num_epochs=40
sigmoid_model3=Sigmoid_class3()
sigmoid_model3_norm=Sigmoid_class3()

for m in sigmoid_model3.modules():
  if isinstance(m,nn.Linear):
    nn.init.xavier_uniform_(m.weight)

for m in sigmoid_model3_norm.modules():
  if isinstance(m, nn.Linear):
    nn.init.xavier_uniform_(m.weight)

train_l, test_l = model_train_test(sigmoid_model3,
                                    train_loader,
                                    test_loader,
                                    num_epochs=num_epochs,
                                    criterion=nn.MSELoss(),
                                    optimizer=optim.SGD,
                                    lr=lr,
                                    cla=False,
                                    eva=mse_cal)

train_l_norm, test_l_norm = model_train_test(sigmoid_model3_norm,
                                    train_loader_norm,
                                    test_loader,
                                    num_epochs=num_epochs,
                                    criterion=nn.MSELoss(),
                                    optimizer=optim.SGD,
                                    lr=lr,
                                    cla=False,
                                    eva=mse_cal)

plt.plot(list(range(num_epochs)), train_l, label='train_mse')
plt.plot(list(range(num_epochs)), train_l_nrom, label='train_norm_mse')
plt.legend(loc=1)

plt.plot(list(range(num_epochs)), test_l, label='test_mse')
plt.plot(list(range(num_epochs)), test_l_nrom, label='test_norm_mse')
plt.legend(loc=1)

weights_vp(sigmoid_model3, att="grad")
weights_vp(sigmoid_model3_norm, att="grad")
```

**值得注意的是，此处我们统一先对模型进行Xavier参数初始化计算然后再使用数据归一化方法，是为了避免不同随机数种子对参数初始化取值的影响，但实际上目前很多神经网络用到的归一化方法再最终效果上都能让模型拜托参数初始化的限制，也就是能够让模型再不死好用初始化方法的情况下仍然可以进行快速和稳定的收敛。Xavier和Normalization一般不共存，但如何选择请参看后面内容。也就是能够让模型再不适用初始化方法的情况下仍然可以进行快速和稳定的收敛**

**根据各层梯度计算公式不难发现，影响梯度平稳性的核心因素有三个，其一是各层的参数，第二是各先性层接收到的数据，第三则是激活函数。因此我们得到保证梯度平稳的第二个方法：输入数据调整（batch normalization method）**

Batch nromalization method is to adjust each batch data to imporve each layer gradient stationarity.

Batch normalization不仅仅是对初始数据的调整，而是对数据流淌到模型的每一层都进行分布调整。第二，该方法调整的数据归一化分布不一定是zero center的分布，它会根据激活函数输出和每层输出结果，会自适应调整分布来达成梯度平稳。

## 1.2 Batch Normalization theory foundamentals

任何归一化的方法，本质都是对数据进行平移和放缩，所谓平移是指数据集加或减某一个数，方所指数据集同意乘以或除以某一个数。而该操作对样本分布特征没有影响。不会影响原始数据的规律，不会影响对数据规律的挖掘。

**整体来看，针对梯度不平稳的解决方案（优化方法）总共分为五类，分别是参数初始化方法、输入数据的归一化方法、衍生激活函数使用方法、学习率调度方法以及梯度下降优化方法。**

**Glorot条件：正向传播时数据方差保持一致、反向传播时参数梯度方差保持一致的条件，也被称为Glorot条件。满足该条件的模型能够进行有效平稳的训练，而为了满足该条件而创建的（当然也是由上述论文提出的）模型初始化参数值设计方法，也被称为Xavier方法。而在Xavier方法中，最核心解决的问题，也就是为了创建Zero-Centered的初始化参数时参数的方差。和我们从朴素的角度思考的方向是一致的。由于Glorot条件和Xavier方法是在2010年提出的，彼时ReLU激活函数还未兴起，因此Xavier方法主要是围绕tanh激活函数可能存在的梯度爆炸或梯度消失进行的优化，Sigmoid激活函数效果次之。不过尽管如此，Glorot条件却是一个通用条件，后续围绕ReLU激活函数、用于解决神经元活性失效的优化方法（如HE初始化方法），也是遵照Glorot条件进行的方法设计。**

Batch Normalization procedures:
- Stage 1. Z-Score zoom
- Stage 2. Transfer and zoom with parameters

</details>


<details>
<summary><h1>2. Batch Normalization implemented in PyTorch</h1></summary>
 
## 2.1 nn.BatchNorm method

We use nn.Linear to create linear layer. Similarily, we use nn.BatchNorm to create BN layer to implement Batch Normalization operation.

## 2.2 Use nn.BatchNorm to implement neural networks with BN

有了对nn.BatchNorm的基本了解之后，我们知道**BN本质上是一种自适应的数据分布调整算法，数据分布的调整并不影响，因此我们可以再任何需要的位置进行BN归一化。另外根据Glorot条件，我们知道再模型构建过程中需要力求各梯度计算的有效性和平稳性，因此我们可以考虑再每一个线性层前面或者后面进行数据归一化处理**

```python

class net_class1(nn.Module):
  def __init__(self, act_fun=torch.relu, in_features=2, n_hidden=4, out_features=1, bias=True, BN_model=None, momentum=0.1):
    super(net_class1, self).__init__()
    self.linear1=nn.Linear(in_features, n_hidden, bias=bias)
    self.normalize1=nn.BatchNorm1d(n_hidden, momentum=momentum)
    self.linear2=nn.Linear(n_hidden, out_features, bias=bias)
    self.BN_model=BN_model
    self.act_fun=act_fun

  def forward(self, x):
    if self.BN_model==None:
      z1=self.linear1(x)
      p1=self.act_fun(z1)
      out=self.linear2(p1)
    elif self.BN_model=='pre':
      z1=self.normalize1(self.linear1(x))
      p1=self.act_fun(z1)
      out=self.linear2(p1)
    elif self.BN_model=='post':
      z1=self.linear1(x)
      p1=self.act_fun(z1)
      out=self.linear2(self.normalize1(p1))
    return out


torch.manual_seed(420)

features, labels=tensorGenReg(w=[2,-1], bias=False, deg=2)

train_loader, test_loader=split_loader(features, labels)

torch.manual_seed(24)
relu_model1_norm=net_class1(BN_model='pre')

relu_model1_norm.train()

list(relu_model1_norm.modules())

for m in list(relu_model1_norm.modules()):
  print(m.training==True)

list(relu_model1_norm.modules())[2].running_mean
list(relu_model1_norm.modules())[2].running_var

list(relu_model1_norm.modules())[2].weight

list(relu_model1_norm.modules())[2].bias

fit(net=relu_model1_norm,
    criterion=nn.MSELoss(),
    optimizer=topim.SGD(relu_model1_norm.parameters(), lr=lr),
    batchdata=train_loader,
    epochs=20,
    cla=False)

list(relu_model1_norm.modules())[2].weight
list(relu_model1_norm.modules())[2].bias

list(relu_model1_norm.modules())[2].running_mean
list(relu_model1_norm.modules())[2].running_var

relu_model1_norm.eval()

mse_cal(train_loader, relu_model1_norm)

mse_cal(test_loader, relu_model1_norm)

```

integrate model_train_test and model_comparison
```python
def model_train_test(model,
                    train_data,
                    test_data,
                    num_epochs=20,
                    criterion=nn.MSELoss(),
                    optimizer=optim.SGD,
                    lr=0.03,
                    init=False,
                    cla=False,
                    eva=mse_cal):
  train_l=[]
  test_l=[]

  for epochs in range(num_epochs):
    model.train()
    fit(net=model,
        criterion=criterion,
        optimizer=optimizer(model.parameters(), lr=lr),
        batchdata=train_data,
        epochs=epochs,
        cla=cla)
    model.eval()
    train_l.append(eva(train_data, model).detach())
    test_l.append(eva(test_data, model).detach())  
```
```python
def model_comparison(model_l,
                    name_l,
                    train_data,
                    test_data,
                    num_epochs=20,
                    criterion=nn.MSELoss(),
                    optimizer=topim.SGD(),
                    lr=0.03,
                    cla=False,
                    eva=mse_cal):
  train_l=torch.zeros(len(model_l), num_epochs)
  test_l=torch.zeros(len(model_l), num_epochs)

  for epochs in range(num_epochs):
    for i, model in enumerate(model_l):
      model.train()
      fit(net=model,
          criterion=criterion,
          optimizer=optimizer(model.parameters(), lr=lr),
          batchdata=train_data,
          epochs=epochs,
          cla=cla)
      model.eval()
      train_l[i][epochs]=eva(train_data, model).detach()
      test_l[i][epochs]=eva(test_data, model).detach()
```

Next, we will compare whether BN layer is effective or not compared to layer without BN
```python
torch.manual_seed(420)

features, labels = tensorGenReg(2=[2, -1], bias=False, deg=2)

train_loader, test_loader = split_loader(features, labels)

torch.manual_seed(24)

sigmoid_model1=net_class1(act_fun=torch.sigmoid)
sigmoid_model1_norm=net_class1(act_fun=torch.sigomoid, BN_model='pre')

model_l=[sigmoid_model1, sigmoid_model1_norm]
name_l=['sigmoid_model1', 'sigmoid_model1_norm']

lr=0.03
num_epochs=40

train_l, test_l = model_comparison(model_l=model_l,
                                  name_l=name_l,
                                  train_data=train_loader,
                                  test_data=test_loader,
                                  num_epochs=num_epochs,
                                  criterion=nn.MSELoss(),
                                  optimizer=optim.SGD(),
                                  lr=lr,
                                  cla=False,
                                  eva=mse_cal)

for i, name in enumerate(name_l):
  plt.plot(list(range(num_epochs)), train_l[i], label=name)

plt.legend(loc=1)
plt.title('mse_train')


for i, name in enumerate(name_l):
  plt.plot(list(range(num_epochs)), test_l[i], label=name)

plt.legend(loc=1)
plt.title('mse_test')

```

</details>


<details>
<summary><h1>3. Batch Normalization fine tune practice</h1></summary>

```python
class net_class2(nn.Module):
  def __init__(self, act_fun=torch.relu, in_features=2, n_hidden1=4, n_hidden2=4, out_features=1, bias=True, BN_model=None, momentum=0.1):
    super(net_class2, self).__init__()
    self.linear1=nn.Linear(in_features, n_hidden1, bias=bias)
    self.normalize1=nn.BatchNorm1d(n_hidden1, momentum=momentum)
    self.linear2=nn.Linear(n_hidden1, n_hidden2, bias=bias)
    self.normalize2=nn.BatchNorm1d(n_hidden2, momentum=momentum)
    self.linear3=nn.Linear(n_hidden2, out_features, bias=bias)
    self.BN_model=BN_model
    self.act_fun=act_fun

  def forward(self, x):
    if self.BN_model==None:
      z1=self.linear1(x)
      p1=self.act_fun(z1)
      z2=self.linear2(p1)
      p2=self.act_fun(z2)
      out=self.linear3(p2)
    elif self.BN_model=='pre':
      z1=self.normalize1(self.linear1(x))
      p1=self.act_fun(z1)
      z2=self.normalize2(self.linear2(p1))
      p2=self.act_fun(z2)
      out=self.linear3(p2)
    elif self.BN_model=='post':
      z1=self.linear1(x)
      p1=self.act_fun(z1)
      z2=self.linear2(self.normalize1(p1))
      p2=self.act_fun(z2)
      out=self.linear3(self.normalize2(p2))
    return out
```

```python
class net_class3(nn.Module):
  def __init__(self, act_fun=torch.relu, in_features=2, n_hidden1=4, n_hidden2=4, n_hidden3=4, out_features=1, bias=True, BN_model=None, momentum=0.1):
    super(net_class3, self).__init__()
    self.linear1=nn.Linear(in_features, n_hidden1, bias=bias)
    self.normalize1=nn.BatchNorm1d(n_hidden1, momentum=momentum)
    self.linear2=nn.Linear(n_hidden1, n_hidden2, bias=bias)
    self.normalize2=nn.BatchNorm1d(n_hidden2, momentum=momentum)
    self.linear3=nn.Linear(n_hidden3, n_hidden2, bias=bias)
    self.normalize3=nn.BatchNorm1d(n_hidden3, momentum=momentum)
    self.linear4=nn.Linear(n_hidden3, out_features, bias=bias)
    self.BN_model=BN_model
    self.act_fun=act_fun

  def forward(self, x):
    if self.BN_model==None:
      z1=self.linear1(x)
      p1=self.act_fun(z1)
      z2=self.linear2(p1)
      p2=self.act_fun(z2)
      z3=self.linear3(p2)
      p3=self.act_fun(z3)
      out=self.linear3(p3)
    elif self.BN_model=='pre':
      z1=self.normalize1(self.linear1(x))
      p1=self.act_fun(z1)
      z2=self.normalize2(self.linear2(p1))
      p2=self.act_fun(z2)
      z3=self.normalize3(self.linear3(p2))
      p3=self.act_fun(z3)
      out=self.linear4(p3)
    elif self.BN_model=='post':
      z1=self.linear1(x)
      p1=self.act_fun(z1)
      z2=self.linear2(self.normalize1(p1))
      p2=self.act_fun(z2)
      z3=self.linear3(self.normalize2(p2))
      p3=self.act_fun(z3)
      out=self.linear4(self.normalize3(p3))
    return out
```

```python
class net_class4(nn.Module):
  def __init__(self, act_fun=torch.relu, in_features=2, n_hidden1=4, n_hidden2=4, n_hidden3=4, n_hidden4=4, out_features=1, bias=True, BN_model=None, momentum=0.1):
    super(net_class4, self).__init__()
    self.linear1=nn.Linear(in_features, n_hidden1, bias=bias)
    self.normalize1=nn.BatchNorm1d(n_hidden1, momentum=momentum)
    self.linear2=nn.Linear(n_hidden1, n_hidden2, bias=bias)
    self.normalize2=nn.BatchNorm1d(n_hidden2, momentum=momentum)
    self.linear3=nn.Linear(n_hidden3, n_hidden2, bias=bias)
    self.normalize3=nn.BatchNorm1d(n_hidden3, momentum=momentum)
    self.linear4=nn.Linear(n_hidden4, n_hidden3, bias=bias)
    self.normalize4=nn.BatchNorm1d(n_hidden4, momentum=momentum)
    self.linear5=nn.Linear(n_hidden4, out_features, bias=bias)
    self.BN_model=BN_model
    self.act_fun=act_fun

  def forward(self, x):
    if self.BN_model==None:
      z1=self.linear1(x)
      p1=self.act_fun(z1)
      z2=self.linear2(p1)
      p2=self.act_fun(z2)
      z3=self.linear3(p2)
      p3=self.act_fun(z3)
      z4=self.linear4(p3)
      p4=self.act_fun(z4)
      out=self.linear5(p4)
    elif self.BN_model=='pre':
      z1=self.normalize1(self.linear1(x))
      p1=self.act_fun(z1)
      z2=self.normalize2(self.linear2(p1))
      p2=self.act_fun(z2)
      z3=self.normalize3(self.linear3(p2))
      p3=self.act_fun(z3)
      z4=self.normalize4(self.linear4(p3))
      p4=self.act_fun(z4)
      out=self.linear5(p4)
    elif self.BN_model=='post':
      z1=self.linear1(x)
      p1=self.act_fun(z1)
      z2=self.linear2(self.normalize1(p1))
      p2=self.act_fun(z2)
      z3=self.linear3(self.normalize2(p2))
      p3=self.act_fun(z3)
      z4=self.linear4(self.normalize3(p3))
      p4=self.act_fun(z4)
      out=self.linear5(self.normalize4(p4))
    return out
```
## 3.1 Batch Normalization and Batch_size fine tune

如果running_mean和running_var越准确，则建模效果会越好。因此batch_size参数需要被考虑进去。
在使用BN时，至少需要保证小批量数据batch_size在15-30以上，才能进行相对准确的预估。
```python
torch.manual_seed(420)

features, labels=tensorGenReg(w=[2,-1], bias=False, deg=2)

train_loader, test_loader = split_loader(features, labels, batch_size=50)

torch.manual_seed(24)

sigmoid_model1=net_class1(act_fun=torch.sigmoid)
sigmoid_model1_norm=net_class1(act_fun=torch.sigmoid, BN_model='pre')

model_l=[sigmoid_model1, sigmoid_model1_norm]
name_l=['sigmoid_model1', 'sigmoid_model1_norm']

lr=0.03
num_epochs=40

train_l, test_l=model_comparison(model_l=model_l,
                                 name_l=name_l,
                                 train_data=train_loader,
                                 test_data=test_loader,
                                 num_epochs=num_epochs,
                                 criterion=nn.MSELoss(),
                                 optimizer=optim.SGD,
                                 lr=lr,
                                 cla=False,
                                 eva=mse_cal)


for i,name in enumerate(name_l):
  plt.plot(list(range(num_epochs)), train_l[i], label=name)
plt.legend(loc=1)
plt.title('mse_train')
```

## 3.2 Batch Normalization results for complicated models

```python
torch.manual_seed(420)

features, labels=tensorGenReg(w=[2,-1,3,1,2], bias=False, deg=2)

train_loader, test_loader=split_loader(features, labels, batch_size=50)

torch.manual_seed(24)

sigmoid_model1=net_class1(act_fun=torch.sigmoid, in_features=5)
sigmoid_model1_norm=net_class1(act_fun=torch.sigmoid, in_features=5, BN_model='pre')

model_ls1=[sigmoid_model1, sigmoid_model1_norm]
name_ls1=['sigmoid_model1', 'sigmoid_model1_norm']

lr=0.03
num_epochs=40

train_ls1, test_ls1 = model_comparison(model_l=model_ls1,
                                       name_l = name_ls1,
                                       train_data = train_loader,
                                       test_data = test_loader,
                                       num_epochs = num_epochs,
                                       criterion = nn.MSELoss(),
                                       optimizer = optim.SGD,
                                       lr = lr,
                                       cla = False,
                                       eva = mse_cal)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.manual_seed(24)

sigmoid_model2=net_class2(act_fun=torch.sigmoid, in_features=5)
sigmoid_model2_norm=net_class2(act_fun=torch.sigmoid, in_features=5, BN_model='pre')

model_ls1=[sigmoid_model2, sigmoid_model2_norm]
name_ls1=['sigmoid_model2', 'sigmoid_model2_norm']

lr=0.03
num_epochs=40

train_ls2, test_ls2 = model_comparison(model_l=model_ls2,
                                       name_l = name_ls2,
                                       train_data = train_loader,
                                       test_data = test_loader,
                                       num_epochs = num_epochs,
                                       criterion = nn.MSELoss(),
                                       optimizer = optim.SGD,
                                       lr = lr,
                                       cla = False,
                                       eva = mse_cal)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.manual_seed(24)

sigmoid_model3=net_class3(act_fun=torch.sigmoid, in_features=5)
sigmoid_model3_norm=net_class3(act_fun=torch.sigmoid, in_features=5, BN_model='pre')

model_ls1=[sigmoid_model3, sigmoid_model3_norm]
name_ls1=['sigmoid_model3', 'sigmoid_model3_norm']

lr=0.03
num_epochs=40

train_ls3, test_ls3 = model_comparison(model_l=model_ls3,
                                       name_l = name_ls3,
                                       train_data = train_loader,
                                       test_data = test_loader,
                                       num_epochs = num_epochs,
                                       criterion = nn.MSELoss(),
                                       optimizer = optim.SGD,
                                       lr = lr,
                                       cla = False,
                                       eva = mse_cal)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.manual_seed(24)

sigmoid_model4=net_class4(act_fun=torch.sigmoid, in_features=5)
sigmoid_model4_norm=net_class4(act_fun=torch.sigmoid, in_features=5, BN_model='pre')

model_ls1=[sigmoid_model4, sigmoid_model4_norm]
name_ls1=['sigmoid_model4', 'sigmoid_model4_norm']

lr=0.03
num_epochs=40

train_ls4, test_ls4 = model_comparison(model_l=model_ls4,
                                       name_l = name_ls4,
                                       train_data = train_loader,
                                       test_data = test_loader,
                                       num_epochs = num_epochs,
                                       criterion = nn.MSELoss(),
                                       optimizer = optim.SGD,
                                       lr = lr,
                                       cla = False,
                                       eva = mse_cal)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

plt.subplot(221)
for i,name in enumerate(name_ls1):
  plt.plot(list(range(num_epochs)), train_ls1[i], label=name)
plt.legend(loc=1)
plt.title('mse_train_ls1')

plt.subplot(222)
for i,name in enumerate(name_ls2):
  plt.plot(list(range(num_epochs)), train_ls2[i], label=name)
plt.legend(loc=1)
plt.title('mse_train_ls2')

plt.subplot(223)
for i,name in enumerate(name_ls3):
  plt.plot(list(range(num_epochs)), train_ls3[i], label=name)
plt.legend(loc=1)
plt.title('mse_train_ls3')

plt.subplot(224)
for i,name in enumerate(name_ls4):
  plt.plot(list(range(num_epochs)), train_ls4[i], label=name)
plt.legend(loc=1)
plt.title('mse_train_ls4')

#~~~~~~~~~~~~~~~~~~

plt.subplot(221)
for i,name in enumerate(name_ls1):
  plt.plot(list(range(num_epochs)), test_ls1[i], label=name)
plt.legend(loc=1)
plt.title('mse_test_ls1')

plt.subplot(222)
for i,name in enumerate(name_ls2):
  plt.plot(list(range(num_epochs)), test_ls2[i], label=name)
plt.legend(loc=1)
plt.title('mse_test_ls2')

plt.subplot(223)
for i,name in enumerate(name_ls3):
  plt.plot(list(range(num_epochs)), test_ls3[i], label=name)
plt.legend(loc=1)
plt.title('mse_test_ls3')

plt.subplot(224)
for i,name in enumerate(name_ls4):
  plt.plot(list(range(num_epochs)), test_ls4[i], label=name)
plt.legend(loc=1)
plt.title('mse_test_ls4')
```
可以看出，越复杂的模型对于梯度不平稳的问题就越明显，因此BN层在解决该问题后模型效果提升就越明显。

对于上面Sigmoid函数，BN层能很大程度上缓解梯度消失问题，从而提升模型收敛速度，并且小幅度提升模型效果。而对于激活函数本身就能输出Zero-Centered结果的tanh函数，BN层的优化效果会更好。

## 3.3 Learning rate optimization with BN layers

加入BN层之后，可以增加模型复杂度，从而有更大空间来调参优化。其中加入BN之后，调整学习率就是随之而来的一种优化方式。当不加BN层的时候，单纯调整学习率收效甚微。

## 3.4 Fine tune summary for neural networks with Batch Normalization
- 简单数据、简单模型下不用BN层，加入BN层效果并不显著
- BN层的使用需要保持running_mean和running_var的无偏性，因此需要谨慎调整batch_size
- 学习率时重要的模型优化的超参数，一般来说学习率曲线都是U型曲线
- 从学习率调整角度出发，对于加入BN层的模型，学习率调整更加有效；对于带BN层模型角度来说，BN层能够帮助模型扩展优化空间，使得很多优化方法都能在原先无效的模型上生效
- 对于复杂问题，在计算能力能够承担的范围内，应当首先构建带BN层的复杂模型，然后再试图进行优化，就像上文所述，很多优化方法只对带BN层的模型有效
- 关于BN和Xavier/Kaiming方法，一般来说，使用BN层的模型不再会用参数初始化方法，从理论上来看添加BN层能够起到参数初始化的相等效果；（另外，带BN层模型一般也不需要使用dropout方法）
- 本节尚未讨论ReLU激活函数的优化，需要知道的是，对于ReLU叠加的模型来说，加入BN层之后能够有效缓解Dead ReLU Problem, 此时无需可以调小学习率，能够在收敛速度和运算结果间保持较好的平衡
- BN层目前大部分深度学习模型的标配，但前提是你有能力去对其进行优化






</details>
