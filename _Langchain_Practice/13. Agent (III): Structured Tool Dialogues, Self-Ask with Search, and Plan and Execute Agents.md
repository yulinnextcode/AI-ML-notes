We have delved into the internal mechanisms of the LangChain program, exploring how the AgentExecutor thinks (Thought), executes (Execute/Act), and observes (Observe). The close connections between these steps are the “life and death causality” in the agent’s reasoning and tool invocation process.

Now, let’s strike while the iron is hot and learn about several more complex agents: **Structured Tool Chat agents, Self-Ask with Search agents, and Plan and Execute agents**.

## What is structural tool

The first version of LangChain was launched in November 2022, designed based on the ReAct paper, primarily focusing on the use of agents and tools, integrating them into the prompt template framework.

Early tool usage was quite simple. When the AgentExecutor guided the model to call tools through reasoning, it could only generate two parts: the tool’s name and the content to be input into the tool. Moreover, in each round, the agent was only allowed to use one tool, and the input content could only be a simple string. This simplified design was intended to make the model’s tasks easier, as performing complex operations could make the execution process less stable.

However, with the development of language models, especially with the emergence of models like GPT-3.5-turbo and GPT-4, reasoning capabilities have gradually improved, providing higher stability and feasibility for agents. This led LangChain to consider relaxing the restrictions on tool usage.

In early 2023, LangChain introduced the “multi-action” agent framework, allowing agents to plan and execute multiple actions. Based on this, LangChain launched the structured tool dialogue agent, allowing for more complex and multifaceted interactions. By specifying the AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent type, agents can call a “structured toolbox” containing a series of complex tools, combining multiple tools to complete a batch of related tasks.

Examples of structured tools include:

1. File management toolset: Supports all file system operations, such as writing, searching, moving, copying, listing directories, and finding files.
2. Web browser toolset: The official PlayWright browser toolkit allows agents to access websites, click, submit forms, and query data.
Next, let’s take the PlayWright toolkit as an example to implement a structured tool dialogue agent.

First, let’s see what the PlayWright toolkit is.

## What is Playwright

Playwright is an open-source automation framework that allows you to simulate real user interactions with web pages, helping developers and testers automate web interactions and testing. Simply put, it’s like a “robot” that can browse web pages, click buttons, fill out forms, read page content, and more, just like a real user using a browser.

Playwright supports multiple browsers, such as Chrome, Firefox, and Safari, which means you can use it to test your website or application to ensure consistent performance across different browsers.

First, let’s install the Playwright tool using pip install playwright.

However, if you only install the Playwright package with pip, it won’t be enough to use it, and you’ll get the following message.

![](images/708511/5cb10de270599b427a4efa9655ceb1c7.jpg)

Therefore, we also need to install the three commonly used browser tools using the playwright install command.

![](images/708511/335f98d28232d1a7160f1d48f334d56d.jpg)

Now, everything is ready, and we can use the Playwright browser tool to access a test web page.

```plain
from playwright.sync_api import sync_playwright

def run():
    # 使用Playwright上下文管理器
    with sync_playwright() as p:
        # 使用Chromium，但你也可以选择firefox或webkit
        browser = p.chromium.launch()

        # 创建一个新的页面
        page = browser.new_page()

        # 导航到指定的URL
        page.goto('https://langchain.com/')

        # 获取并打印页面标题
        title = page.title()
        print(f"Page title is: {title}")

        # 关闭浏览器
        browser.close()

if __name__ == "__main__":
    run()

```

这个简单的Playwright脚本，它打开了一个新的浏览器实例。过程是：导航到指定的URL；获取页面标题并打印页面的标题；最后关闭浏览器。

输出如下：

```plain
Page title is: LangChain

```

This script demonstrates how Playwright works, all done directly in the command line. It doesn’t require us to actually open the Chrome browser and manually click on the menu bar, drag the progress bar, etc.

The table below lists the advantages of using the command line for automated web testing.

![](images/708511/0a5909f879b043b5f17d7c8ea5a88a20.jpg)

Now that you understand the basic concept of the Playwright toolkit, let’s use it as a toolset to implement a structured tool dialogue agent.

Using Structured Tool Dialogue Agent
Here, we will use the Agent type STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION. The tool to be used is the PlayWrightBrowserToolkit, a toolkit in LangChain based on the PlayWrightBrowser package, inheriting from the BaseToolkit class.

The PlayWrightBrowserToolkit provides a series of interactive tools for the PlayWright browser, which can operate in synchronous or asynchronous modes.

The specific tools included are:

![](images/708511/ce51ayya392733c6b55ec3568caaac46.jpg)

Next, let’s see how the structured tool dialogue agent automatically completes the tasks we assign to it by combining various tools from the PlayWrightBrowserToolkit.

```plain
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import create_async_playwright_browser

async_browser = create_async_playwright_browser()
toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = toolkit.get_tools()
print(tools)

from langchain.agents import initialize_agent, AgentType
from langchain.chat_models import ChatAnthropic, ChatOpenAI

# LLM不稳定，对于这个任务，可能要多跑几次才能得到正确结果
llm = ChatOpenAI(temperature=0.5)

agent_chain = initialize_agent(
    tools,
    llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)

async def main():
    response = await agent_chain.arun("What are the headers on python.langchain.com?")
    print(response)

import asyncio
loop = asyncio.get_event_loop()
loop.run_until_complete(main())

```
这个简单的Playwright脚本，它打开了一个新的浏览器实例。过程是：导航到指定的URL；获取页面标题并打印页面的标题；最后关闭浏览器。

输出如下：

```plain
Page title is: LangChain

```
这个脚本展示了Playwright的工作方式，一切都是在命令行里面直接完成。它不需要我们真的去打开Chome网页，然后手工去点击菜单栏、拉动进度条等。

下面这个表，我列出了使用命令行进行自动化网页测试的优势。

![](images/708511/0a5909f879b043b5f17d7c8ea5a88a20.jpg)

现在你了解了Playwright这个工具包的基本思路，下面我们就开始使用它来作为工具集，来实现结构化工具对话代理。

## Use structural tool kits conversational agent

Here, we will use the Agent type STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION. The tool to be used is the PlayWrightBrowserToolkit, a toolkit in LangChain based on the PlayWrightBrowser package, inheriting from the BaseToolkit class.

The PlayWrightBrowserToolkit provides a series of interactive tools for the PlayWright browser, which can operate in synchronous or asynchronous modes.

The specific tools included are:

![](images/708511/ce51ayya392733c6b55ec3568caaac46.jpg)

Next, let’s see how the structured tool dialogue agent automatically completes the tasks we assign to it by combining various tools from the PlayWrightBrowserToolkit.


```plain
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import create_async_playwright_browser

async_browser = create_async_playwright_browser()
toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = toolkit.get_tools()
print(tools)

from langchain.agents import initialize_agent, AgentType
from langchain.chat_models import ChatAnthropic, ChatOpenAI

# LLM不稳定，对于这个任务，可能要多跑几次才能得到正确结果
llm = ChatOpenAI(temperature=0.5)

agent_chain = initialize_agent(
    tools,
    llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)

async def main():
    response = await agent_chain.arun("What are the headers on python.langchain.com?")
    print(response)

import asyncio
loop = asyncio.get_event_loop()
loop.run_until_complete(main())

```

In this example, we ask the large model what headings are on the webpage python.langchain.com.

Obviously, the large model cannot contain the internal information of this webpage because LangChain did not exist when ChatGPT completed its training in September 2021. Therefore, the large model inevitably needs to use the PlayWrightBrowser tool to solve the problem.

### First Round of Thinking
The first round of thinking after the agent enters the AgentExecutor Chain is as follows:

> Thought: I can use the “navigate\_browser” tool to visit the website and then use the “get\_elements” tool to retrieve the headers. Let me do that.
> Action:\`\`\`{“action”: “navigate\_browser”, “action\_input”: {“url”: “ [https://python.langchain.com](https://python.langchain.com)”}}\`\`\`
> Observation: Navigating to [https://python.langchain.com](https://python.langchain.com) returned status code 200

![](images/708511/6a5718eef084ac988a23e5488e967302.jpg)

### Second Round of Thinking

![](images/708511/663de1fda23de782af9233328ca5c2e5.jpg)

> Thought:Now that I have successfully navigated to the website, I can use the “get\_elements” tool to retrieve the headers. I will specify the CSS selector for the headers and retrieve their text.

> Action: \`\`\`{“action”: “get\_elements”, “action\_input”: {“selector”: “h1, h2, h3, h4, h5, h6”, “attributes”: \[“innerText”\]}}\`\`\`

> Observation: \[{“innerText”: “Introduction”}, {“innerText”: “Get started​”}, {“innerText”: “Modules​”}, {“innerText”: “Model I/O​”}, {“innerText”: “Data connection​”}, {“innerText”: “Chains​”}, {“innerText”: “Agents​”}, {“innerText”: “Memory​”}, {“innerText”: “Callbacks​”}, {“innerText”: “Examples, ecosystem, and resources​”}, {“innerText”: “Use cases​”}, {“innerText”: “Guides​”}, {“innerText”: “Ecosystem​”}, {“innerText”: “Additional resources​”}, {“innerText”: “Support”}, {“innerText”: “API reference​”}\]

### Thrid Round of Thinking

> Thought:The headers on python.langchain.com are:
>
> 1. Introduction
>
>
>    … …
> 2. API reference

第三轮思考：模型已经找到了网页中的所有标题。

> Action:
>
> ```
> {
>   "action": "Final Answer",
>   "action_input": "The headers on python.langchain.com are: 1. Introduction 2. Get started 3. Modules 4. Model I/O 5. Data connection 6. Chains 7. Agents 8. Memory 9. Callbacks 10. Examples, ecosystem, and resources 11. Use cases 12. Guides 13. Ecosystem 14. Additional resources 15. Support 16. API reference"
> }
>
> ```

AgentExecutor Chain结束之后，成功输出python.langchain.com这个页面中各级标题的具体内容。

![](images/708511/961a5c0cc2b9c19d7147b2120608a663.jpg)

在这个过程中，结构化工具代理组合调用了Playwright工具包中的两种不同工具，自主完成了任务。

## Use Self-Ask with Search Agent

Having covered the Structured Tool Chat agent, let’s take a look at the Self-Ask with Search agent.

Self-Ask with Search is another useful agent type in LangChain (SELF_ASK_WITH_SEARCH). It uses a technique called “Follow-up Question” combined with “Intermediate Answer” to help the large model find transitional answers to factual questions, leading to the final answer.

What does this mean? Let me demonstrate with an example, and you’ll understand. In this example, we use SerpAPIWrapper as the tool and OpenAI as the language model to create a Self-Ask with Search agent.

```plain
from langchain import OpenAI, SerpAPIWrapper
from langchain.agents import initialize_agent, Tool
from langchain.agents import AgentType

llm = OpenAI(temperature=0)
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Intermediate Answer",
        func=search.run,
        description="useful for when you need to ask with search",
    )
]

self_ask_with_search = initialize_agent(
    tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True
)
self_ask_with_search.run(
    "使用玫瑰作为国花的国家的首都是哪里?"
)

```

![](images/708511/dd6dcfa6c90384abc80640fe5ea1850d.jpg)

Actually, you might have noticed that the question “What is the capital of the country that uses the rose as its national flower?” is not a simple one. It is a multi-hop question—there is an intermediate process between the question and the final answer.

A multi-hop question requires multiple steps of reasoning or multiple queries to obtain the final answer. Such questions cannot be answered directly through a single query or information source; instead, they require crossing multiple information points or combining and integrating data from various sources.

In other words, the answer to the question depends on the answer to another sub-question, which in turn may depend on the answer to yet another question. This is like a series of question jumps. For humans, answering such questions may require finding a series of intermediate answers from different information sources and then combining these intermediate answers to reach the final conclusion.

The question “What is the capital of the country that uses the rose as its national flower?” does not directly ask which country uses the rose as its national flower, nor does it directly ask what the capital of the UK is. Instead, it first requires identifying the country that uses the rose as its national flower (the UK) and then further asking what the capital of that country is. This requires multi-hop querying.

Why is the Self-Ask with Search agent suitable for solving multi-hop questions? Here are a few reasons:

- 1. Toolset: The agent includes the necessary search tools to solve the problem, which can be used to query and verify multiple information points. In this case, we equipped the agent with the SerpAPIWrapper tool in the program.
- 2. Step-by-step approach: The agent can ask further questions based on the answer to the first question until it reaches the final answer. This step-by-step approach ensures the accuracy of the answer.
- 3. Self-questioning and searching: The agent can ask questions and search for answers on its own. For example, it can first determine which country uses the rose as its national flower and then determine what the capital of that country is.
- 4. Decision chain: The agent executes tasks through a decision chain, allowing it to track and handle complex multi-hop questions, which is crucial for solving problems that require multi-step reasoning.
In the example above, through two follow-up questions by the large model, the search tool provided two intermediate answers, and finally, the model gave the final answer to the question—London.

## Use Plan and execute Agent

In this final lesson, let me introduce you to a relatively new type of agent: the Plan and Execute agent.

The Plan and Execute agent achieves its goals by first planning what to do and then executing subtasks. This idea is inspired by the Plan-and-Solve paper. The paper proposes a Plan-and-Solve prompt, which consists of two parts: first, creating a plan and breaking the overall task into smaller subtasks; then, executing these subtasks according to the plan.

The uniqueness of this agent lies in the fact that its planning and execution are no longer carried out by the same agent. Instead:

- The planning is done by one large language model agent (responsible for reasoning).
- The execution is done by another large language model agent (responsible for tool invocation).
Since this agent is relatively new, it belongs to LangChain’s experimental package langchain_experimental, so you need to install the langchain_experimental package first.

```plain
pip install -U langchain langchain_experimental

```

下面我们来使用一下这个代理。在这里，我们创建了Plan and execute代理，这个代理和之前看到的代理不同，它有一个Planner，有一个Executor，它们可以是不同的模型。

当然，在这个示例中，我们都使用了ChatOpenAI模型。

```plain
from langchain.chat_models import ChatOpenAI
from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner
from langchain.llms import OpenAI
from langchain import SerpAPIWrapper
from langchain.agents.tools import Tool
from langchain import LLMMathChain

search = SerpAPIWrapper()
llm = OpenAI(temperature=0)
llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)
tools = [
    Tool(
        name = "Search",
        func=search.run,
        description="useful for when you need to answer questions about current events"
    ),
    Tool(
        name="Calculator",
        func=llm_math_chain.run,
        description="useful for when you need to answer questions about math"
    ),
]
model = ChatOpenAI(temperature=0)
planner = load_chat_planner(model)
executor = load_agent_executor(model, tools, verbose=True)
agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)

agent.run("在纽约，100美元能买几束玫瑰?")

```

输出如下：

![](images/708511/fd28e5f19a6a8b8ef9c4d68b3e5c0d38.jpg)

![](images/708511/8ea16266717acf88a2fedb72283744b2.jpg)

在上面输出中，PlanAndExecute 链的调用流程以及代理的思考过程，我就留给你来做分析了，相信你可以把握住Plan and execute代理解决问题的基本脉络。

## Summary

This lesson is the final one on Agents and also the last lesson on all the foundational knowledge of LangChain. I’ve summarized two tables for you.

The first table introduces common agent types in LangChain and their descriptions. Among these agents, many we have already used together, while others you may need to explore on your own by reading the relevant documentation.

![](/_Langchain_Practice/img/708511/ee248367eef96616690831498519eeae.jpg)

The second table is a concise summary of the various components of LangChain.

![](/_Langchain_Practice/img/708511/e26993dd3957bfd2947424abb9de7cde.png)

![](/_Langchain_Practice/img/708511/577333985abb70b890d94bf99fe58ed9.jpg)

By now, you should be familiar with these images and have grasped their essence.

Lastly, there’s one more topic worth discussing: the “Indexes” in the diagram. In fact, “Indexes” was a component in the early versions of LangChain, but it has now been integrated into the Retrieval unit. Retrieval (including Indexes) essentially involves embedding and storing discrete documents and other information into a vector database, and then retrieving them. We covered this process in Lesson 3, and it will be further explored in future lessons.

![](/_Langchain_Practice/img/708511/e3yyf61d8ccc0b2ba47a76dfc1fdf190.jpg)

Additionally, in the new six components of LangChain documentation, there is another module—Callbacks—which we haven’t covered yet but will be introduced in subsequent lessons.

Alright, that’s all for the foundational knowledge of LangChain. Starting from the next lesson, we will integrate the components we’ve learned so far and delve into more application-oriented content.









