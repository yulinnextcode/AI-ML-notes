
## Project framework

![](/_Langchain_Practice/img/699436/c66995f1bf8575fb8fyye6293200eabf.jpg)

The entire framework is divided into three parts:

- Data Sources: Data can come in many forms, including unstructured data like PDFs, structured data like SQL, and code such as Python or Java. In this example, we focus on processing unstructured data.
- Application (LLM App): Using a large language model as the logic engine to generate the answers we need.
- Use-Cases: The answers generated by the large language model can be used to build systems like QA/chatbots.

Core Implementation Mechanism: The core implementation mechanism of this project is the data processing pipeline shown in the diagram below.

![](/_Langchain_Practice/img/699436/73a46eecd42038961db9067e75de3387.jpg)

The specific process is divided into the following five steps:

Loading: The document loader loads documents into a format that LangChain can read.
Splitting: The text splitter splits the documents into segments of specified sizes, which I call ‘document chunks’ or ‘document pieces.’
Storage: The ‘document chunks’ from the previous step are stored in a vector database (Vector DB) as ‘embeddings,’ forming ‘embedding pieces.’
Retrieval: The application retrieves the split documents from storage (e.g., by comparing cosine similarity to find embedding pieces similar to the input question).
Output: The question and similar embedding pieces are passed to the language model (LLM), which uses the prompt containing the question and retrieved segments to generate an answer.
The above five steps are introduced very simply. Some concepts (such as embeddings and vector storage) appear for the first time and require some background knowledge to understand. Don’t worry, we will explain these five steps in detail next.

## Data Load

First, we use document_loaders in LangChain to load various formats of text files. (I placed these files in the OneFlower directory. If you create your own folder, you need to adjust the directory in the code.)

In this step, we load text from PDF, Word, and TXT files, and then store these texts in a list. (Note: You may need to install libraries such as PyPDF and Docx2txt.)

```plain
import os
os.environ["OPENAI_API_KEY"] = '你的Open AI API Key'

# 1.Load 导入Document Loaders
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import Docx2txtLoader
from langchain.document_loaders import TextLoader

# 加载Documents
base_dir = '.\OneFlower' # 文档的存放目录
documents = []
for file in os.listdir(base_dir):
    # 构建完整的文件路径
    file_path = os.path.join(base_dir, file)
    if file.endswith('.pdf'):
        loader = PyPDFLoader(file_path)
        documents.extend(loader.load())
    elif file.endswith('.docx'):
        loader = Docx2txtLoader(file_path)
        documents.extend(loader.load())
    elif file.endswith('.txt'):
        loader = TextLoader(file_path)
        documents.extend(loader.load())

```

Here we first import the OpenAI API Key. Because later we need to use two different models from OpenAI to do the following two things:

- Use OpenAI’s Embedding model to embed the documents.
- Call OpenAI’s GPT model to generate answers in the Q&A system.

Of course, the large models supported by LangChain are not limited to OpenAI. You can follow this framework and replace both the Embedding model and the language model responsible for generating answers with other open-source models.

When running the above program, in addition to importing the correct OpenAI Key, you should also pay attention to the installation of toolkits. When using LangChain, various toolkits are often needed depending on the specific task (for example, the above code requires PyPDF and Docx2txt tools). They are very easy to install. If the program reports that a package is missing, just install the relevant package using pip install.

## Documents Split

Next, we need to split the loaded text into smaller chunks for embedding and vector storage. In this step, we use the RecursiveCharacterTextSplitter from LangChain to split the text.

```plain
# 2.Split 将Documents切分成块以便后续进行嵌入和向量存储
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)
chunked_documents = text_splitter.split_documents(documents)

```

Now, our documents have been split into document chunks of about 200 characters each. This step prepares them for storage in the vector database.

## Vector database storage

Next, we convert these split texts into embeddings and store them in a vector database. In this example, we use OpenAIEmbeddings to generate the embeddings and then use the Qdrant vector database to store them (you need to pip install qdrant-client for this).

```plain
# 3.Store 将分割嵌入并存储在矢量数据库Qdrant中
from langchain.vectorstores import Qdrant
from langchain.embeddings import OpenAIEmbeddings
vectorstore = Qdrant.from_documents(
    documents=chunked_documents, # 以分块的文档
    embedding=OpenAIEmbeddings(), # 用OpenAI的Embedding Model做嵌入
    location=":memory:",  # in-memory 存储
    collection_name="my_documents",) # 指定collection_name

```

## Information retrival

Once the internal documents are stored in the vector database, we need to extract the most relevant information based on the question and task. At this point, the basic method of information extraction is to convert the question into a vector as well, and then compare it with the vectors in the vector database to extract the closest information.

Comparison between vectors is usually based on vector distance or similarity. In high-dimensional space, commonly used methods for calculating vector distance or similarity include Euclidean distance and cosine similarity.

- Euclidean Distance: This is the most direct distance measurement method, similar to measuring the straight-line distance between two points on a two-dimensional plane. In high-dimensional space, the Euclidean distance between two vectors is the square root of the sum of the squares of the differences in each corresponding dimension.
- Cosine Similarity: In many cases, we are more concerned with the direction of the vector rather than its magnitude. For example, in text processing, the vector of a word may have a large difference in magnitude due to the length of the text, but the direction better reflects its semantics. Cosine similarity measures the similarity in direction between vectors, with values ranging from -1 to 1. The closer the value is to 1, the more similar the directions of the two vectors.

Of course, you might ask, when should you choose Euclidean distance and when should you choose cosine similarity?

In simple terms, use Euclidean distance when you care about differences in magnitude, and use cosine similarity when you care about semantic differences in text.

Specifically, Euclidean distance measures absolute distance and can reflect the absolute differences between vectors well. When we care about the absolute size of data, such as in a product recommendation system where the user’s purchase volume might reflect their preference strength, Euclidean distance can be considered. Similarly, when the sizes of vectors in the dataset are similar and the data distribution is roughly uniform, Euclidean distance is also suitable.

Cosine similarity measures the similarity in direction and is more concerned with the angular difference between vectors rather than their magnitude. It is particularly useful when dealing with text data or other high-dimensional sparse data. For example, in tasks like information retrieval and text classification, text data is often represented as high-dimensional word vectors, and the direction of the word vectors better reflects their semantic similarity. In such cases, cosine similarity can be used.

In this case, we are dealing with text data, and the goal is to build a Q&A system that needs to understand and compare potential answers semantically. Therefore, I recommend using cosine similarity as the metric. By comparing the direction of the question and answer vectors in the semantic space, we can find the answer that best matches the question.

In the code for this step, we will create a chat model. Then, we need to create a RetrievalQA chain, which is a retrieval-based Q&A model used to generate answers to questions.

The RetrievalQA chain consists of the following two important components.

- LLM: The large language model responsible for answering questions.
- Retriever (vectorstore.as_retriever()): Responsible for retrieving relevant documents based on the question, finding specific “embedding pieces.” These “embedding pieces” correspond to “document chunks” that will be passed along with the question into the large model as knowledge information. The knowledge retrieved from local documents is crucial because a large model trained on internet information cannot possess the internal knowledge of “EasySpeed Flowers” as a private enterprise.

```plain
# 4. Retrieval 准备模型和Retrieval链
import logging # 导入Logging工具
from langchain.chat_models import ChatOpenAI # ChatOpenAI模型
from langchain.retrievers.multi_query import MultiQueryRetriever # MultiQueryRetriever工具
from langchain.chains import RetrievalQA # RetrievalQA链

# 设置Logging
logging.basicConfig()
logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)

# 实例化一个大模型工具 - OpenAI的GPT-3.5
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# 实例化一个MultiQueryRetriever
retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)

# 实例化一个RetrievalQA链
qa_chain = RetrievalQA.from_chain_type(llm,retriever=retriever_from_llm)

```

Now that we are prepared for the subsequent steps, the next step is to receive specific questions from the system user, retrieve information based on the questions, and generate answers.

## Generate answers and display

This step is the main UI interaction part of the Q&A system application. Here, a Flask application (you need to install the Flask package) is created to receive user questions and generate corresponding answers, which are then rendered and presented through index.html.

In this step, we use the previously created RetrievalQA chain to retrieve relevant documents and generate answers. Then, this information is returned to the user and displayed on the webpage.

```plain
# 5. Output 问答系统的UI实现
from flask import Flask, request, render_template
app = Flask(__name__) # Flask APP

@app.route('/', methods=['GET', 'POST'])
def home():
    if request.method == 'POST':

        # 接收用户输入作为问题
        question = request.form.get('question')

        # RetrievalQA链 - 读入问题，生成答案
        result = qa_chain({"query": question})

        # 把大模型的回答结果返回网页进行渲染
        return render_template('index.html', result=result)

    return render_template('index.html')

if __name__ == "__main__":
    app.run(host='0.0.0.0',debug=True,port=5000)

```

```plain
<body>
    <div class="container">
        <div class="header">
            <h1>易速鲜花内部问答系统</h1>
            <img src="{{ url_for('static', filename='flower.png') }}" alt="flower logo" width="200">
        </div>
        <form method="POST">
            <label for="question">Enter your question:</label><br>
            <input type="text" id="question" name="question"><br>
            <input type="submit" value="Submit">
        </form>
        {% if result is defined %}
            <h2>Answer</h2>
            <p>{{ result.result }}</p>
        {% endif %}
    </div>
</body>

```

The project structure is as follows:

![](/_Langchain_Practice/img/699436/2110cd73ddb8677f9b188d41c589c73e.png)

After running the program, we launch a webpage at http://127.0.0.1:5000/. When interacting with the webpage, you will find that the Q&A system perfectly generates answers specific to the internal information of EasySpeed Flowers.
![](/_Langchain_Practice/img/699436/46b5b08c5f022f2c4c5975436b3e2d17.png)

## Summary

Let’s review the above process. As shown in the diagram below, we first slice the local knowledge and create embeddings, store them in the vector database, then pass the user’s input and the retrieved local knowledge from the vector database to the large model, and finally generate the desired answer.

![](/_Langchain_Practice/img/699436/249c631211275e40f3e72d05dda976af.jpg)

I will delve into the components of LangChain, including models, chains, memory, and agents. I will guide you through implementing more tasks and developing even more amazing applications.

