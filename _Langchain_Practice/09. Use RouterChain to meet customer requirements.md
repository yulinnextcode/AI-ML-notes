## Task

First, let’s take a look at the task we need to complete today.

Let’s assume that our flower operation intelligent customer service ChatBot usually receives two main types of questions.

- 1. Flower Care (keeping flowers healthy, how to water, fertilize, etc.)
- 2. Flower Decoration (how to arrange flowers, how to decorate venues, etc.)
Your requirement is that if the first type of question is received, you need to give instructions to ChatBot A; if the second type of question is received, you need to give instructions to ChatBot B.

![](images/703556/d8491e696c03f49a331c94e31d20e559.jpg)

We can construct two different target chains based on these two scenarios. When encountering different types of questions, LangChain will automatically guide the large language model to select different templates through the RouterChain.

Of course, our operation process will encounter more types of questions, and you only need to expand the logic in the same way.

## Overall Framework
RouterChain, also known as a routing chain, can dynamically select the next chain for a given input. We will use the router chain to determine which processing template is more suitable for the question based on the user’s question content, and then send the question to that processing template for an answer. If the question does not fit any of the defined processing templates, it will be sent to the default chain.

Here, we will use LLMRouterChain and MultiPromptChain (also a routing chain) to implement the routing function. The MultiPromptChain will call the LLMRouterChain to select the most relevant prompt for the given question and then use that prompt to answer the question.

## Specific steps are as follows:

- 1. Construct Processing Templates: Define two string templates for flower care and flower decoration respectively.
- 2. Prompt Information: Use a list to organize and store the key information of these two processing templates, such as the template’s key, description, and actual content.
- 3. Initialize Language Model: Import and instantiate the language model.
- 4. Construct Target Chains: Construct corresponding LLMChain for each template in the prompt information and store them in a dictionary.
- 5. Construct LLM Router Chain: This is the core part of the decision-making process. First, it constructs a routing template based on the prompt information, and then uses this template to create an LLMRouterChain.
- 6. Construct Default Chain: If the input does not fit any of the defined processing templates, this default chain will be triggered.
- 7. Construct Multi-Prompt Chain: Use MultiPromptChain to combine the LLM Router Chain, target chains, and default chain into a complete decision-making system.

## Specific Implementation
Below is the specific code implementation for using the routing chain to automatically select the processing template.

### Constructing Prompt Templates
First, we construct two prompt templates for the two scenarios.

```plain
# 构建两个场景的模板
flower_care_template = """你是一个经验丰富的园丁，擅长解答关于养花育花的问题。
                        下面是需要你来回答的问题:
                        {input}"""

flower_deco_template = """你是一位网红插花大师，擅长解答关于鲜花装饰的问题。
                        下面是需要你来回答的问题:
                        {input}"""

# 构建提示信息
prompt_infos = [
    {
        "key": "flower_care",
        "description": "适合回答关于鲜花护理的问题",
        "template": flower_care_template,
    },
    {
        "key": "flower_decoration",
        "description": "适合回答关于鲜花装饰的问题",
        "template": flower_deco_template,
    }]

```

### Initialize LLM
接下来，我们初始化语言模型。

```plain
# 初始化语言模型
from langchain.llms import OpenAI
import os
os.environ["OPENAI_API_KEY"] = '你的OpenAI Key'
llm = OpenAI()

```

### Construct target chain
下面，我们循环prompt\_infos这个列表，构建出两个目标链，分别负责处理不同的问题。

```plain
# 构建目标链
from langchain.chains.llm import LLMChain
from langchain.prompts import PromptTemplate
chain_map = {}
for info in prompt_infos:
    prompt = PromptTemplate(template=info['template'],
                            input_variables=["input"])
    print("目标提示:\n",prompt)
    chain = LLMChain(llm=llm, prompt=prompt,verbose=True)
    chain_map[info["key"]] = chain

```

这里，目标链提示是这样的：

```plain
目标提示:
input_variables=['input']
output_parser=None partial_variables={}
template='你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\n                        下面是需要你来回答的问题:\n
{input}' template_format='f-string'
validate_template=True

目标提示:
input_variables=['input']
output_parser=None partial_variables={}
template='你是一位网红插花大师，擅长解答关于鲜花装饰的问题。\n                        下面是需要你来回答的问题:\n
{input}' template_format='f-string'
validate_template=True

```

对于每个场景，我们创建一个 LLMChain（语言模型链）。每个链会根据其场景模板生成对应的提示，然后将这个提示送入语言模型获取答案。

### Construct router chain

下面，我们构建路由链，负责查看用户输入的问题，确定问题的类型。

```plain
# 构建路由链
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE as RounterTemplate
destinations = [f"{p['key']}: {p['description']}" for p in prompt_infos]
router_template = RounterTemplate.format(destinations="\n".join(destinations))
print("路由模板:\n",router_template)
router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser(),)
print("路由提示:\n",router_prompt)
router_chain = LLMRouterChain.from_llm(llm,
                                       router_prompt,
                                       verbose=True)

```

输出：

````
路由模板:
 Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.

<< FORMATTING >>
Return a markdown code snippet with a JSON object formatted to look like:
```json
{{
    "destination": string \ name of the prompt to use or "DEFAULT"
    "next_inputs": string \ a potentially modified version of the original input
}}
```

REMEMBER: "destination" MUST be one of the candidate prompt names specified below OR it can be "DEFAULT" if the input is not well suited for any of the candidate prompts.
REMEMBER: "next_inputs" can just be the original input if you don't think any modifications are needed.

<< CANDIDATE PROMPTS >>
flower_care: 适合回答关于鲜花护理的问题
flower_decoration: 适合回答关于鲜花装饰的问题

<< INPUT >>
{input}

<< OUTPUT >>

路由提示:
input_variables=['input'] output_parser=RouterOutputParser(default_destination='DEFAULT', next_inputs_type=<class 'str'>, next_inputs_inner_key='input')
partial_variables={}
template='Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n\n
<< FORMATTING >>\n
Return a markdown code snippet with a JSON object formatted to look like:\n```json\n{{\n "destination": string \\ name of the prompt to use or "DEFAULT"\n    "next_inputs": string \\ a potentially modified version of the original input\n}}\n```\n\n
REMEMBER: "destination" MUST be one of the candidate prompt names specified below OR it can be "DEFAULT" if the input is not well suited for any of the candidate prompts.\n
REMEMBER: "next_inputs" can just be the original input if you don\'t think any modifications are needed.\n\n<< CANDIDATE PROMPTS >>\n
flower_care: 适合回答关于鲜花护理的问题\n
flower_decoration: 适合回答关于鲜花装饰的问题\n\n
<< INPUT >>\n{input}\n\n<< OUTPUT >>\n'
template_format='f-string'
validate_template=True

````

### Construct default chain

In addition to the target chains and the routing chain, we also need to prepare a default chain. If the routing chain does not find a suitable chain, the default chain will handle the processing.

```plain
# 构建默认链
from langchain.chains import ConversationChain
default_chain = ConversationChain(llm=llm,
                                  output_key="text",
                                  verbose=True)

```

### Construct multiple prompt chain

Finally, we use the MultiPromptChain class to integrate the previous chains together to achieve the routing function. This MultiPromptChain class is a multi-choice chain that uses an LLM Router Chain to select between multiple prompts.

**There are three key elements in MultiPromptChain**

- router_chain (type RouterChain): This chain is used to determine the target chain and its input. Given an input, this router_chain decides which destination_chain should be selected and what specific input should be passed to it.
- destination_chains (type Mapping[str, LLMChain]): This is a mapping that maps names to candidate chains that can process the input. For example, you might have multiple methods (or “chains”) for processing text input, each targeting a specific type of question. destination_chains can be a dictionary like this: {'weather': weather_chain, 'news': news_chain}. Here, weather_chain might specifically handle weather-related questions, while news_chain handles news-related questions.
- default_chain (type LLMChain): When the router_chain cannot map the input to any of the destination_chains, the LLMChain will use this default chain. This is a fallback option to ensure that even if the router cannot decide the correct chain, there is always a chain to process the input.

**Its workflow is as follows:**

1. The input is first passed to the router_chain.
2. The router_chain decides which destination_chain to use based on certain criteria or logic.
3. The input is then routed to the selected destination_chain, which processes it and returns the result.
4. If the router_chain cannot decide the correct destination_chain, the input is passed to the default_chain.

In this way, MultiPromptChain provides us with a mechanism to dynamically route input among multiple processing chains to get the most relevant or optimal output.

Codes are as follows:
```plain
# 构建多提示链
from langchain.chains.router import MultiPromptChain
chain = MultiPromptChain(
    router_chain=router_chain,
    destination_chains=chain_map,
    default_chain=default_chain,
    verbose=True)

```

## Run router chain

**Test A：**

print(chain.run(“如何为玫瑰浇水？”))

输出：

![](images/703556/89d0bfac97b259b93240a10cf777d9a2.png)

**Test B：**

print(chain.run(“如何为婚礼场地装饰花朵？”))

输出：

![](images/703556/4f848ca6592476358a25bf91996aa0ed.png)

**Test C：**

print(chain.run(“如何考入哈佛大学？”))

输出：

![](images/703556/acd4a69df2cef81b1f7bcf33f9b4bb12.png)

These three tests were routed to three different target chains. Two of them were our preset “expert type” target chains, while the third question, “How to get into Harvard University?” was immediately recognized by the model as not belonging to any flower operation business scenario. The routing chain threw it into a “default chain” — ConversationChain to handle it.

## Summary

In this example, we saw LLMRouterChain and MultiPromptChain. The LLMRouterChain inherits from RouterChain, while the MultiPromptChain inherits from MultiRouteChain.

Overall, we organized other chains through the MultiPromptChain to complete the routing function.

```plain
chain = MultiPromptChain(
    router_chain=router_chain,
    destination_chains=chain_map,
    default_chain=default_chain,
    verbose=True)

```

在LangChain的 chains -> router -> base.py 文件中，可以看到RouterChain和MultiRouteChain的代码实现。






















