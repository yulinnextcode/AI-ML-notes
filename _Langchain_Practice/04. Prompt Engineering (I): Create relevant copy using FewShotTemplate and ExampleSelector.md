How to use prompt templates in LangChain to do prompt engineering well?

For prompt engineering with large models, Andrew Ng provided two main principles in his ChatGPT Prompt Engineering for Developers course: the first principle is to write clear and specific instructions, and the second principle is to give the model time to think.

Similarly, in OpenAI’s official documentation GPT Best Practices, six strategies are given that align with the above two principles:

1. Write clear instructions
2. Provide references (examples)
3. Break down complex tasks into subtasks
4. Give GPT time to think
5. Use external tools
6. Iterate on the prompt

## Prompt Structure

![](images/700699/b77a15cd83b66bba55032d711bcf3c16.png)

In this prompt framework:

- Instruction tells the model what the task is roughly about and how to do it, such as how to use the provided external information, how to handle queries, and how to construct the output. This is usually a fixed part of a prompt template. A common use case is to tell the model “You are a helpful XX assistant,” which makes it take its role more seriously.
- Context serves as an additional source of knowledge for the model. This information can be manually inserted into the prompt, retrieved through a vector database, or pulled in through other means (such as calling APIs, calculators, etc.). A common use case is to pass the knowledge retrieved from a vector database as context to the model.
- Prompt Input is usually the specific question or task that the large model needs to do. This part can actually be combined with the “Instruction” part. However, separating it into an independent component makes it more structured and easier to reuse templates. This is usually passed to the prompt template as a variable before calling the model to form a specific prompt.
- Output Indicator marks the beginning of the text to be generated. It’s like writing “Solution” at the beginning of a math test, indicating that you are about to start answering. If generating Python code, you can use “import” to indicate to the model that it must start writing Python code (since most Python scripts start with import). This part is often optional when we are conversing with ChatGPT. However, in LangChain, agents often use a “Thought:” as a lead word to indicate that the model should start outputting its reasoning.

Next, let’s see how to use various prompt templates in LangChain for prompt engineering to input higher-quality prompts into the large model.

## LangChain prompt template types

LangChain provides two basic types of templates: String (StringPromptTemplate) and Chat (BaseChatPromptTemplate), and builds different types of prompt templates based on them:

![](/_Langchain_Practice/img/700699/feefbb0a166f53f14f647b88e1025cyy.jpg)

Here are some prompts templates as follows:

```plain

from langchain.prompts.prompt import PromptTemplate
from langchain.prompts import FewShotPromptTemplate
from langchain.prompts.pipeline import PipelinePromptTemplate
from langchain.prompts import ChatPromptTemplate
from langchain.prompts import (
    ChatMessagePromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

```

I found that sometimes without specifying .prompts, templates can be imported directly from the LangChain package.

```plain
from langchain import PromptTemplate

```

## Use PromptTemplate

Below is a simple explanation of how to use PromptTemplate through an example.

```plain
from langchain import PromptTemplate

template = """\
你是业务咨询顾问。
你给一个销售{product}的电商公司，起一个好的名字？
"""
prompt = PromptTemplate.from_template(template)

print(prompt.format(product="鲜花"))

```

Output：

```plain
你是业务咨询顾问。
你给一个销售鲜花的电商公司，起一个好的名字？

```

`"你是业务咨询顾问。你给一个销售{product}的电商公司，起一个好的名字？"` is the original prompt template, where {product} is a placeholder.

然后通过PromptTemplate的from\_template方法，我们创建了一个提示模板对象，并通过prompt.format方法将模板中的 {product} 替换为 `"鲜花"`。

这样，就得到了一句具体的提示： _你是业务咨询顾问。你给一个销售鲜花的电商公司，起一个好的名字？_——这就要求大语言模型，要有的放矢。

在上面这个过程中，LangChain中的模板的一个方便之处是from\_template方法可以从传入的字符串中自动提取变量名称（如product），而无需刻意指定。 **上面程序中的product自动成为了format方法中的一个参数**。

当然，也可以通过提示模板类的构造函数，在创建模板时手工指定input\_variables，示例如下：

```plain
prompt = PromptTemplate(
    input_variables=["product", "market"],
    template="你是业务咨询顾问。对于一个面向{market}市场的，专注于销售{product}的公司，你会推荐哪个名字？"
)
print(prompt.format(product="鲜花", market="高端"))

```
Output：

```plain
你是业务咨询顾问。对于一个面向高端市场的，专注于销售鲜花的公司，你会推荐哪个名字？

```

上面的方式直接生成了提示模板，并没有通过from\_template方法从字符串模板中创建提示模板。二者效果是一样的。

## Use ChatPromptTemplate

For chat models like ChatGPT launched by OpenAI, LangChain also provides a series of templates. The difference with these templates is that they have corresponding roles.

The following code demonstrates the various message roles in OpenAI’s Chat Model.

```plain
import openai
openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
)

```

Here is an example:

```plain
# 导入聊天消息类模板
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
# 模板的构建
template="你是一位专业顾问，负责为专注于{product}的公司起名。"
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template="公司主打产品是{product_detail}。"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# 格式化提示消息生成提示
prompt = prompt_template.format_prompt(product="鲜花装饰", product_detail="创新的鲜花设计。").to_messages()

# 下面调用模型，把提示传入模型，生成结果
import os
os.environ["OPENAI_API_KEY"] = '你的OpenAI Key'
from langchain.chat_models import ChatOpenAI
chat = ChatOpenAI()
result = chat(prompt)
print(result)

```

## FewShot origens

In prompt engineering, the concepts of Few-Shot and Zero-Shot learning are also widely applied.

- In a Few-Shot learning setup, the model is given a few examples to help it understand the task and generate the correct response.
- In a Zero-Shot learning setup, the model generates a response based solely on the task description, without needing any examples.

## Use FewShotPromptTemplate

**1\. Create example**

First, create some examples to serve as samples for the prompt. Each example is a dictionary where the keys are input variables and the values are the values of these input variables.

```plain
# 1. 创建一些示例
samples = [
  {
    "flower_type": "玫瑰",
    "occasion": "爱情",
    "ad_copy": "玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。"
  },
  {
    "flower_type": "康乃馨",
    "occasion": "母亲节",
    "ad_copy": "康乃馨代表着母爱的纯洁与伟大，是母亲节赠送给母亲的完美礼物。"
  },
  {
    "flower_type": "百合",
    "occasion": "庆祝",
    "ad_copy": "百合象征着纯洁与高雅，是你庆祝特殊时刻的理想选择。"
  },
  {
    "flower_type": "向日葵",
    "occasion": "鼓励",
    "ad_copy": "向日葵象征着坚韧和乐观，是你鼓励亲朋好友的最好方式。"
  }
]

```

The samples list contains four dictionaries, each representing a type of flower, a suitable occasion, and the corresponding advertisement copy. These sample examples are the reference information passed to the model as examples when constructing a FewShotPrompt.

**2\. Create prompt template**

Configure a prompt template to format an example as a string. This formatter should be a PromptTemplate object.

```plain
# 2. 创建一个提示模板
from langchain.prompts.prompt import PromptTemplate
template="鲜花类型: {flower_type}\n场合: {occasion}\n文案: {ad_copy}"
prompt_sample = PromptTemplate(input_variables=["flower_type", "occasion", "ad_copy"],
                               template=template)
print(prompt_sample.format(**samples[0]))

```

提示模板的输出如下：

```plain
鲜花类型: 玫瑰
场合: 爱情
文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。

```

In this step, we created a PromptTemplate object. This object generates prompts based on the specified input variables and template string. Here, the input variables include \"flower_type\", \"occasion\", and \"ad_copy\". The template is a string containing variable names enclosed in curly braces, which will be replaced by the corresponding variable values.

At this point, we have converted the examples in the dictionary into prompt templates, which can form specific, usable LangChain prompts. For example, I replaced the variables in the template with the data from samples[0], generating a complete prompt.

**3\. Create FewShotPromptTemplate object**

Then, by using the prompt_sample created in the previous step and all the examples in the samples list, create a FewShotPromptTemplate object to generate more complex prompts.

```plain
# 3. 创建一个FewShotPromptTemplate对象
from langchain.prompts.few_shot import FewShotPromptTemplate
prompt = FewShotPromptTemplate(
    examples=samples,
    example_prompt=prompt_sample,
    suffix="鲜花类型: {flower_type}\n场合: {occasion}",
    input_variables=["flower_type", "occasion"]
)
print(prompt.format(flower_type="野玫瑰", occasion="爱情"))

```

Output：

```plain
鲜花类型: 玫瑰
场合: 爱情
文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。

鲜花类型: 康乃馨
场合: 母亲节
文案: 康乃馨代表着母爱的纯洁与伟大，是母亲节赠送给母亲的完美礼物。

鲜花类型: 百合
场合: 庆祝
文案: 百合象征着纯洁与高雅，是你庆祝特殊时刻的理想选择。

鲜花类型: 向日葵
场合: 鼓励
文案: 向日葵象征着坚韧和乐观，是你鼓励亲朋好友的最好方式。

鲜花类型: 野玫瑰
场合: 爱情

```

**4\. Create new content using LLM**

Finally, by outputting this object to the large model, we can get the copy we need based on the prompt!

```plain
# 4. 把提示传递给大模型
import os
os.environ["OPENAI_API_KEY"] = '你的Open AI Key'
from langchain.llms import OpenAI
model = OpenAI(model_name='text-davinci-003')
result = model(prompt.format(flower_type="野玫瑰", occasion="爱情"))
print(result)

```
Output：

```plain
文案: 野玫瑰代表着爱情的坚贞，是你向心爱的人表达爱意的最佳礼物。

```

## Use template selector

If we have many examples, it is impractical and inefficient to send all examples to the model at once. Additionally, including too many tokens each time will waste bandwidth (OpenAI charges based on the number of tokens).

LangChain provides us with an example selector to choose the most appropriate samples. (Note that because the example selector uses vector similarity comparison, a vector database needs to be installed here. I am using the open-source Chroma, but you can also choose Qdrant, which you have used before.)

Below is the example code for using the example selector.

```plain
# 5. 使用示例选择器
from langchain.prompts.example_selector import SemanticSimilarityExampleSelector
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# 初始化示例选择器
example_selector = SemanticSimilarityExampleSelector.from_examples(
    samples,
    OpenAIEmbeddings(),
    Chroma,
    k=1
)

# 创建一个使用示例选择器的FewShotPromptTemplate对象
prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=prompt_sample,
    suffix="鲜花类型: {flower_type}\n场合: {occasion}",
    input_variables=["flower_type", "occasion"]
)
print(prompt.format(flower_type="红玫瑰", occasion="爱情"))

```

输出：

```plain
鲜花类型: 玫瑰
场合: 爱情
文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。

鲜花类型: 红玫瑰
场合: 爱情

```

In this step, it first creates a SemanticSimilarityExampleSelector object, which can select the most relevant examples based on semantic similarity. Then, it creates a new FewShotPromptTemplate object, which uses the selector created in the previous step to select the most relevant examples to generate the prompt.

Next, we use this template to generate a new prompt. Since our prompt needs to create copy for red roses, the example selector example_selector will find the most similar example based on semantic similarity (cosine similarity), which is ‘roses’, and use this example to construct the FewShot template.

This way, we avoid passing too many irrelevant templates to the large model, saving on token usage.

## Summary

It introduced the principles of prompt engineering, the usage of various prompt templates, and the most important concept of Few-Shot learning. In simple terms, it means providing the model with some examples as references so that the model can understand what you want.

![](images/700699/f46817a7ed56c6fef64a6aeee4c1yy0d.png)

In summary, providing examples is crucial for solving certain tasks. Generally, the Few-Shot approach can significantly improve the quality of the model’s responses. However, if the Few-Shot prompts are not effective, it may indicate insufficient learning by the model on the task. In such cases, we recommend fine-tuning the model or trying more advanced prompting techniques.

In the next lesson, we will explore output parsing and introduce another highly regarded prompting technique known as ‘Chain of Thought’ (CoT). This technique is notable for its unique application method and potential practical value.




