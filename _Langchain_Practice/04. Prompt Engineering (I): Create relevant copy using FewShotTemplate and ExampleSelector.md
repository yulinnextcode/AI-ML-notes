How to use prompt templates in LangChain to do prompt engineering well?

For prompt engineering with large models, Andrew Ng provided two main principles in his ChatGPT Prompt Engineering for Developers course: the first principle is to write clear and specific instructions, and the second principle is to give the model time to think.

Similarly, in OpenAI’s official documentation GPT Best Practices, six strategies are given that align with the above two principles:

1. Write clear instructions
2. Provide references (examples)
3. Break down complex tasks into subtasks
4. Give GPT time to think
5. Use external tools
6. Iterate on the prompt

## Prompt Structure

![](images/700699/b77a15cd83b66bba55032d711bcf3c16.png)

In this prompt framework:

- Instruction tells the model what the task is roughly about and how to do it, such as how to use the provided external information, how to handle queries, and how to construct the output. This is usually a fixed part of a prompt template. A common use case is to tell the model “You are a helpful XX assistant,” which makes it take its role more seriously.
- Context serves as an additional source of knowledge for the model. This information can be manually inserted into the prompt, retrieved through a vector database, or pulled in through other means (such as calling APIs, calculators, etc.). A common use case is to pass the knowledge retrieved from a vector database as context to the model.
- Prompt Input is usually the specific question or task that the large model needs to do. This part can actually be combined with the “Instruction” part. However, separating it into an independent component makes it more structured and easier to reuse templates. This is usually passed to the prompt template as a variable before calling the model to form a specific prompt.
- Output Indicator marks the beginning of the text to be generated. It’s like writing “Solution” at the beginning of a math test, indicating that you are about to start answering. If generating Python code, you can use “import” to indicate to the model that it must start writing Python code (since most Python scripts start with import). This part is often optional when we are conversing with ChatGPT. However, in LangChain, agents often use a “Thought:” as a lead word to indicate that the model should start outputting its reasoning.

Next, let’s see how to use various prompt templates in LangChain for prompt engineering to input higher-quality prompts into the large model.

## LangChain prompt template types

LangChain provides two basic types of templates: String (StringPromptTemplate) and Chat (BaseChatPromptTemplate), and builds different types of prompt templates based on them:

![](images/700699/feefbb0a166f53f14f647b88e1025cyy.jpg)

Here are some prompts templates as follows:

```plain

from langchain.prompts.prompt import PromptTemplate
from langchain.prompts import FewShotPromptTemplate
from langchain.prompts.pipeline import PipelinePromptTemplate
from langchain.prompts import ChatPromptTemplate
from langchain.prompts import (
    ChatMessagePromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

```





























