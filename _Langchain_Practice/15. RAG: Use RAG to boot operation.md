What is RAG? Its full name is Retrieval-Augmented Generation, which combines the capabilities of retrieval and generation to introduce external knowledge into text sequence generation tasks. RAG integrates traditional language generation models with large-scale external knowledge bases, allowing the model to dynamically retrieve relevant information from these knowledge bases when generating responses or text. This combination aims to enhance the model’s generation capabilities, enabling it to produce richer, more accurate, and well-founded content, especially in scenarios requiring specific details or external factual support.

The working principle of RAG can be summarized in several steps:

1. **Retrieval**: For a given input (question), the model first uses a retrieval system to find relevant documents or passages from a large collection of documents. This retrieval system is usually based on dense vector search, such as vector databases like ChromaDB or Faiss.
2. **Context Encoding**: After finding the relevant documents or passages, the model encodes them along with the original input (question).
3. **Generation**: Using the encoded context information, the model generates the output (answer). This is typically done by a large model.
A key feature of RAG is that it does not rely solely on information within the training data but can retrieve information from large external knowledge bases. This makes RAG models particularly suitable for handling questions that did not appear in the training data.

![](images/712147/f326343298bc0bc540978604203a3e0d.jpg)

Tasks involving RAG are currently in high demand in real-world enterprise applications, and this is a key focus area for LangChain. In this lesson, I will review all the tools related to this in LangChain to help you understand what LangChain can achieve in this field.

## Document Loading
The first step in RAG is document loading. LangChain provides various types of document loaders to load different types of documents (HTML, PDF, code) and integrates with other major providers in this field, such as Airbyte and Unstructured.IO.

Below is a list of commonly used document loaders.

![](images/712147/2af251fa78768b54a7d6a4a96423a867.jpg)

## Text Transformation
After loading documents, the next step is text transformation. The most common transformation is splitting long documents into smaller chunks (or segments, or nodes) to fit the model’s context window. LangChain has many built-in document transformers that can easily split, combine, filter, and otherwise manipulate documents.

## Text Splitters
Splitting long texts into chunks might sound simple, but there are some nuances. The quality of text splitting can affect the quality of retrieval results. Ideally, we want to keep semantically related text fragments together.

In LangChain, the text splitter works as follows:

1. Split the text into small, semantically meaningful chunks (usually sentences).
2. Start combining these small chunks into a larger chunk until a certain size is reached.
3. Once that size is reached, a chunk is formed, and a new text chunk can start. This new chunk should have some overlap with the previously generated chunk to maintain context between chunks.
Therefore, the various text splitters provided by LangChain can help you set your splitting strategy and parameters from the following perspectives:

1. How the text is split
2. The size of the chunks
3. The length of overlapping text between chunks
Here are the descriptions and examples of these text splitters:

![](images/712147/517c22ba8c7d78a755d5b29ec16d3e83.jpg)

You might be concerned about the practical considerations of text splitting. Here are some key points I’ve summarized:

**First, the specific limitations of LLMs**. GPT-3.5-turbo supports a context window of 4096 tokens, meaning the total of input tokens and generated output tokens cannot exceed 4096, or an error will occur. To ensure we don’t exceed this limit, we can reserve about 2000 tokens for input prompts, leaving about 2000 tokens for the returned message. Thus, if you extract five relevant information chunks, each chunk should not exceed 400 tokens.

Additionally, **the choice of text splitting strategy is related to the type of task**.

- Tasks requiring detailed text examination are best served with smaller chunks. For example, spell checking, grammar checking, and text analysis may need to identify individual words or characters. Tasks like spam detection, plagiarism detection, sentiment analysis, keyword extraction for SEO, and topic modeling also fall into this category.
- Tasks requiring a comprehensive understanding of the text use larger chunks. For example, machine translation, text summarization, and question-answering tasks need to understand the overall meaning of the text. Natural language inference, question-answering, and machine translation need to identify relationships between different parts of the text. Creative writing also falls into this broad category.
- Finally, consider the nature of the text being split. For example, if the text has a strong structure, like code or HTML, you might want to use larger chunks. If the text has a weaker structure, like novels or news articles, you might want to use smaller chunks.

You can experiment with different chunk sizes and the size of the overlapping window between chunks to find the best solution for your specific problem.

### Other Forms of Text Transformation
In addition to splitting text, LangChain integrates various tools to perform other types of transformations on documents. Let’s analyze them one by one:

1. Filtering Redundant Documents: Using the EmbeddingsRedundantFilter tool, you can identify similar documents and filter out redundant information. This means if you have multiple highly similar or nearly identical documents, this feature can help identify and remove these duplicates, saving storage space and improving retrieval efficiency.
2. Translating Documents: By integrating with the doctran tool, you can translate documents from one language to another.
3. Extracting Metadata: Through integration with the doctran tool, you can extract key information (such as dates, authors, keywords, etc.) from the document content and store it as metadata. Metadata describes the attributes or content of a document, helping to manage, classify, and retrieve documents more effectively.
4. Converting Conversation Formats: By integrating with the doctran tool, you can convert conversational document content into a Q/A format, making it easier to extract and query specific information or answers. This is particularly useful for processing interviews, dialogues, or other interactive content.
So, document transformation is not limited to simple text splitting but can include additional operations aimed at better preparing and optimizing documents for subsequent indexing and retrieval functions.

## Text Embeddings
After forming text chunks, we use LLMs to create embeddings, converting text into numerical representations that computers can more easily process and compare. Models capable of text embeddings are available from providers like OpenAI, Cohere, and Hugging Face.

Embeddings create vector representations of text, allowing us to think about text in vector space and perform operations like semantic search, where we find the most similar text fragments in the vector space.

![](images/712147/b54fc88694120820cd1afea29946d9ba.png)

In LangChain, the Embeddings class is designed to interact with text embedding models, providing a standard interface for all these providers.

```plain
# 初始化Embedding类
from langchain.embeddings import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()

```

It provides two methods:

- 1. The first is the embed_documents method, which creates embeddings for documents. This method accepts multiple texts as input, meaning you can convert several documents into their vector representations at once.
- 2. The second is the embed_query method, which creates embeddings for queries. This method accepts a single text as input, usually the user’s search query.
**Why are two methods needed?** Although it seems that both methods are for text embeddings, LangChain separates them. The reason is that some embedding providers use different embedding methods for documents and queries. Documents are the content to be searched, while queries are the actual search requests. These two might require different handling or optimization due to their nature and purpose.

Here is an example code snippet for the embed_documents method:


```plain
embeddings = embeddings_model.embed_documents(
    [
        "您好，有什么需要帮忙的吗？",
        "哦，你好！昨天我订的花几天送达",
        "请您提供一些订单号？",
        "12345678",
    ]
)
len(embeddings), len(embeddings[0])

```

输出：

```plain
(4, 1536)

```

embed\_documents 方法的示例代码如下：

```plain
embedded_query = embeddings_model.embed_query("刚才对话中的订单号是多少?")
embedded_query[:3]

```

输出：

```plain
[-0.0029746221837547455, -0.007710168602107487, 0.00923260021751183]

```

## Storing Embeddings
Calculating embeddings can be a time-consuming process. To speed up this process, we can store or temporarily cache the calculated embeddings so that they can be directly read the next time they are needed, without recalculating.

### Cache Storage
CacheBackedEmbeddings is an embedding wrapper that supports caching, allowing embeddings to be cached in a key-value store. The specific operation is to hash the text and use this hash value as the cache key.

To initialize a CacheBackedEmbeddings, the main method is to use from_bytes_store. It requires the following parameters:

- underlying_embedder: The embedder that actually calculates the embeddings.
- document_embedding_cache: The cache used to store document embeddings.
- namespace (optional): The namespace for the document cache to avoid conflicts with other caches.
- 
**Different caching strategies are as follows:**

- 1. InMemoryStore: Caches embeddings in memory. Mainly used for unit testing or prototyping. Do not use this cache for long-term storage of embeddings.
- 2. LocalFileStore: Stores embeddings in the local file system. Suitable for situations where you do not want to rely on external databases or storage solutions.
- 3. RedisStore: Caches embeddings in a Redis database. This is a good choice when a high-speed and scalable caching solution is needed.
Here is an example code snippet for caching embeddings in memory:

```plain
# 导入内存存储库，该库允许我们在RAM中临时存储数据
from langchain.storage import InMemoryStore

# 创建一个InMemoryStore的实例
store = InMemoryStore()

# 导入与嵌入相关的库。OpenAIEmbeddings是用于生成嵌入的工具，而CacheBackedEmbeddings允许我们缓存这些嵌入
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings

# 创建一个OpenAIEmbeddings的实例，这将用于实际计算文档的嵌入
underlying_embeddings = OpenAIEmbeddings()

# 创建一个CacheBackedEmbeddings的实例。
# 这将为underlying_embeddings提供缓存功能，嵌入会被存储在上面创建的InMemoryStore中。
# 我们还为缓存指定了一个命名空间，以确保不同的嵌入模型之间不会出现冲突。
embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings,  # 实际生成嵌入的工具
    store,  # 嵌入的缓存位置
    namespace=underlying_embeddings.model  # 嵌入缓存的命名空间
)

# 使用embedder为两段文本生成嵌入。
# 结果，即嵌入向量，将被存储在上面定义的内存存储中。
embeddings = embedder.embed_documents(["你好", "智能鲜花客服"])

```

Explanation of the Code
First, we set up a storage space in memory, then initialize an embedding tool that will actually generate the embeddings. After that, this embedding tool is wrapped in a caching tool to generate embeddings for two pieces of text.

As for the other two types of caches, the usage of embeddings is not complicated, and you can refer to the LangChain documentation for more details.

## Vector Databases (Vector Stores)
A more common way to store vectors is through vector databases. LangChain supports a wide variety of vector databases, many of which are open-source, while others are commercial. Examples include Elasticsearch, Faiss, Chroma, and Qdrant.

Since there are so many options, I’ve listed them in a table for you.

![](images/712147/2eb52480f790fd3281ae905ee1c58077.jpg)

So, how do you choose among so many types of vector databases?

This involves many technical and business considerations, and you should select **based on specific needs**.

1. Data Scale and Speed Requirements: Consider the size of your data and the speed requirements for queries. Some vector databases perform better with large-scale data, while others excel in low-latency queries.
2. Durability and Reliability: Depending on your application scenario, determine if you need high availability, backup, and failover features for your data.
3. Ease of Use and Community Support: Consider the learning curve of the vector database, the completeness of its documentation, and the activity of its community.
4. Cost: Consider the total cost of ownership, including licensing, hardware, operation, and maintenance costs.
5. Features: Consider if you need specific features, such as multimodal search.
6. Security: Ensure that the vector database meets your security and compliance requirements.
When evaluating vector databases, conducting performance benchmarking is key to understanding the actual performance of the vector database. This can help you assess query speed, write speed, and concurrency performance.

There is no “best” vector database, only the “most suitable” one. Do some research and testing based on your needs to ensure that the vector database you choose meets your business and technical requirements.

## Data Retrieval
In LangChain, the Retriever is the core entry point for the data retrieval module, returning relevant documents through unstructured queries.

### Vector Store Retriever
The vector store retriever is the most common and primarily supports vector retrieval. Of course, LangChain also supports retrievers for other types of storage formats.

Below is an implementation of an end-to-end data retrieval function. We use the VectorstoreIndexCreator to create an index, and in the index’s query method, we use the as_retriever method of the vector store class to directly use the vector database (Vector Store) as the retriever to complete the retrieval task.

```plain
# 设置OpenAI的API密钥
import os
os.environ["OPENAI_API_KEY"] = 'Your OpenAI Key'

# 导入文档加载器模块，并使用TextLoader来加载文本文件
from langchain.document_loaders import TextLoader
loader = TextLoader('LangChainSamples/OneFlower/易速鲜花花语大全.txt', encoding='utf8')

# 使用VectorstoreIndexCreator来从加载器创建索引
from langchain.indexes import VectorstoreIndexCreator
index = VectorstoreIndexCreator().from_loaders([loader])

# 定义查询字符串, 使用创建的索引执行查询
query = "玫瑰花的花语是什么？"
result = index.query(query)
print(result) # 打印查询结果

```

输出：

```plain
玫瑰花的花语是爱情、热情、美丽。

```

You might think that this data retrieval process is too simple. This is thanks to LangChain’s powerful encapsulation capabilities. If we examine the code of the VectorstoreIndexCreator class in vectorstore.py, you will find that it encapsulates the vector store, embedding, and text splitter, and even the document loader (if you use the from_documents method).

```plain
class VectorstoreIndexCreator(BaseModel):
    """Logic for creating indexes."""

    vectorstore_cls: Type[VectorStore] = Chroma
    embedding: Embeddings = Field(default_factory=OpenAIEmbeddings)
    text_splitter: TextSplitter = Field(default_factory=_get_default_text_splitter)
    vectorstore_kwargs: dict = Field(default_factory=dict)

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        arbitrary_types_allowed = True

    def from_loaders(self, loaders: List[BaseLoader]) -> VectorStoreIndexWrapper:
        """Create a vectorstore index from loaders."""
        docs = []
        for loader in loaders:
            docs.extend(loader.load())
        return self.from_documents(docs)

    def from_documents(self, documents: List[Document]) -> VectorStoreIndexWrapper:
        """Create a vectorstore index from documents."""
        sub_docs = self.text_splitter.split_documents(documents)
        vectorstore = self.vectorstore_cls.from_documents(
            sub_docs, self.embedding, **self.vectorstore_kwargs
        )
        return VectorStoreIndexWrapper(vectorstore=vectorstore)

```

Therefore, the retrieval function above is equivalent to the integration of a series of tools we discussed in Lesson 2. We can also use the following code to explicitly specify the vectorstore, embedding, and text_splitter for the index creator and replace them with the tools you need, such as another vector database or a different embedding model.

```plain
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()
index_creator = VectorstoreIndexCreator(
    vectorstore_cls=Chroma,
    embedding=OpenAIEmbeddings(),
    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
)

```

So, the next question is how index.query(query) completes the specific retrieval and text generation tasks. Here, we neither see the large model nor the LangChain document retrieval tools (such as the QARetrieval chain we saw in Lesson 2).

The secret lies in the source code. In the query method of the VectorStoreIndexWrapper class, you can see that when the method is called, the RetrievalQA chain is initiated to complete the retrieval function.

```plain
class VectorStoreIndexWrapper(BaseModel):
    """Wrapper around a vectorstore for easy access."""

    vectorstore: VectorStore

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        arbitrary_types_allowed = True

    def query(
        self,
        question: str,
        llm: Optional[BaseLanguageModel] = None,
        retriever_kwargs: Optional[Dict[str, Any]] = None,
        **kwargs: Any
    ) -> str:
        """Query the vectorstore."""
        llm = llm or OpenAI(temperature=0)
        retriever_kwargs = retriever_kwargs or {}
        chain = RetrievalQA.from_chain_type(
            llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs
        )
        return chain.run(question)

```

The vector store retriever we used above is a lightweight wrapper for the vector store class, making it conform to the retriever interface. It uses search methods in the vector store (such as similarity search and MMR) to query the text in the vector store.

### Various Types of Retrievers
In addition to the vector store retriever, LangChain also provides many other retrieval tools.

![](images/712147/f87c2d22bb1e71419ee129c9871724a8.jpg)

Each of these retrieval tools has its own features, and you can refer to their documentation and try using them.

## Index
In the final part of this lesson, let’s look at the index in LangChain. Simply put, an index is a method for efficiently managing and locating document information, ensuring each document has a unique identifier and is easy to retrieve.

Although in the example from Lesson 2, we completed a RAG task without explicitly using an index, effectively managing and indexing documents is a key step in complex information retrieval tasks. The index API provided by LangChain offers an efficient and intuitive solution for developers. Specifically, its advantages include:

- Avoiding Duplicate Content: Ensures there is no redundant data in your vector store.
- Updating Only Changed Content: Can detect which content has been updated, avoiding unnecessary rewrites.
- Saving Time and Money: Does not recalculate embeddings for unchanged content, reducing the consumption of computational resources.
- Optimizing Search Results: Reduces duplicate and irrelevant data, improving the accuracy of searches.
LangChain uses a RecordManager to track which documents have been written to the vector store.

When indexing, the API hashes each document to ensure each document has a unique identifier. This hash value is based not only on the document’s content but also on its metadata.

Once hashing is complete, the following information is stored in the RecordManager:

- Document Hash: A unique identifier calculated based on the document’s content and metadata.
- Write Time: Records when the document was added to the vector store.
- Source ID: A metadata field indicating the original source of the document.
This method ensures that even if a document undergoes multiple transformations or processing, its status and source can be accurately tracked, ensuring the document data is correctly managed and indexed.

## Summary

This lesson covers a lot of content, and I’ve provided many tables for your reference, so there’s a lot of information to digest. You can also review Lesson 2 to gain a deeper understanding of the RAG process.

The most common method for storing and searching unstructured data using Retrieval-Augmented Generation (RAG) is to embed this unstructured data and store the generated embedding vectors. When querying, the text to be queried is also embedded, and the system retrieves the embedding vectors that are “most similar” to the query embedding. The vector database is responsible for storing the embedding data and performing vector searches for you.

![](images/712147/39ab4b67b2689e6daf9a83bc5895b684.jpg)

As you can see, RAG essentially creates a “map” for unstructured data. When a user makes a query, the query is also embedded, and your application searches this “map” for the most matching location, thereby quickly and accurately retrieving information.

In our flower operation scenario, RAG can play a significant role in many aspects. Your flowers come in various varieties, colors, and meanings, and this data is often natural and loose, i.e., unstructured. Using RAG, you can embed vectors to associate your flower inventory with related unstructured information (such as meanings, colors, origins, etc.). When customers or employees want to query information about a specific flower, the system can quickly provide accurate answers.

Additionally, RAG can be applied to order management. Each order, whether it’s the customer’s name, address, type of flowers purchased, or order status, can be considered unstructured data. With RAG, we can easily embed and retrieve these orders, providing customers with real-time order updates, tracking, and query services.

Of course, for information like orders, it is more common to organize them into structured data and store them in a database (at least in a CSV or Excel file) for efficient and accurate querying. Can LLMs help us query entries in database tables? In the next lesson, I will reveal the answer for you.











