# 01｜LangChain System Installation and Quick Start



## LangChain installation

```plain
pip install langchain

```

These are the minimum requirements for installing LangChain. Here, I want to remind you that LangChain needs to integrate with various models and data repositories, such as the important OpenAI API interface, the open-source large model library HuggingFace Hub, and support for various vector databases. By default, the necessary dependencies are not installed simultaneously.

After installation, you also need to update to the latest version of LangChain to use the newer tools.

```plain
pip install --upgrade langchain

```

If you want to install from the source code, you can clone the repository and run:

```plain
pip install -e

```

## OpenAI API

LangChain essentially provides a wrapper for various large model APIs. It is designed to facilitate the use of these APIs by providing frameworks, modules, and interfaces.


With an OpenAI account and key, you can see various information on the dashboard, such as model costs and usage. The image below shows the access limits for various models. TPM and RPM stand for tokens-per-minute and requests-per-minute, respectively. For GPT-4, you can call the API up to 200 times per minute and transmit 40,000 tokens per minute.
![](images/699400/66e055f3c48b4bc3e11ffe0e85a5c7f3.png)

Here, we need to highlight two types of models shown in the image: Chat Models and Text Models. These two types of models represent large language models. Of course, OpenAI also provides Image, Audio, and other types of models, but currently, they are not the main focus of LangChain support and are fewer in number.

- **Chat Model，聊天模型**，For generating conversations between humans and AI, the representative models are, of course, gpt-3.5-turbo (also known as ChatGPT) and GPT-4. OpenAI also provides other versions, such as gpt-3.5-turbo-0613, which represents a snapshot of ChatGPT from June 13, 2023, and gpt-3.5-turbo-16k, which can accept 16K tokens instead of the usual 4K. (Note that gpt-3.5-turbo-16k is not available for us to use, and the more tokens you transmit, the more it costs.)

- **Text Model，文本模型**，Before ChatGPT was released, everyone used this type of model API to call GPT-3. The representative text model is text-davinci-003 (based on GPT-3). In this model family, there are also models specifically trained for text embeddings, such as text-embedding-ada-002, and models for similarity comparison, such as text-similarity-curie-001.

The two models mentioned above provide similar functions: they both receive conversational input (also called a prompt) and return a response text (also called an output). However, their invocation methods and required input formats are different, which we will explain further later.

### 调用Text模型

Step 1: First, register your API Key.

Step 2: Use the command pip install openai to install the OpenAI library.

Step 3: Import the OpenAI API Key.

Step 4: Import the OpenAI library. (If you didn’t import the OpenAI library when importing the OpenAI API Key in the previous step)

Step 5: Call the Text model and return the result.

```plain
response = openai.Completion.create(
  model="text-davinci-003",
  temperature=0.5,
  max_tokens=100,
  prompt="请给我的花店起个名")

```

Step 6: Print the text returned by the large model.

When you call the Completion.create method of OpenAI, it returns a response object that contains the output generated by the model and some other information. This response object is a dictionary structure with multiple fields.

In the case of using a Text model (such as text-davinci-003), the main fields of the response object include:

![](images/699400/4cb717e0258971c7e92dace9c4d8f2ce.jpg)

The choices field is a list because, in some cases, you can ask the model to generate multiple possible outputs. Each choice is a dictionary that contains the following fields:

- text: The text generated by the model.
- finish_reason: The reason the model stopped generating text. Possible values include stop (encountered a stop token), length (reached the maximum length), or temperature (stopped based on the set temperature parameter).

So, the line of code response.choices[0].text.strip() means: from the response, get the first choice (if you didn’t specify the n parameter when calling the model, there will be only one response), then get the text of that choice and remove any leading or trailing whitespace. This is usually the output you want from the model.

### 调用Chat模型

Overall, the process of calling Chat models and Text models is exactly the same; only the data formats of the input (prompt) and output (response) are different.

```plain
response = openai.ChatCompletion.create(
  model="gpt-4",
  messages=[
        {"role": "system", "content": "You are a creative AI."},
        {"role": "user", "content": "请给我的花店起个名"},
    ],
  temperature=0.8,
  max_tokens=60
)
print(response['choices'][0]['message']['content'])

```

In this code, apart from the parameters like temperature and max_tokens that we have already introduced, there are two concepts specific to Chat models: messages and roles!

First, let’s talk about messages. Messages are the prompts passed to the model. Here, the messages parameter is a list containing multiple messages. Each message has a role (which can be system, user, or assistant) and content (the content of the message). The system message sets the context of the conversation (e.g., “You are a great intelligent assistant”), and the user message makes a specific request (e.g., “Please name my flower shop”). The model’s task is to generate a reply based on these messages.

Next, let’s talk about roles. In OpenAI’s Chat models, system, user, and assistant are all roles of messages. Each role has different meanings and functions.

- system: System messages are primarily used to set the context or background of the conversation. This helps the model understand its role and task in the conversation. For example, you can use a system message to set a scene, letting the model know it is playing the role of a doctor, lawyer, or a knowledgeable AI assistant. System messages are usually given at the beginning of the conversation.

- user: User messages are sent from the user or human role. They typically contain the requests or questions that the user wants the model to answer or complete. User messages can be a question, a statement, or any other content the user wants the model to respond to.

- assistant: Assistant messages are the model’s replies. For example, when you send a new conversation request in a multi-turn dialogue using the API, you can provide the context of the previous conversation through assistant messages. However, note that the last message in the conversation should always be a user message, as the model always responds to the last user message.

When using the Chat model to generate content, the returned response will contain one or more choices. Each choice includes a message. Each message also contains a role and content. The role can be system, user, or assistant, indicating the sender of the message, and the content contains the actual message content.

A typical response object might look like this:

```plain
{
 'id': 'chatcmpl-2nZI6v1cW9E3Jg4w2Xtoql0M3XHfH',
 'object': 'chat.completion',
 'created': 1677649420,
 'model': 'gpt-4',
 'usage': {'prompt_tokens': 56, 'completion_tokens': 31, 'total_tokens': 87},
 'choices': [
   {
    'message': {
      'role': 'assistant',
      'content': '你的花店可以叫做"花香四溢"。'
     },
    'finish_reason': 'stop',
    'index': 0
   }
  ]
}

```

This is the basic structure of the response. It is actually quite similar to the response structure returned by the Text model, except that the Text field in choices is replaced with Message. You can parse this object to get the information you need. For example, to get the model’s reply, you can use response['choices'][0]['message']['content'].

### Chat模型 vs Text模型

Both Chat models and Text models have their own advantages, and their applicability depends on the specific use case.

Compared to Text models, Chat models are better designed for handling conversations or multi-turn interactions. This is because they can accept a list of messages as input, rather than just a single string. This message list can include historical information from system, user, and assistant, providing more context when handling interactive dialogues.

The main advantages of this design include:

1. Management of Conversation History: By using Chat models, you can more easily manage the history of the conversation and provide this historical information to the model when needed. For example, you can include past user inputs and model responses in the message list, allowing the model to consider this historical information when generating new responses.

2. Role Simulation: Through the system role, you can set the context of the conversation, providing the model with additional guidance to better control the output. Of course, in Text models, you can also set roles for the AI in the prompt as part of the input.

However, for simple single-turn text generation tasks, using Text models might be simpler and more straightforward. For example, if you only need the model to generate a piece of text based on a simple prompt, then a Text model might be more suitable. From the results above, the Chat model provides us with more complete text, like a full sentence, while the Text model outputs several names. This is because ChatGPT has been aligned (through reinforcement learning from human feedback), making its answers more like real conversation scenarios.

Alright, we have understood OpenAI’s API calls to this extent. After all, we mainly access OpenAI through the advanced encapsulated framework LangChain.

## Call Text and Chat models using LangChain. 

Great! Let’s go through how to call Text and Chat models using LangChain. 
### Call Text model

```plain
import os
os.environ["OPENAI_API_KEY"] = '你的Open API Key'
from langchain.llms import OpenAI
llm = OpenAI(
    model="text-davinci-003",
    temperature=0.8,
    max_tokens=60,)
response = llm.predict("请给我的花店起个名")
print(response)

```
output：

```plain
花之缘、芳华花店、花语心意、花风旖旎、芳草世界、芳色年华

```

This is just a simple wrapper for the OpenAI API: First, import the OpenAI class from LangChain, create an LLM (large language model) object, specify the model to use and some generation parameters. Use the created LLM object and the message list to call the __call__ method of the OpenAI class for text generation. The generated result is stored in the response variable. There is nothing particularly noteworthy to explain.

### Call Chat model

```plain
import os
os.environ["OPENAI_API_KEY"] = '你的Open API Key'
from langchain.chat_models import ChatOpenAI
chat = ChatOpenAI(model="gpt-4",
                    temperature=0.8,
                    max_tokens=60)
from langchain.schema import (
    HumanMessage,
    SystemMessage
)
messages = [
    SystemMessage(content="你是一个很棒的智能助手"),
    HumanMessage(content="请给我的花店起个名")
]
response = chat(messages)
print(response)

```

Output：

```plain
content='当然可以，叫做"花语秘境"怎么样？'
additional_kwargs={} example=False

```

Whether it’s the OpenAI (Text model) in langchain.llms or the ChatOpenAI (Chat model) in langchain.chat_models, the structure of the response variable returned is simpler than directly calling the OpenAI API. This is because LangChain has already parsed the output of the large language model, retaining only the most important text part of the response.

## Summary

Understanding the evolution of OpenAI from Text models to Chat models, and when to choose the Chat model versus the Text model. Additionally, the most basic call flow for these two models. Once we master this content, we can continue with the subsequent learning.

## Notes

1. From above two examples, it seems that using LangChain is not more convenient than directly calling the OpenAI API? Moreover, you still need the OpenAI API to call the GPT family models. So, what is the core value of LangChain?

2. LangChain supports more than just OpenAI models. Can you try other models from the HuggingFace open-source community to see if they work? Hint: You should choose text generation, text-to-text generation, and question-answering models.

```plain
from langchain import HuggingFaceHub
llm = HuggingFaceHub(model_id="bigscience/bloom-1b7")

```













