# 01｜LangChain System Installation and Quick Start



## LangChain installation

```plain
pip install langchain

```

These are the minimum requirements for installing LangChain. Here, I want to remind you that LangChain needs to integrate with various models and data repositories, such as the important OpenAI API interface, the open-source large model library HuggingFace Hub, and support for various vector databases. By default, the necessary dependencies are not installed simultaneously.

After installation, you also need to update to the latest version of LangChain to use the newer tools.

```plain
pip install --upgrade langchain

```

If you want to install from the source code, you can clone the repository and run:

```plain
pip install -e

```

## OpenAI API

LangChain essentially provides a wrapper for various large model APIs. It is designed to facilitate the use of these APIs by providing frameworks, modules, and interfaces.


With an OpenAI account and key, you can see various information on the dashboard, such as model costs and usage. The image below shows the access limits for various models. TPM and RPM stand for tokens-per-minute and requests-per-minute, respectively. For GPT-4, you can call the API up to 200 times per minute and transmit 40,000 tokens per minute.
![](images/699400/66e055f3c48b4bc3e11ffe0e85a5c7f3.png)

Here, we need to highlight two types of models shown in the image: Chat Models and Text Models. These two types of models represent large language models. Of course, OpenAI also provides Image, Audio, and other types of models, but currently, they are not the main focus of LangChain support and are fewer in number.

- **Chat Model，聊天模型**，For generating conversations between humans and AI, the representative models are, of course, gpt-3.5-turbo (also known as ChatGPT) and GPT-4. OpenAI also provides other versions, such as gpt-3.5-turbo-0613, which represents a snapshot of ChatGPT from June 13, 2023, and gpt-3.5-turbo-16k, which can accept 16K tokens instead of the usual 4K. (Note that gpt-3.5-turbo-16k is not available for us to use, and the more tokens you transmit, the more it costs.)

- **Text Model，文本模型**，Before ChatGPT was released, everyone used this type of model API to call GPT-3. The representative text model is text-davinci-003 (based on GPT-3). In this model family, there are also models specifically trained for text embeddings, such as text-embedding-ada-002, and models for similarity comparison, such as text-similarity-curie-001.

The two models mentioned above provide similar functions: they both receive conversational input (also called a prompt) and return a response text (also called an output). However, their invocation methods and required input formats are different, which we will explain further later.

### 调用Text模型

Step 1: First, register your API Key.

Step 2: Use the command pip install openai to install the OpenAI library.

Step 3: Import the OpenAI API Key.

Step 4: Import the OpenAI library. (If you didn’t import the OpenAI library when importing the OpenAI API Key in the previous step)

Step 5: Call the Text model and return the result.

```plain
response = openai.Completion.create(
  model="text-davinci-003",
  temperature=0.5,
  max_tokens=100,
  prompt="请给我的花店起个名")

```

Step 6: Print the text returned by the large model.

When you call the Completion.create method of OpenAI, it returns a response object that contains the output generated by the model and some other information. This response object is a dictionary structure with multiple fields.

In the case of using a Text model (such as text-davinci-003), the main fields of the response object include:

![](images/699400/4cb717e0258971c7e92dace9c4d8f2ce.jpg)

The choices field is a list because, in some cases, you can ask the model to generate multiple possible outputs. Each choice is a dictionary that contains the following fields:

- text: The text generated by the model.
- finish_reason: The reason the model stopped generating text. Possible values include stop (encountered a stop token), length (reached the maximum length), or temperature (stopped based on the set temperature parameter).

So, the line of code response.choices[0].text.strip() means: from the response, get the first choice (if you didn’t specify the n parameter when calling the model, there will be only one response), then get the text of that choice and remove any leading or trailing whitespace. This is usually the output you want from the model.

### 调用Chat模型

Overall, the process of calling Chat models and Text models is exactly the same; only the data formats of the input (prompt) and output (response) are different.

```plain
response = openai.ChatCompletion.create(
  model="gpt-4",
  messages=[
        {"role": "system", "content": "You are a creative AI."},
        {"role": "user", "content": "请给我的花店起个名"},
    ],
  temperature=0.8,
  max_tokens=60
)
print(response['choices'][0]['message']['content'])

```

In this code, apart from the parameters like temperature and max_tokens that we have already introduced, there are two concepts specific to Chat models: messages and roles!

First, let’s talk about messages. Messages are the prompts passed to the model. Here, the messages parameter is a list containing multiple messages. Each message has a role (which can be system, user, or assistant) and content (the content of the message). The system message sets the context of the conversation (e.g., “You are a great intelligent assistant”), and the user message makes a specific request (e.g., “Please name my flower shop”). The model’s task is to generate a reply based on these messages.











