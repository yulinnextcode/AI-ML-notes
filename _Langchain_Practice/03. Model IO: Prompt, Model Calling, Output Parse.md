A detailed analysis of the six core components in LangChain.

Models: Located at the lowest layer of the LangChain framework, they are the core elements of applications built on language models. LangChain application development involves using LangChain as a framework to call large models via API to solve specific problems.

![](images/699451/76619cf2f73ef200dd57cd16c0d55ec4.png)

## Model I/O

We can break down the process of using models into three parts: Input Prompts (corresponding to ‘Format’ in the diagram), Calling the Model (corresponding to ‘Predict’ in the diagram), and Parsing the Output (corresponding to ‘Parse’ in the diagram). These three parts form a whole, so in LangChain, this process is collectively referred to as Model I/O (Input/Output).

![](images/699451/ac67214287154dcfbbf12d81086c8023.png)

In every aspect of model I/O, LangChain provides us with templates and tools to quickly form interfaces for calling various language models.

- Prompt Templates: The first step in using a model is to input the prompt information into the model. You can create LangChain templates to dynamically select different inputs based on actual needs and adjust the inputs for specific tasks and applications.
- Language Models: LangChain allows you to call language models through a common interface. This means that no matter which language model you want to use, you can call it in the same way, thus improving flexibility and convenience.
- Output Parsing: LangChain also provides the functionality to extract information from the model’s output. Through output parsers, you can precisely obtain the needed information from the model’s output without dealing with redundant or irrelevant data. More importantly, you can convert the unstructured text returned by large models into structured data that programs can process.

Next, let’s delve into these three aspects with examples. First, let’s look at the construction of prompt templates in LangChain.

## Prompt templates

A good prompt (which is essentially a good question or instruction) can definitely make your use of language models much more effective.

The specific principles, as mentioned by Andrew Ng in his Prompt Engineering Course, are:

- Give the model clear and explicit instructions.
- Allow the model to think step by step.

Generate a brief description for each type of flower for sale, so whenever your employees or customers want to learn about a particular flower, they can use this template to generate suitable text.

The way to create this prompt template is as follows:

```plain
# 导入LangChain中的提示模板
from langchain import PromptTemplate
# 创建原始模板
template = """您是一位专业的鲜花店文案撰写员。\n
对于售价为 {price} 元的 {flower_name} ，您能提供一个吸引人的简短描述吗？
"""
# 根据原始模板创建LangChain提示模板
prompt = PromptTemplate.from_template(template)
# 打印LangChain提示模板的内容
print(prompt)

```

The specific content of the prompt template is as follows:

```plain
input_variables=['flower_name', 'price']
output_parser=None partial_variables={}
template='/\n您是一位专业的鲜花店文案撰写员。
\n对于售价为 {price} 元的 {flower_name} ，您能提供一个吸引人的简短描述吗？\n'
template_format='f-string'
validate_template=True

```

Next, we will use this newly constructed prompt template to generate prompts and input them into the large language model.

## **Language Model**

LangChain supports three main types of models:

- Large Language Models (LLM), also known as Text Models. These models take text strings as input and return text strings as output. Examples of typical LLMs include OpenAI’s text-davinci-003, Facebook’s LLaMA, and ANTHROPIC’s Claude.
- Chat Models, primarily represented by OpenAI’s ChatGPT series. These models are usually supported by language models, but their APIs are more structured. Specifically, these models take a list of chat messages as input and return chat messages.
- Text Embedding Models, which take text as input and return a list of floating-point numbers, known as embeddings. An example of a text embedding model is OpenAI’s text-embedding-ada-002, which we have seen before. Text embedding models are responsible for storing documents in a vector database and are not directly related to the prompt engineering we are discussing here.

Then, we will call the language model to help us write the copy and return the result.

```plain
# 设置OpenAI API Key
import os
os.environ["OPENAI_API_KEY"] = '你的Open AI API Key'

# 导入LangChain中的OpenAI模型接口
from langchain import OpenAI
# 创建模型实例
model = OpenAI(model_name='text-davinci-003')
# 输入提示
input = prompt.format(flower_name=["玫瑰"], price='50')
# 得到模型的输出
output = model(input)
# 打印输出内容
print(output)

```

By reusing the prompt template, we can simultaneously generate copy for multiple flowers.

```plain
# 导入LangChain中的提示模板
from langchain import PromptTemplate
# 创建原始模板
template = """您是一位专业的鲜花店文案撰写员。\n
对于售价为 {price} 元的 {flower_name} ，您能提供一个吸引人的简短描述吗？
"""
# 根据原始模板创建LangChain提示模板
prompt = PromptTemplate.from_template(template)
# 打印LangChain提示模板的内容
print(prompt)

# 设置OpenAI API Key
import os
os.environ["OPENAI_API_KEY"] = '你的Open AI API Key'

# 导入LangChain中的OpenAI模型接口
from langchain import OpenAI
# 创建模型实例
model = OpenAI(model_name='text-davinci-003')

# 多种花的列表
flowers = ["玫瑰", "百合", "康乃馨"]
prices = ["50", "30", "20"]

# 生成多种花的文案
for flower, price in zip(flowers, prices):
    # 使用提示模板生成输入
    input_prompt = prompt.format(flower_name=flower, price=price)

    # 得到模型的输出
    output = model(input_prompt)

    # 打印输出内容
    print(output)

```

What is the significance of using LangChain in this process? Can’t I achieve the same functionality by directly calling OpenAI’s API?

Indeed, let’s take a look at the code to accomplish the above functionality using the OpenAI API directly.

```plain
import openai # 导入OpenAI
openai.api_key = 'Your-OpenAI-API-Key' # API Key

prompt_text = "您是一位专业的鲜花店文案撰写员。对于售价为{}元的{}，您能提供一个吸引人的简短描述吗？" # 设置提示

flowers = ["玫瑰", "百合", "康乃馨"]
prices = ["50", "30", "20"]

# 循环调用Text模型的Completion方法，生成文案
for flower, price in zip(flowers, prices):
    prompt = prompt_text.format(price, flower)
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=100
    )
    print(response.choices[0].text.strip()) # 输出文案

```

ou will find the advantages of LangChain. We only need to define the template once, and we can use it to generate various prompts. Compared to simply using f-strings to format text, this method is more concise and easier to maintain. LangChain also integrates features like output_parser, template_format, and whether to validate_template in the prompt templates.

More importantly, by using LangChain prompt templates, we can easily switch the program to different models without modifying any prompt-related code.

Next, we will use the exact same prompt template to generate prompts and send them to an open-source model in HuggingFaceHub to create the copy. (Note: You need to register for a HUGGINGFACEHUB_API_TOKEN)

![](images/699451/c8c0d84349ebd2d1d82a2836383164ec.png)

```plain
# 导入LangChain中的提示模板
from langchain import PromptTemplate
# 创建原始模板
template = """You are a flower shop assitiant。\n
For {price} of {flower_name} ，can you write something for me？
"""
# 根据原始模板创建LangChain提示模板
prompt = PromptTemplate.from_template(template)
# 打印LangChain提示模板的内容
print(prompt)
import os
os.environ['HUGGINGFACEHUB_API_TOKEN'] = '你的HuggingFace API Token'
# 导入LangChain中的OpenAI模型接口
from langchain import HuggingFaceHub
# 创建模型实例
model= HuggingFaceHub(repo_id="google/flan-t5-large")
# 输入提示
input = prompt.format(flower_name=["rose"], price='50')
# 得到模型的输出
output = model(input)
# 打印输出内容
print(output)

```

The message I want to convey to you is: you can reuse templates and program structures to call any model through the LangChain framework. If you are familiar with the training process of machine learning, doesn’t LangChain remind you of frameworks like PyTorch and TensorFlow—models can be freely chosen and independently trained, while the framework for calling models is often systematic and reusable.

Therefore, the benefits of using LangChain and prompt templates are:

- Readability of Code: Using templates makes the prompt text easier to read and understand, especially for complex prompts or situations involving multiple variables.
- Reusability: Templates can be reused in multiple places, making your code more concise and eliminating the need to reconstruct the prompt string wherever a prompt is needed.
- Maintenance: If you need to modify the prompt later, using templates means you only need to modify the template itself, rather than searching through the code to find and change every instance where the prompt is used.
- Variable Handling: If your prompt involves multiple variables, templates can automatically handle the insertion of variables, avoiding the need for manual string concatenation.
- Parameterization: Templates can generate different prompts based on different parameters, which is very useful for personalized text generation.

Now, let’s move on to the final step of model I/O, output parsing.

## Output Parse

Next, we will use LangChain’s output parser to refactor the program, enabling the model to generate structured responses. We will then parse these responses and directly store the parsed data into a CSV document.

```plain
# 通过LangChain调用模型
from langchain import PromptTemplate, OpenAI

# 导入OpenAI Key
import os
os.environ["OPENAI_API_KEY"] = '你的OpenAI API Key'

# 创建原始提示模板
prompt_template = """您是一位专业的鲜花店文案撰写员。
对于售价为 {price} 元的 {flower_name} ，您能提供一个吸引人的简短描述吗？
{format_instructions}"""

# 创建模型实例
model = OpenAI(model_name='text-davinci-003')

# 导入结构化输出解析器和ResponseSchema
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
# 定义我们想要接收的响应模式
response_schemas = [
    ResponseSchema(name="description", description="鲜花的描述文案"),
    ResponseSchema(name="reason", description="问什么要这样写这个文案")
]
# 创建输出解析器
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

# 获取格式指示
format_instructions = output_parser.get_format_instructions()
# 根据原始模板创建提示，同时在提示中加入输出解析器的说明
prompt = PromptTemplate.from_template(prompt_template,
                partial_variables={"format_instructions": format_instructions})

# 数据准备
flowers = ["玫瑰", "百合", "康乃馨"]
prices = ["50", "30", "20"]

# 创建一个空的DataFrame用于存储结果
import pandas as pd
df = pd.DataFrame(columns=["flower", "price", "description", "reason"]) # 先声明列名

for flower, price in zip(flowers, prices):
    # 根据提示准备模型的输入
    input = prompt.format(flower_name=flower, price=price)

    # 获取模型的输出
    output = model(input)

    # 解析模型的输出（这是一个字典结构）
    parsed_output = output_parser.parse(output)

    # 在解析后的输出中添加“flower”和“price”
    parsed_output['flower'] = flower
    parsed_output['price'] = price

    # 将解析后的输出添加到DataFrame中
    df.loc[len(df)] = parsed_output

# 打印字典
print(df.to_dict(orient='records'))

# 保存DataFrame到CSV文件
df.to_csv("flowers_with_descriptions.csv", index=False)

```

In this code, we first define the output structure. We want the model’s generated answer to include two parts: the flower’s description (description) and the reason for writing this description (reason). Therefore, we define a list named response_schemas, which contains two ResponseSchema objects corresponding to these two parts of the output.

Based on this list, I create an output parser using the StructuredOutputParser.from_response_schemas method.

Then, we obtain the format instructions for the output through the get_format_instructions() method of the output parser object. We create a new prompt template based on the original string template and the output parser’s format instructions (this template integrates the output parsing structure information). We then generate the model’s input using the new template and obtain the model’s output. At this point, the model’s output structure will follow our instructions as closely as possible to facilitate parsing by the output parser.

For each flower and price combination, we use output_parser.parse(output) to parse the model’s generated copy into the predefined data format, which is a Python dictionary containing the values for the description and reason fields.

```plain
parsed_output
{'description': 'This 50-yuan rose is... feelings.', 'reason': 'The description is s...y emotion.'}
len(): 2

```

Finally, integrate all the information into a pandas DataFrame object (you need to install the Pandas library). This DataFrame object contains the values for flower, price, description, and reason. The description and reason are parsed from the model’s output by the output_parser, while flower and price are added by us.

We can print the contents of the DataFrame and easily process it in the program, such as saving it as a CSV file. At this point, the data is no longer vague, unstructured text but clearly structured and formatted data. The output parser plays a significant role in this process.

![](images/699451/3264157dc13f229641d87dcb34dafbf2.png)

## Summary

To summarize the benefits of using the LangChain framework, you will find it has several advantages:

- Template Management: In large projects, there may be many different prompt templates. Using LangChain can help you better manage these templates, keeping your code clear and tidy.
- Variable Extraction and Checking: LangChain can automatically extract and check the variables in the templates, ensuring you haven’t forgotten to fill in any variables.
- Model Switching: If you want to try using different models, you only need to change the model’s name without modifying the code.
- Output Parsing: LangChain’s prompt templates can embed definitions of the output format, making it easier to handle the formatted output in subsequent processing.

Next, we will continue to explore prompt templates in LangChain and see how high-quality prompt engineering can enable the model to produce more accurate and higher-quality outputs.



