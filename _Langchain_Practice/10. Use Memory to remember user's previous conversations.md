By default, both LLMs and agents are stateless, meaning each model call is independent of other interactions. In other words, every time we start a new conversation with the large language model through the API, it doesn’t know that you actually chatted with it yesterday or the day before.

You might say, “That’s impossible! Every time I chat with ChatGPT, it clearly remembers what I mentioned before.”

That’s right! ChatGPT can remember what you’ve said before because it uses a memory mechanism. This mechanism records the previous conversation context and includes it as part of the prompt in the latest call to the model. Memory mechanisms are crucial in building chatbots.

## Use ConversationChain

Let’s look at a simple example and print out the built-in prompt template in the ConversationChain. This will help you understand the significance of this conversation format.

```plain
from langchain import OpenAI
from langchain.chains import ConversationChain

# 初始化大语言模型
llm = OpenAI(
    temperature=0.5,
    model_name="text-davinci-003"
)

# 初始化对话链
conv_chain = ConversationChain(llm=llm)

# 打印对话的模板
print(conv_chain.prompt.template)

```

输出：

```plain
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
{history}
Human: {input}
AI:

```

Here, the prompt sets up a basic conversation framework between a human (us) and an AI (text-davinci-003):

“The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.”

At the same time, this prompt tries to reduce hallucinations, meaning it minimizes the information the model makes up, by stating:

“If the AI does not know the answer to a question, it truthfully says it does not know.”

Next, we see two parameters: {history} and {input}.

- {history} is where the conversation memory is stored, containing the dialogue history between the human and the AI.
- {input} is where new input goes, which you can think of as the text box input when conversing with ChatGPT.
These two parameters are passed to the LLM through the prompt template, and we expect the returned output to be a continuation of the conversation.

So, with the {history} parameter and the prefixes Human and AI, we can store the historical conversation information in the prompt template and pass it as new prompt content to the model in the next round of conversation. This is the principle of the memory mechanism.

Now, let’s add the memory function to the ConversationChain.

## Use ConversationBufferMemory

In LangChain, the simplest memory mechanism can be implemented using ConversationBufferMemory.

Let’s introduce ConversationBufferMemory into the conversation chain.

```plain
from langchain import OpenAI
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory

# 初始化大语言模型
llm = OpenAI(
    temperature=0.5,
    model_name="text-davinci-003")

# 初始化对话链
conversation = ConversationChain(
    llm=llm,
    memory=ConversationBufferMemory()
)

# 第一天的对话
# 回合1
conversation("我姐姐明天要过生日，我需要一束生日花束。")
print("第一次对话后的记忆:", conversation.memory.buffer)

```

输出：

```plain
第一次对话后的记忆:
Human: 我姐姐明天要过生日，我需要一束生日花束。
AI:  哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。

```

在下一轮对话中，这些记忆会作为一部分传入提示。

```plain
# 回合2
conversation("她喜欢粉色玫瑰，颜色是粉色的。")
print("第二次对话后的记忆:", conversation.memory.buffer)

```

输出：

```plain
第二次对话后的记忆:
Human: 我姐姐明天要过生日，我需要一束生日花束。
AI:  哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。
Human: 她喜欢粉色玫瑰，颜色是粉色的。
AI:  好的，那我可以推荐一束粉色玫瑰的生日花束给你。你想要多少朵？我可以帮你定制一束，比如说十朵、二十朵或者更多？

```

下面，我们继续对话，同时打印出此时提示模板的信息。

```plain
# 回合3 （第二天的对话）
conversation("我又来了，还记得我昨天为什么要来买花吗？")
print("/n第三次对话后时提示:/n",conversation.prompt.template)
print("/n第三次对话后的记忆:/n", conversation.memory.buffer)

```

模型输出：

```plain
Human: 我姐姐明天要过生日，我需要一束生日花束。
AI:  哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。
Human: 她喜欢粉色玫瑰，颜色是粉色的。
AI:  好的，那我可以推荐一束粉色玫瑰的生日花束给你，你想要多少朵？
Human: 我又来了，还记得我昨天为什么要来买花吗？
AI:  是的，我记得你昨天来买花是因为你姐姐明天要过生日，你想要买一束粉色玫瑰的生日花束给她。

```

In fact, all this chat history information is passed into the {history} parameter of the ConversationChain’s prompt template, creating a new prompt input that includes the chat log.

With the memory mechanism, the LLM can understand the previous conversation content. This straightforward storage of all content provides the LLM with the maximum amount of information. However, the new input also includes more tokens (all chat history), which means slower response times and higher costs. Additionally, when the token limit of the LLM (context window) is reached, long conversations cannot be remembered (for text-davinci-003 and gpt-3.5-turbo, the maximum input limit per call is 4096 tokens).

Let’s look at some solutions for when there are too many tokens and the chat history is too long.

## Using ConversationBufferWindowMemory
When it comes to memory, our human brains are not infinite either. Sometimes, when there are too many things, we can only erase some distant memories. After all, the most recent experiences are the freshest and most important.

ConversationBufferWindowMemory is a buffer window memory. The idea is to only save the most recent interactions between the human and the AI. Therefore, it adds a window value ( k ) on top of the previous “buffer memory.” This means we only keep a certain number of past interactions and “forget” the earlier ones.

Let’s look at an example.

```plain
from langchain import OpenAI
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferWindowMemory

# 创建大语言模型实例
llm = OpenAI(
    temperature=0.5,
    model_name="text-davinci-003")

# 初始化对话链
conversation = ConversationChain(
    llm=llm,
    memory=ConversationBufferWindowMemory(k=1)
)

# 第一天的对话
# 回合1
result = conversation("我姐姐明天要过生日，我需要一束生日花束。")
print(result)
# 回合2
result = conversation("她喜欢粉色玫瑰，颜色是粉色的。")
# print("\n第二次对话后的记忆:\n", conversation.memory.buffer)
print(result)

# 第二天的对话
# 回合3
result = conversation("我又来了，还记得我昨天为什么要来买花吗？")
print(result)

```

第一回合的输出：

```plain
{'input': '我姐姐明天要过生日，我需要一束生日花束。',
'history': '',
 'response': ' 哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？'}

```

第二回合的输出：

```plain
{'input': '她喜欢粉色玫瑰，颜色是粉色的。',
'history': 'Human: 我姐姐明天要过生日，我需要一束生日花束。\nAI:  哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？',
'response': ' 好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？'}

```

第三回合的输出：

```plain
{'input': '我又来了，还记得我昨天为什么要来买花吗？',
'history': 'Human: 她喜欢粉色玫瑰，颜色是粉色的。\nAI:  好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？',
'response': '  当然记得，你昨天来买花是为了给你喜欢的人送一束粉色玫瑰花束，表达你对TA的爱意。'}

```

In the given example, setting ( k=1 ) means the window will only remember the latest interaction with the AI, retaining only the last human response and the AI’s response.

In the third round, when we ask, “Do you remember why I came to buy flowers yesterday?”, since we only keep the most recent interaction (( k=1 )), the model has forgotten the correct answer. So, although it says it remembers, it can only vaguely say “someone you like” without mentioning the keyword “sister.” However, if (just if) in the second round, the model could answer, “I can help you find flowers for your sister,” then, even without the first round’s history, the model might still infer the true intention of buying flowers yesterday based on the information from the last round.

Although this method is not suitable for remembering distant interactions, it is very good at limiting the number of tokens used. If you only need to remember recent interactions, buffer window memory is a good choice. However, if you need to mix long-term and short-term interaction information, there are other options.

## Using ConversationSummaryMemory
As mentioned above, if the model could say “I can help you find flowers for your sister” in the second round, then in the third round, even with a window size of ( k=1 ), it could still answer correctly.

Why is this?

Because the model **summarizes the previous questions when answering new ones**.

The idea of ConversationSummaryMemory is to summarize the conversation history and then pass it to the {history} parameter. This method aims to avoid excessive token usage by summarizing previous conversations.

ConversationSummaryMemory has several key features:

- 1. Summarizing Conversations: This method does not save the entire conversation history but summarizes each new interaction and adds it to the “running summary” of all previous interactions.
- 2. Using LLM for Summarization: The summarization function is driven by another LLM, meaning the AI itself performs the conversation summarization.
- 3. Suitable for Long Conversations: This method is particularly advantageous for long conversations. Although the initial token usage is higher, the growth rate of the summarization method slows down as the conversation progresses. Meanwhile, the regular buffer memory model continues to grow linearly.
Let’s look at a code example using ConversationSummaryMemory.

```plain
from langchain.chains.conversation.memory import ConversationSummaryMemory

# 初始化对话链
conversation = ConversationChain(
    llm=llm,
    memory=ConversationSummaryMemory(llm=llm)
)

```

第一回合的输出：

```plain
{'input': '我姐姐明天要过生日，我需要一束生日花束。',
'history': '',
'response': ' 我明白，你需要一束生日花束。我可以为你提供一些建议吗？我可以推荐一些花束给你，比如玫瑰，康乃馨，百合，仙客来，郁金香，满天星等等。挑选一束最适合你姐姐的生日花束吧！'}

```

第二回合的输出：

```plain
{'input': '她喜欢粉色玫瑰，颜色是粉色的。',
'history': "\nThe human asked what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asked the AI for advice on what type of flower bouquet to get for their sister's birthday, to which the AI provided a variety of suggestions.",
'response': ' 为了为你的姐姐的生日准备一束花，我建议你搭配粉色玫瑰和白色康乃馨。你可以在玫瑰花束中添加一些紫色的满天星，或者添加一些绿叶以增加颜色对比。这将是一束可爱的花束，让你姐姐的生日更加特别。'}

```

第三回合的输出：

```plain
{'input': '我又来了，还记得我昨天为什么要来买花吗？',
'history': "\n\nThe human asked what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asked the AI for advice on what type of flower bouquet to get for their sister's birthday, to which the AI suggested pink roses and white carnations with the addition of purple aster flowers and green leaves for contrast. This would make a lovely bouquet to make the sister's birthday extra special.",
'response': ' 确实，我记得你昨天想买一束花给你的姐姐作为生日礼物。我建议你买粉红色的玫瑰花和白色的康乃馨花，再加上紫色的雏菊花和绿叶，这样可以让你的姐姐的生日更加特别。'}

```

I see that here, the 'history' parameter is no longer a simple copy-paste of the previous human-AI conversation but a summarized and organized overview.

Here, **we not only use the LLM to answer each round of questions but also use the LLM to provide a summary statement of the previous conversation to save on token usage**. The LLM that helps us summarize the conversation and the one used to answer questions can be the same large model or different large models.

The advantage of ConversationSummaryMemory is that it can reduce the number of tokens used for long conversations, allowing more rounds of conversation information to be recorded. It is also intuitive and easy to use. However, its drawback is that for shorter conversations, it may lead to higher token usage. Additionally, the memory of the conversation history entirely depends on the summarizing LLM’s ability, which requires tokens for summarization, increasing costs and not limiting conversation length.

By summarizing conversation history to optimize and manage token usage, ConversationSummaryMemory provides a good method for scenarios expecting multiple rounds of long-term conversations. However, this method is still limited by the number of tokens. After a while, we will still exceed the context window limit of the large model.

Moreover, the summarization process does not distinguish between recent and long-term conversations (recent conversations are usually more important), so we need to continue looking for new memory management methods.

## Using ConversationSummaryBufferMemory
The last memory mechanism I want to introduce to you is ConversationSummaryBufferMemory, a hybrid memory model that combines various memory mechanisms mentioned above, including ConversationSummaryMemory and ConversationBufferWindowMemory. This model aims to summarize early interactions in the conversation while retaining the original content of recent interactions as much as possible.

It achieves this through the max_token_limit parameter. When the length of the latest conversation text is within 300 words, LangChain will remember the original conversation content; when the conversation text exceeds this length, the model will summarize all content exceeding the preset length to save on token usage.

```plain
from langchain.chains.conversation.memory import ConversationSummaryBufferMemory

# 初始化对话链
conversation = ConversationChain(
    llm=llm,
    memory=ConversationSummaryBufferMemory(
        llm=llm,
        max_token_limit=300))

```

第一回合的输出：

```plain
{'input': '我姐姐明天要过生日，我需要一束生日花束。',
'history': '',
'response': ' 哇，你姐姐要过生日啊！那太棒了！我建议你去买一束色彩鲜艳的花束，因为这样可以代表你给她的祝福和祝愿。你可以去你家附近的花店，或者也可以从网上订购，你可以看看有没有特别的花束，比如彩色玫瑰或者百合花，它们会更有特色。'}

```

第二回合的输出：

```plain
{'input': '她喜欢粉色玫瑰，颜色是粉色的。',
'history': 'Human: 我姐姐明天要过生日，我需要一束生日花束。\nAI:  哇，你姐姐要过生日啊！那太棒了！我建议你去买一束色彩鲜艳的花束，因为这样可以代表你给她的祝福和祝愿。你可以去你家附近的花店，或者也可以从网上订购，你可以看看有没有特别的花束，比如彩色玫瑰或者百合花，它们会更有特色。',
'response': ' 好的，那粉色玫瑰就是一个很好的选择！你可以买一束粉色玫瑰花束，这样你姐姐会很开心的！你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品'}

```

第三回合的输出：

```plain
{'input': '我又来了，还记得我昨天为什么要来买花吗？',
'history': "System: \nThe human asked the AI for advice on buying a bouquet for their sister's birthday. The AI suggested buying a vibrant bouquet as a representation of their wishes and blessings, and recommended looking for special bouquets like colorful roses or lilies for something more unique.\nHuman: 她喜欢粉色玫瑰，颜色是粉色的。\nAI:  好的，那粉色玫瑰就是一个很好的选择！你可以买一束粉色玫瑰花束，这样你姐姐会很开心的！你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品",
'response': ' 是的，我记得你昨天来买花是为了给你姐姐的生日。你想买一束粉色玫瑰花束来表达你的祝福和祝愿，你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品}

```

Exactly! In the second round, the memory mechanism fully recorded the first round of conversation. However, by the third round, it detected that the first two rounds exceeded 300 bytes, so it summarized the earlier conversations to save token resources.

The advantage of ConversationSummaryBufferMemory is that it can recall earlier interactions through summarization while ensuring that recent interactions are not missed due to the buffer. Of course, for shorter conversations, ConversationSummaryBufferMemory may also increase the number of tokens used.

Overall, ConversationSummaryBufferMemory offers a lot of flexibility. It is the only memory type so far that can recall earlier interactions and fully store recent ones. In terms of saving tokens, ConversationSummaryBufferMemory is competitive compared to other methods.

## Summary

Here are the comparision between 4 conversional memory

|Memory Mechanism Type	| Features	| Advantages	| Disadvantages	 | Applicable Scenarios
|ConversationBufferMemory	| Saves the entire conversation history	| Simple and direct, provides the maximum amount of information	| Slower response time and higher cost as conversation length increases	| Suitable for short conversations or scenarios requiring complete history|
|ConversationBufferWindowMemory |	Only saves the most recent interactions	| Limits token usage, retains the latest information	| Cannot remember earlier interactions	| Suitable for scenarios needing to remember recent interactions|
|ConversationSummaryMemory	| Summarizes			

![](images/704183/a06b5db35405b74yy317de917eacbdc0.jpg)

Of course, ConversationBufferWindowMemory is the most straightforward in terms of saving tokens, but it completely forgets the conversation content before the ( k ) rounds, making it not the best choice for certain scenarios.









