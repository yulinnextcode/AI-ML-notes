In the previously introduced Chain of Thought (CoT), I demonstrated the ability of LLMs to execute reasoning trajectories. Before providing an answer, the large model can achieve complex reasoning through intermediate reasoning steps (especially when combined with few-shot prompting) to obtain better results and complete more challenging tasks.

However, merely applying Chain of Thought reasoning does not solve the inherent problem of large models: the inability to actively update their knowledge, leading to factual hallucinations. In other words, due to the lack of contact with the external world, the large model only possesses the knowledge it encountered during training and the additional knowledge provided in the prompt as context. If you ask a question beyond its knowledge scope, the large model will either honestly tell you, “My training data cuts off at XXXX-XX-XX,” or it will start making things up in a serious manner.

How can this problem be solved?

It’s not difficult. You can have the large model first search the local knowledge base to check the accuracy of the information in the prompt. If it is accurate, then output it; if it is not accurate, then correct it. If the local knowledge base does not have the corresponding information, you can call tools for external searches to verify the accuracy of the prompt information.

![](images/707191/7032d003ac36e858cbb53f90bb4f3a1b.jpg)

The aforementioned local knowledge base and search engines are not encapsulated within the large model’s internal knowledge; we refer to them as “external tools.”

## The Role of Agents

Whenever you encounter a situation where the model needs to make autonomous judgments, call tools on its own, and decide the next steps independently, the Agent comes into play.

![](images/707191/e26993dd3957bfd2947424abb9de7cde.png)

An agent acts like a multifunctional interface that can access and use a set of tools. Based on the user’s input, the agent decides which tools to call. It can not only use multiple tools simultaneously but also use the output data of one tool as the input for another.

To use agents in LangChain, we only need to understand the following three elements.

- Large Model: Provides the logical engine, responsible for generating predictions and processing inputs.
- External Tools: Interact with the large model and may include data cleaning tools, search engines, applications, etc.
- Agent: Controls the interaction, calls the appropriate external tools, and manages the flow of the entire interaction process.

![](images/707191/9a9550e7df156d15975dc027b3201d31.png)

The above idea seems simple, but it is worth our careful consideration.

There are many points in this process where the large model needs to autonomously determine the next action (operation). Without guidance, the large model itself does not have this capability. For example, consider the following series of operations:

- When to start searching the local knowledge base (this is relatively simple, as it can be preset as the first step)?
- How to determine that the local knowledge base search is complete and the next step can begin?
- Which external search tool to call (e.g., Google engine)?
- How to determine that the external search tool has returned the desired content?
- How to determine that the verification of the information’s accuracy is complete and the next step can begin?
So, how does the agent in LangChain autonomously plan, judge, and execute actions?

## ReAct Framework
Here, I want you to think about this: If you receive a new task, how do you make decisions and complete the next action?

For example, if you run a flower shop and often experience price changes due to weather fluctuations, how would you price your flowers every morning?

You might tell me that you would check the cost price of flowers on Google (action), which is the expected purchase price. Then, based on the price (observation), you would determine how much to mark up (thinking), and finally calculate a selling price (action)!

![](images/707191/58bdbe17948a0ed2d52ceb3557194a12.png)

You see, in this simple example, you observe, think, and then take specific actions. Here, observation and thinking are collectively referred to as the reasoning process, which guides your actions.

The ReAct framework we are discussing today draws inspiration from the synergy between “acting” and “reasoning,” which enables humans to learn new tasks and make decisions or inferences. This framework is also the theoretical basis for large models to act as “intelligent agents,” autonomously and continuously generating reasoning trajectories and task-specific actions.

First, let me clarify that this ReAct does not refer to the popular front-end development framework React. Here, it specifically refers to a thinking framework that guides large language models in reasoning and acting. This framework was proposed by Shunyu Yao et al. in their ICLR 2023 conference paper, “ReAct: Synergizing Reasoning and Acting in Language Models.”

A key insight from this paper is that large language models can achieve greater synergy by generating reasoning traces and task-specific actions.

Specifically, it involves guiding the model to generate a task-solving trajectory: observing the environment, thinking, and taking action, i.e., observe-think-act. Simplified further, it becomes the Reasoning-Acting framework.

Reasoning includes observing the current environment and state and generating reasoning trajectories. This allows the model to induce, track, and update action plans, and even handle exceptions. Acting involves guiding the large model to take the next step, such as interacting with external sources (like knowledge bases or environments) to gather information or provide the final answer.

Each reasoning process in ReAct is meticulously documented, improving the model’s explainability and reliability in problem-solving. This framework has shown good results in various language and decision-making tasks.

Let’s illustrate this with a specific example. Suppose I give the large model a task: find a pepper shaker in a virtual environment and place it in a drawer.

In this task, a model without reasoning ability cannot search various corners of the room or determine the next action after finding the pepper shaker, thus failing to complete the task. With ReAct, this series of sub-goals will be specifically captured in each reasoning process.

![](images/707191/638e1b0098211b1b622283e0f7100596.png)

Now, let’s return to the problem we faced at the beginning. By using Chain of Thought (CoT) prompts alone, LLMs can execute reasoning trajectories to solve arithmetic and common-sense reasoning problems. However, such models, due to their lack of interaction with the external world or inability to update their knowledge, can lead to hallucinations.

![](images/707191/1189768e0ae5b6199fd6db301d2401c8.png)

Combining the ReAct framework with Chain of Thought (CoT) prompts allows large models to use both internal knowledge and external information during the reasoning process. This results in more reliable and practical responses, enhancing the explainability and credibility of LLMs.

LangChain has perfectly encapsulated and implemented the ReAct framework through the Agent class, significantly increasing the autonomy of large models. Your large model has now evolved from a Bot that can only chat using its internal knowledge to an intelligent agent capable of using tools.

The ReAct framework prompts LLMs to generate reasoning trajectories and actions for tasks. This enables agents to systematically perform dynamic reasoning to create, maintain, and adjust action plans. It also supports interaction with external environments (such as Google Search, Wikipedia) to incorporate additional information into the reasoning process.

## Implementing the ReAct Framework with Agents
Let’s use one of the most common agent types in LangChain, ZERO_SHOT_REACT_DESCRIPTION, to analyze how an LLM performs reasoning under the guidance of the ReAct framework.

Here, we will give the agent a task: find the current market price of roses and then calculate the new price with a 15% markup.

Before we start, there’s a preparation step: you need to register an account on serpapi.com and obtain your SERPAPI_API_KEY, which will provide the Google search tool for the large model.

![](images/707191/1841f5d709cd27f1000ee9a5b593325b.png)

先安装SerpAPI的包。

```
pip install google-search-results

```

设置好OpenAI和SerpAPI的API密钥。

```
# 设置OpenAI和SERPAPI的API密钥
import os
os.environ["OPENAI_API_KEY"] = 'Your OpenAI API Key'
os.environ["SERPAPI_API_KEY"] = 'Your SerpAPI API Key'

```

再导入所需的库。

```plain
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.llms import OpenAI

```

然后加载将用于控制代理的语言模型。

```plain
llm = OpenAI(temperature=0)

```

接下来，加载一些要使用的工具，包括serpapi（这是调用Google搜索引擎的工具）以及llm-math（这是通过LLM进行数学计算的工具）。

```plain
tools = load_tools(["serpapi", "llm-math"], llm=llm)

```

最后，让我们使用工具、语言模型和代理类型来初始化代理。

```plain
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

```

好了，现在我们让代理来回答我刚才提出的问题了！目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？

```plain
agent.run("目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？")

```

大模型成功遵循了ReAct框架，它输出的思考与行动轨迹如下：

```plain
> Entering new  chain...
 I need to find the current market price of roses and then calculate the new price with a 15% markup.
Action: Search
Action Input: "Average price of roses"
Observation: According to the study, the average price for a dozen roses in the United States is $80.16. The Empire State hovers closer to that number than its neighbors, with a bouquet setting back your average New Yorker $78.33.
Thought: I need to calculate the new price with a 15% markup.
Action: Calculator
Action Input: 80.16 * 1.15
Observation: Answer: 92.18399999999998
Thought: I now know the final answer.
Final Answer: The new price with a 15% markup would be $92.18.
> Finished chain.

```

![](images/707191/c99893b6d8311d9ac95aeb8d818e1914.png)

可以看到，ZERO\_SHOT\_REACT\_DESCRIPTION类型的智能代理在LangChain中，自动形成了一个完善的思考与行动链条，而且给出了正确的答案。

你可以对照下面这个表格，再巩固一下这个链条中的每一个环节。

![](images/707191/56fbe79e086052895f301383c27f4a0c.jpg)

这个思维链条中，智能代理有思考、有观察、有行动，成功通过搜索和计算两个操作，完成了任务。在下一讲中，我们将继续深入剖析LangChain中的不同类型的代理，并利用它完成更为复杂的任务。

## Summary

In this lesson, we introduced what agents are in LangChain. More importantly, we discussed the driving force behind agents’ autonomous actions—the ReAct framework.

Through the ReAct framework, large models are guided to generate a task-solving trajectory, which involves observing the environment, thinking, and taking action. The observation and thinking stages are collectively referred to as reasoning, while the stage of implementing the next action is called acting. Each step in the reasoning process is meticulously documented, improving the model’s explainability and reliability in problem-solving.

- In the reasoning stage, the model observes the current environment and state, generating reasoning trajectories. This allows the model to induce, track, and update action plans, and even handle exceptions.
- In the acting stage, the model takes the next action, such as interacting with external sources (like knowledge bases or environments) to gather information or provide the final answer.

These advantages of the ReAct framework give it tremendous potential for future development. As technology advances, we can expect the ReAct framework to handle more and more complex tasks. Especially with the development of embodied intelligence, the ReAct framework will enable intelligent agents to perform more complex interactions in virtual or real environments. For example, intelligent agents might navigate in virtual environments or manipulate physical objects in real environments. This will greatly expand the application scope of AI, allowing it to better serve our lives and work.










