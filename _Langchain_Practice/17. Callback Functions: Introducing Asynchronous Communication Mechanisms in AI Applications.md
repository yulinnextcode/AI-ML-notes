## Callback Functions and Asynchronous Programming

You might be familiar with callback functions. A callback function (Function A) is passed as an argument to another function (Function B) and is executed inside Function B. When Function B completes certain operations, it calls (or “callbacks”) Function A. This programming pattern is common in handling asynchronous operations, such as event listeners, timed tasks, or network requests.

In programming, asynchronous typically refers to the ability of code to continue executing without waiting for a particular operation (like I/O operations, network requests, database queries, etc.) to complete. The implementation of asynchronous mechanisms involves event loops, task queues, and other complex underlying mechanisms. This contrasts with synchronous programming, where operations must be completed in the order they appear.

Here is a simple example of a callback function.

```plain
def compute(x, y, callback):
    result = x + y
    callback(result)

def print_result(value):
    print(f"The result is: {value}")

def square_result(value):
    print(f"The squared result is: {value**2}")

# 使用print_result作为回调
compute(3, 4, print_result)  # 输出: The result is: 7

# 使用square_result作为回调
compute(3, 4, square_result)  # 输出: The squared result is: 49

```
而下面的例子，就是在异步操作时使用回调函数的示例。

```plain
import asyncio

async def compute(x, y, callback):
    print("Starting compute...")
    await asyncio.sleep(0.5)  # 模拟异步操作
    result = x + y
    # callback(result)
    print("Finished compute...")

def print_result(value):
    print(f"The result is: {value}")

async def another_task():
    print("Starting another task...")
    await asyncio.sleep(1)
    print("Finished another task...")

async def main():
    print("Main starts...")
    task1 = asyncio.create_task(compute(3, 4, print_result))
    task2 = asyncio.create_task(another_task())

    await task1
    await task2
    print("Main ends...")

asyncio.run(main())

```
In this example, when we call asyncio.create_task(compute(3, 4, print_result)), the compute function starts executing. When it encounters await asyncio.sleep(2), it pauses and hands control back to the event loop. At this point, the event loop can choose to start executing another_task, which is another asynchronous task. This way, you can clearly see that even though the compute function hasn’t finished, the another_task function can start and complete. This is asynchronous programming, allowing you to perform multiple operations simultaneously without waiting for one to finish before starting another.

## Callback Handlers in LangChain
LangChain’s callback mechanism allows you to perform custom operations at different stages of your application, such as logging, monitoring, and data stream processing. This mechanism is implemented through CallbackHandler objects.

A callback handler in LangChain is an object that implements the CallbackHandler interface, providing a method for each type of event that can be monitored. When an event is triggered, the CallbackManager calls the appropriate method on these handlers.

BaseCallbackHandler is the most basic callback handler, which you can extend to create your own. It includes various methods such as on_llm_start/on_chat (called when the LLM starts running) and on_llm_error (called when the LLM encounters an error).

LangChain also provides some built-in handlers, such as StdOutCallbackHandler, which logs all events to the standard output, and FileCallbackHandler, which logs all events to a specified file.

## Using Callback Handlers in Components

LangChain components, such as Chains, Models, Tools, and Agents, offer two types of callback settings: constructor callbacks and request callbacks. You can pass a callback handler when initializing LangChain or use a callback in individual requests. For example, if you want to log all requests in a chain, you can pass the handler during initialization. If you only want to use a callback for a specific request, you can pass it during the request.

Here is the difference between the two methods:

![](images/713685/a593e19a4c3693365756a5c34a96355e.jpg)

Below is a sample code snippet that uses LangChain to perform a simple task, combining LangChain’s callback mechanism with the loguru logging library to output relevant events to both the standard output and the "output.log" file.

```plain
from loguru import logger

from langchain.callbacks import FileCallbackHandler
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

logfile = "output.log"

logger.add(logfile, colorize=True, enqueue=True)
handler = FileCallbackHandler(logfile)

llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

# this chain will both print to stdout (because verbose=True) and write to 'output.log'
# if verbose=False, the FileCallbackHandler will still write to 'output.log'
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=True)
answer = chain.run(number=2)
logger.info(answer)

```
When initializing LLMChain, specifying the verbose parameter is equivalent to adding a callback handler that outputs to the console. This is very useful when debugging your program, as it will output all event information to the console.

In short, LangChain provides a flexible way to monitor and operate different stages of your application through its callback system.

## Custom Callback Functions
We can also customize callback functions using BaseCallbackHandler and AsyncCallbackHandler. Here is an example.

```plain
import asyncio
from typing import Any, Dict, List

from langchain.chat_models import ChatOpenAI
from langchain.schema import LLMResult, HumanMessage
from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler

# 创建同步回调处理器
class MyFlowerShopSyncHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"获取花卉数据: token: {token}")

# 创建异步回调处理器
class MyFlowerShopAsyncHandler(AsyncCallbackHandler):

    async def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        print("正在获取花卉数据...")
        await asyncio.sleep(0.5)  # 模拟异步操作
        print("花卉数据获取完毕。提供建议...")

    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        print("整理花卉建议...")
        await asyncio.sleep(0.5)  # 模拟异步操作
        print("祝你今天愉快！")

# 主要的异步函数
async def main():
    flower_shop_chat = ChatOpenAI(
        max_tokens=100,
        streaming=True,
        callbacks=[MyFlowerShopSyncHandler(), MyFlowerShopAsyncHandler()],
    )

    # 异步生成聊天回复
    await flower_shop_chat.agenerate([[HumanMessage(content="哪种花卉最适合生日？只简单说3种，不超过50字")]])

# 运行主异步函数
asyncio.run(main())

```

In this flower shop customer service program, when a customer asks for flower recommendations, we use both a synchronous and an asynchronous callback.

MyFlowerShopSyncHandler is a synchronous callback that simply prints out the flower data being fetched whenever a new token is generated.

MyFlowerShopAsyncHandler, on the other hand, is asynchronous. When the customer service starts providing flower recommendations, it simulates asynchronous data fetching. After the recommendation is completed, it also simulates a finishing operation, such as sending a thank you note to the customer.

This combination of synchronous and asynchronous operations allows the program to handle customer requests more efficiently while providing real-time feedback.

**The asynchronous aspect is reflected in several ways:**

- 1. Simulated Delay Operation: In MyFlowerShopAsyncHandler, we use await asyncio.sleep(0.5) to simulate the process of asynchronously fetching flower information from other requests. When this await statement is executed, the current on_llm_start function “pauses” and releases control back to the event loop. This means that during this sleep period, other asynchronous tasks (such as requests from other customers) can be processed.
- 2. Callback Mechanism: When ChatOpenAI processes each new token, it calls the on_llm_new_token method. Since this is a synchronous callback, it outputs immediately. However, the asynchronous start and end callbacks on_llm_start and on_llm_end have a small delay operation at the beginning and end, simulated by await asyncio.sleep(0.5).
- 3. Event Loop: Python’s asyncio library provides an event loop that allows multiple asynchronous tasks to run concurrently. In our example, although it seems that all operations happen sequentially, because we use asynchronous operations and callbacks, if there are other concurrent tasks, they can run during the await pause.
To more clearly demonstrate the advantages of asynchronous operations, we would typically run multiple asynchronous tasks simultaneously in a program and observe how they “concurrently” execute. But in this simple example, we mainly demonstrate the basic mechanism of asynchronous operations through simulated delays.

Thus, callback functions provide a mechanism for asynchronous operations, allowing you to define “what to do when the operation is complete,” while the actual implementation of asynchronous mechanisms involves deeper underlying work, such as event loops and task scheduling.

## Constructing a Token Counter with get_openai_callback
Next, I’ll guide you through using LangChain’s callback functions to construct a token counter. This counting function is crucial for monitoring the consumption of large model sessions and controlling costs.

Before constructing the token counter, let’s recall the memory mechanism from Lesson 10. We generated ConversationBufferMemory with the following code.

```plain
from langchain import OpenAI
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory

# 初始化大语言模型
llm = OpenAI(
    temperature=0.5,
    model_name="text-davinci-003")

# 初始化对话链
conversation = ConversationChain(
    llm=llm,
    memory=ConversationBufferMemory()
)

# 第一天的对话
# 回合1
conversation("我姐姐明天要过生日，我需要一束生日花束。")
print("第一次对话后的记忆:", conversation.memory.buffer)

# 回合2
conversation("她喜欢粉色玫瑰，颜色是粉色的。")
print("第二次对话后的记忆:", conversation.memory.buffer)

# 回合3 （第二天的对话）
conversation("我又来了，还记得我昨天为什么要来买花吗？")
print("/n第三次对话后时提示:/n",conversation.prompt.template)
print("/n第三次对话后的记忆:/n", conversation.memory.buffer)

```

At the same time, we also provided an estimate diagram of the token consumption for various memory mechanisms.

![](images/713685/b605f14e7c9151c5172fff5860285e52.png)

However, this diagram is just an estimate. To accurately measure how many tokens each memory mechanism consumes, we need to use callback functions.

Next, we will refactor this program using the callback function mechanism. To do this, we first need to ensure that we use the get_openai_callback context manager when interacting with the large language model.

In Python, a context manager is typically used to manage resources such as files or network connections, which need to be set up before use and cleaned up afterward. Context managers are often used with the with statement to ensure resources are properly set up and cleaned up.

get_openai_callback is designed to monitor the number of tokens used when interacting with OpenAI. When you enter this context, it tracks token usage through a listener. When you exit the context, it cleans up the listener and provides a total token count. In this way, it acts as a callback mechanism, allowing you to perform specific actions or collect specific information when certain events occur.

Here is the specific code:

```plain
from langchain import OpenAI
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain.callbacks import get_openai_callback

# 初始化大语言模型
llm = OpenAI(temperature=0.5, model_name="text-davinci-003")

# 初始化对话链
conversation = ConversationChain(
    llm=llm,
    memory=ConversationBufferMemory()
)

# 使用context manager进行token counting
with get_openai_callback() as cb:
    # 第一天的对话
    # 回合1
    conversation("我姐姐明天要过生日，我需要一束生日花束。")
    print("第一次对话后的记忆:", conversation.memory.buffer)

    # 回合2
    conversation("她喜欢粉色玫瑰，颜色是粉色的。")
    print("第二次对话后的记忆:", conversation.memory.buffer)

    # 回合3 （第二天的对话）
    conversation("我又来了，还记得我昨天为什么要来买花吗？")
    print("/n第三次对话后时提示:/n",conversation.prompt.template)
    print("/n第三次对话后的记忆:/n", conversation.memory.buffer)

# 输出使用的tokens
print("\n总计使用的tokens:", cb.total_tokens)

```
这里，我使用了get\_openai\_callback上下文管理器来监控与ConversationChain的交互。这允许我们计算在这些交互中使用的总Tokens数。

输出：

```plain
总计使用的tokens: 966

```

下面，我再添加了一个additional\_interactions异步函数，用于演示如何在多个并发交互中计算Tokens。

> 当我们讨论异步交互时，指的是我们可以启动多个任务，它们可以并发（而不是并行）地运行，并且不会阻塞主线程。在Python中，这是通过asyncio库实现的，它使用事件循环来管理并发的异步任务。

```plain
import asyncio
# 进行更多的异步交互和token计数
async def additional_interactions():
    with get_openai_callback() as cb:
        await asyncio.gather(
            *[llm.agenerate(["我姐姐喜欢什么颜色的花？"]) for _ in range(3)]
        )
    print("\n另外的交互中使用的tokens:", cb.total_tokens)

# 运行异步函数
asyncio.run(additional_interactions())

```

简单解释一下。

1. `async def`：这表示additional\_interactions是一个异步函数。它可以使用await关键字在其中挂起执行，允许其他异步任务继续。
2. `await asyncio.gather(...)`：这是asyncio库提供的一个非常有用的方法，用于并发地运行多个异步任务。它会等待所有任务完成，然后继续执行。
3. `*[llm.agenerate(["我姐姐喜欢什么颜色的花？"]) for _ in range(3)]`：这实际上是一个Python列表解析，它生成了3个 llm.agenerate(…)的异步调用。asyncio.gather将并发地运行这3个调用。

由于这3个llm.agenerate调用是并发的，所以它们不会按顺序执行，而是几乎同时启动，并在各自完成时返回。这意味着，即使其中一个调用由于某种原因需要更长时间，其他调用也不会被阻塞，它们会继续并完成。

## Summary Time
Callback functions are an important and widely used concept in computer science, allowing us to execute specific code at particular times or under certain conditions.

Callback functions have many applications in development:

- 1. Asynchronous Programming: In JavaScript, callback functions are often used for asynchronous programming. For example, when you send an AJAX request to a server, you can provide a callback function that will be called when the server’s response arrives.
- 2. Event Handling: In many programming languages and frameworks, callback functions are used as event handlers. For instance, you might write a callback function to handle user click events, which will be called when a user clicks a button.
- 3. Timers: You can use callback functions to create timers. For example, you can use JavaScript’s setTimeout or setInterval functions and provide a callback function that will be called after a specified time.
In LangChain, the callback mechanism also provides users with flexibility and customization capabilities to better control and respond to events. CallbackHandler allows developers to inject custom behavior at specific stages or conditions in the chain, such as response handling in asynchronous programming or event handling in event-driven programming. This provides LangChain with flexibility and extensibility, making it adaptable to various application scenarios.













