# 1. Variable relavance
```python
import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt

A = np.array([[1,2,3],[4,5,10]]).T
np.corrcoef(A[:,0], A[:,1])

X=np.random.randn(20)
y=X+1

np.corrcoef(X,y)

a=y.shape
ran=np.random.normal(size=a)
delta=0.5
r=ran*delta
y1=y+r

plt.subplot(121)  # 1 row 2 column 1st
plt.plot(X,y,'o')
plt.title('y=x+1')
plt.subplot(122)  # 1 row 2 column 2nd
plt.plot(x,y1,'o')
plt.title('y=x+1+r')
```
# 2. Dataset generator
```python
num_inputs=2
num_examples=1000

np.random.seed(24)
np.random.randn(2,2)
w_true=np.array([2,-1]).reshaoe(-1,1)
b_true=np.array(1)

delta=0.01
features=np.random.randn(num_examples, num_inputs)
labels_true=features.dot(w_true)+b_true
labels=labels_true+np.random.normal(size=labels_true.shape)*delta

plt.scatter(features, labels)
```
Create data generator
```python
def arrayGenReg(num_examples=1000, w=[2,-1,1], bias=True, delta=0.01, deg=1)
  if bias==True:
    num_inputs=len(w)-1
    features_true=np.random.randn(num_examples, num_inputs)
    w_true=np.array(w[:-1]).reshape(-1,1)
    b_true=np.array(w[-1])
    labels_true=np.power(features_true, deg).dot(w_true)+b_true
    features=np.concatenate((features_true, np.ones_like(labels_true)), axis=1)
  else:
    num_inputs=len(w)
    features=np.random.randn(num_examples, num_inputs)
    w_true=np.array(w).reshape(-1,1)
    labels_true=np.power(features, deg).dot(w_true)
  labels=labels_true+np.random.normal(size=labels_true.shape)*delta
  return features, labels
```
**Randome seed**
```python

np.random.seed(24)
np.random.randn(9)   # output a no matter how run many times

np.random.randn(9)   # will output b

np.random.seed(24)
np.random.randn(9)  # output a

np.random.randn(9)  # will still output b 
```
```python
import random

l=list(range(5))

random.seed(24)
random.shuffle(1)  #[4,2,0,1,3]

random.shuffle(l)  #[1，4，0，3，2]

l=list(range(5))

random.seed(24)
random.shuffle(1)  #[4,2,0,1,3]

random.shuffle(l)  #[1，4，0，3，2]

l=list(range(5))

np.random.seed(24)
random.shuffle(1)  #[4,0,3,2,1], can not repeat because np.randome.seed is different from random.seed
```
> [!IMPORTANT]
> Based on above codes, np module's np.random.seed() does not affect random module. User should use corresponding module's random seed function.
> random module use random.seed()
> numpy module use np.random.seed()
# 3. Manually implementation of linear regression
Follow steps to create linear regression models:
- Step 1, Prepare data
- Step 2, adopt models
- Step 3, construct loss function
```python
np.random.seed(24)
w=np.random.randn(3).reshape(-1,1)

y_hat=features.dot(w)

def SSELoss(X,w,y):
  y_hat=X.dot(w)
  SSE=(y-y_hat).T.dot(y-y_hat)
  return SSE

SSELoss(features,w,labels)
```
- Step 4, use LSE to solve loss function
```python
np.linalg.det(features.T.dot(features)) # not 0 means can be inverse
w=np.linalg.inv(features.T.dot(features)).dot(features.T).dot(labels)
```
The limitation of linear regression models:
- The relationship could be nonlinear
- XTX could be not inversible. At this time, needs to add another item in the loss function to make avoid XTX inversible:
  - add 1-norm will be Lasso regression, can also be called L1 regulerazation
  - add 2-norm will be ridge regression, can also be called L2 regularazation
  - add both 1-norm and 2-norm will be elastic-net regression

**Except SSE, we can also use R-square, SSR, SST**
# 4. Model reliability and cross validation
```python
import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt

from ML_basic_function import *

np.random.seed(24)
np.random.shuffle(A)

np.vsplit(A, [2,])
def array_split(fatures, labels, rate=0.7, random_state=24):
  np.random.seed(random_state)
  np.random.shuffle(features)
  np.random.seed(random_state)
  np.random.shuffle(labels)
  num_input=len(labels)
  split_indices=int(num_input*rate)
  Xtrain, Xtest=np.vsplit(features, [split_indices,])
  ytrain, ytest=np.vsplit(labels, [split_indices,])
  return Xtrain, Xtest, ytrain, ytest
```
Manually implement linear regression
```python
#Prepare data
np.random.seed(24)

features, labels=arrayGenReg(delta=0.01)

Xtrain,Xtest,ytrain,ytesarray_split(features, labels)

w = np.linalg.inv(Xtrain.T.dot(Xtrain)).dot(Xtrain.T).dot(ytrain)

SSELoss(Xtest,w,ytest)
```
**Cross validation is mainly used for hyper-parameters, train/test split is used for parameters optimization**





























