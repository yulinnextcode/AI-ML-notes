<details>
<summary><h1>1. Boosting method</h1></summary>

## 1.1 Boosting principals and procedure

Boosting is a well-known ensemble learning strategy that combines the predictions of numerous base models to produce a more robust overall model.

>Unlike many ML models which focus on high quality prediction done by a single model, boosting algorithms seek to improve the prediction power by training a sequence of weak models, each compensating the weaknesses of its predecessors.

>![Python_File_Operation](https://miro.medium.com/v2/resize:fit:786/format:webp/1*4XuD6oRrgVqtaSwH-cu6SA.png)
>Boosting is a machine learning strategy that combines numerous weak learners into strong learners to increase model accuracy. The following are the steps in the boosting algorithm:
>
>1. Initialise weights: At the start of the process, each training example is given equal weight.
>  
>2. Train a weak learner: The weighted training data is used to train a weak learner. A weak learner is a simple model that outperforms random guessing only marginally. A decision tree with a few levels, for example, can be employed as a weak learner.
>
>3. Error calculation: The error of the weak learner on the training data is computed. The weighted sum of misclassified cases constitutes the error.
>  
>4. Update weights: Weights are updated according to the mistake rate of the training examples. Misclassified examples are given higher weights, whereas correctly classified examples are given lower weights.
>
>5. Repeat: Steps 2–4 are repeated several times. A new weak learner is trained on the updated weights of the training examples in each cycle.
>  
>6. Combine weak learners: The final model is made up of all of the weak learners that were trained in the preceding steps. The accuracy of each weak learner is weighted, and the final prediction is based on the weighted total of the weak learners.
>
>7. Forecast: The finished model is used to forecast fresh instances’ class labels.
>
>The boosting approach is designed to produce a strong learner that is accurate on the training data and can generalize effectively to new data. The algorithm can produce a model that is more accurate than any of the >individual weak learners by merging many weak learners.

>To understand Boosting, it is crucial to recognize that boosting is a generic algorithm rather than a specific model. Boosting needs you to specify a weak model (e.g. regression, shallow decision trees, etc) and then improves it.
>
>![Python_File_Operation](https://miro.medium.com/v2/resize:fit:720/format:webp/1*_DlfhMI2OFewBB4R-izszg.png)
>
>'More details can be found [here](https://medium.com/@brijesh_soni/understanding-boosting-in-machine-learning-a-comprehensive-guide-bdeaa1167a6).
>

## 1.2 Boosting vs Bagging

>![Python_File_Operation](https://miro.medium.com/v2/resize:fit:1400/0*o4zHKed7WKVdifju)

>![Python_File_Operation](https://miro.medium.com/v2/resize:fit:786/format:webp/1*r24_G4jmjpffc8Xqhq2R1g.png)>

|  | Bagging | Boosting |
| ------------- | ------------- | ------------- |
| Weak estimator  | Mutually dependently, built parallelly | mutually interrelate, built in order |
| Sampling method  | training data sampling with replacement, features sampling without replacement | training data sampling with replacement, features sampling without replacement |
| Ensemble method  | average for regression, vote for classification | each algorithm has its own ensemble algorithm |
| Target  | Reduce variance | Reduce bias |
| Representatives  | Random Forest | Adaboost, GBDT |

Bagging is used to reduce the variance of weak learners. Boosting is used to reduce the bias of weak learners.

We use bagging for combining weak learners of high variance. Bagging aims to produce a model with lower variance than the individual weak models. These weak learners are homogenous, meaning they are of the same type.

Bagging is also known as Bootstrap aggregating. It consists of two steps: bootstrapping and aggregation.

We use boosting for combining weak learners with high bias. Boosting aims to produce a model with a lower bias than that of the individual models. Like in bagging, the weak learners are homogeneous.

Boosting involves sequentially training weak learners. Here, each subsequent learner improves the errors of previous learners in the sequence. A sample of data is first taken from the initial dataset. This sample is used to train the first model, and the model makes its prediction. The samples can either be correctly or incorrectly predicted. The samples that are wrongly predicted are reused for training the next model. In this way, subsequent models can improve on the errors of previous models.

Unlike bagging, which aggregates prediction results at the end, boosting aggregates the results at each step. They are aggregated using weighted averaging.

## 1.3 Boosting algorithm procedures

Three key elements of boosting algorithm are:
- **Loss function L(x,y)**: evaluate the difference between model prediction results and actual results
- **weak estimator f(x)**: different boosthing algorithms may follow different procedure to build a tree (C4.5 or CART). Boosting can use any algorithm as weak estimator, but most commonly use decision tree.
- **Ensemble results H(x)**: use different methods

> [!IMPORTANT]
> Based on the result of the previous if evaluator f(x)t-1, calculate the loss function L(x,y), and use L(x,y) to adaptively affect the construction of the next if evaluator f(x)t , the result output by the integrated
> model is affected by all evaluators f(x)0~f(x)T as a whole

## 3. Implement boosting in sklearn

| Boosting algorithm | Library | Ensemble class |
| ------------- | ------------- | ------------- |
| ADB classifier | sklearn | AdaBoostClassifier |
| ADB regressor | sklearn | AdaBoostRegressor |
| GBDT classifier | sklearn | GradientBoostingClassifier |
| GBDT regressor  | sklearn | GradientBoostingRegressor |
| Histogram GBDT classifier | sklearn | HistGradientBoostingClassifier |
| Histogram GBDT regressor  | sklearn | HistGradientBoostingRegressor |
| eXtreme Gradient Boosting | xgboost | xgboost.train() |
| Light Gradient Boosting Machine  | xgboost | lightgbm.train() |
| CatBoost  | catboost | catboost.train() |

</details>


<details>
<summary><h1>2. AdaBoost</h1></summary>
The principle behind boosting algorithms is that we first build a model on the training dataset and then build a second model to rectify the errors present in the first model. This procedure is continued until and unless the errors are minimized and the dataset is predicted correctly. Boosting algorithms work in a similar way, it combines multiple models (weak learners) to reach the final output (strong learners).

Talking about the main idea behind the AdaBoost algorithm, we find that it iteratively trains a sequence of weak classifiers on different subsets of the training data. During each iteration, the algorithm assigns higher weights to the misclassified samples from the previous iteration, thereby focusing on the more challenging examples. This process allows the subsequent weak classifiers to pay more attention to the previously misclassified samples and improve their performance.
  
## 2.1 AdaBoost parameters and loss functions  
```python
class sklearn.ensemble.AdaBoostClassifier(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm="SAMME.R", random_state=None)

class sklearn.ensemble.AdaBoostRegressor(base_estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)
```
- base_estimator, and its attribute base_estimator_ and estimators_
```python
from sklearn.ensemble import AdaBoostClassifier as ABC
from sklearn.ensemble import AdaBoostRegressor as ABR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.tree import DecisionTreeRegressor as DTR
from sklearn.datasets import load_digits

base_estimator = DTC(max_depth=10, max_features=30)

clf=ABC(base_estimator=base_estimator, n_estimators=3).fit(X_c, y_c)
clf.estimators_   # output 3 base estimators which will be created randomly
```

> [!IMPORTANT]
> Adaboost use classification tree as base estimator for calssification tasks. (same as random forest)
> Adaboost use regression tree as base estimator for regression tasks. (same as random forest)
> GBDT, CatBoost, XGBoost's base estimators are all regression tree.

- learning rate
AdaBoost also supports a learning rate that controls the contribution of each model to the ensemble prediction.

This is controlled by the “learning_rate” argument and by default is set to 1.0 or full contribution. Smaller or larger values might be appropriate depending on the number of models used in the ensemble. There is a balance between the contribution of the models and the number of trees in the ensemble.

More trees may require a smaller learning rate; fewer trees may require a larger learning rate. It is common to use values between 0 and 1 and sometimes very small values to avoid overfitting such as 0.1, 0.01 or 0.001.

- algorithm and loss
algorithm appears in sklearn.ensemble.AdaBoostClassifier;
loss appears in sklearn.ensemble.AdaBoostRegressor;

- n_estimators

## 2.2 AdaBoost regression procedure

We need to understand following key questions about Boosting algorithms:
- What is loss function L(x,y) expression? How does loss function affect building model
- What is base estimator f(x)? boosting algorithm follow which procedure to build a tree ?(C4.5 or IE3 or CART)
- What is ensemble result function H(x)? How does ensemble aogorithm afftect ensembled results? 


</details>





































