<details>
<summary><h1>1. Boosting method</h1></summary>

## 1.1 Boosting principals and procedure

Boosting is a well-known ensemble learning strategy that combines the predictions of numerous base models to produce a more robust overall model.

>Unlike many ML models which focus on high quality prediction done by a single model, boosting algorithms seek to improve the prediction power by training a sequence of weak models, each compensating the weaknesses of its predecessors.

>![Python_File_Operation](https://miro.medium.com/v2/resize:fit:786/format:webp/1*4XuD6oRrgVqtaSwH-cu6SA.png)
>Boosting is a machine learning strategy that combines numerous weak learners into strong learners to increase model accuracy. The following are the steps in the boosting algorithm:
>
>1. Initialise weights: At the start of the process, each training example is given equal weight.
>  
>2. Train a weak learner: The weighted training data is used to train a weak learner. A weak learner is a simple model that outperforms random guessing only marginally. A decision tree with a few levels, for example, can be employed as a weak learner.
>
>3. Error calculation: The error of the weak learner on the training data is computed. The weighted sum of misclassified cases constitutes the error.
>  
>4. Update weights: Weights are updated according to the mistake rate of the training examples. Misclassified examples are given higher weights, whereas correctly classified examples are given lower weights.
>
>5. Repeat: Steps 2–4 are repeated several times. A new weak learner is trained on the updated weights of the training examples in each cycle.
>  
>6. Combine weak learners: The final model is made up of all of the weak learners that were trained in the preceding steps. The accuracy of each weak learner is weighted, and the final prediction is based on the weighted total of the weak learners.
>
>7. Forecast: The finished model is used to forecast fresh instances’ class labels.
>
>The boosting approach is designed to produce a strong learner that is accurate on the training data and can generalize effectively to new data. The algorithm can produce a model that is more accurate than any of the >individual weak learners by merging many weak learners.

>To understand Boosting, it is crucial to recognize that boosting is a generic algorithm rather than a specific model. Boosting needs you to specify a weak model (e.g. regression, shallow decision trees, etc) and then improves it.
>
>![Python_File_Operation](https://miro.medium.com/v2/resize:fit:720/format:webp/1*_DlfhMI2OFewBB4R-izszg.png)
>
>'More details can be found [here](https://medium.com/@brijesh_soni/understanding-boosting-in-machine-learning-a-comprehensive-guide-bdeaa1167a6).
>

## 1.2 Boosting vs Bagging

>![Python_File_Operation](https://miro.medium.com/v2/resize:fit:1400/0*o4zHKed7WKVdifju)

>![Python_File_Operation](https://miro.medium.com/v2/resize:fit:786/format:webp/1*r24_G4jmjpffc8Xqhq2R1g.png)>

|  | Bagging | Boosting |
| ------------- | ------------- | ------------- |
| Weak estimator  | Mutually dependently, built parallelly | mutually interrelate, built in order |
| Sampling method  | training data sampling with replacement, features sampling without replacement | training data sampling with replacement, features sampling without replacement |
| Ensemble method  | average for regression, vote for classification | each algorithm has its own ensemble algorithm |
| Target  | Reduce variance | Reduce bias |
| Representatives  | Random Forest | Adaboost, GBDT |

Bagging is used to reduce the variance of weak learners. Boosting is used to reduce the bias of weak learners.

We use bagging for combining weak learners of high variance. Bagging aims to produce a model with lower variance than the individual weak models. These weak learners are homogenous, meaning they are of the same type.

Bagging is also known as Bootstrap aggregating. It consists of two steps: bootstrapping and aggregation.

We use boosting for combining weak learners with high bias. Boosting aims to produce a model with a lower bias than that of the individual models. Like in bagging, the weak learners are homogeneous.

Boosting involves sequentially training weak learners. Here, each subsequent learner improves the errors of previous learners in the sequence. A sample of data is first taken from the initial dataset. This sample is used to train the first model, and the model makes its prediction. The samples can either be correctly or incorrectly predicted. The samples that are wrongly predicted are reused for training the next model. In this way, subsequent models can improve on the errors of previous models.

Unlike bagging, which aggregates prediction results at the end, boosting aggregates the results at each step. They are aggregated using weighted averaging.

## 1.3 Boosting algorithm procedures

Three key elements of boosting algorithm are:
- **Loss function L(x,y)**: evaluate the difference between model prediction results and actual results
- **weak estimator f(x)**: different boosthing algorithms may follow different procedure to build a tree (C4.5 or CART). Boosting can use any algorithm as weak estimator, but most commonly use decision tree.
- **Ensemble results H(x)**: use different methods

> [!IMPORTANT]
> Based on the result of the previous if evaluator f(x)t-1, calculate the loss function L(x,y), and use L(x,y) to adaptively affect the construction of the next if evaluator f(x)t , the result output by the integrated
> model is affected by all evaluators f(x)0~f(x)T as a whole

| Boosting algorithm | Library | Ensemble class |
| ------------- | ------------- | ------------- |
| ADB classifier | sklearn | AdaBoostClassifier |
| ADB regressor | sklearn | AdaBoostRegressor |
| GBDT classifier | sklearn | GradientBoostingClassifier |
| GBDT regressor  | sklearn | GradientBoostingRegressor |
| Histogram GBDT classifier | sklearn | HistGradientBoostingClassifier |
| Histogram GBDT regressor  | sklearn | HistGradientBoostingRegressor |
| eXtreme Gradient Boosting | xgboost | xgboost.train() |
| Light Gradient Boosting Machine  | xgboost | lightgbm.train() |
| CatBoost  | catboost | catboost.train() |

## 3. Implement boosting in sklearn





</details>


<details>
<summary><h1>2. AdaBoost</h1></summary>
## 2.1 AdaBoost parameters and loss functions  

## 2.2 AdaBoost regression procedure

</details>





































