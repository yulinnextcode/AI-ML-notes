# 1. Logistic regression vs linear regression
Linear regression is used to solve regression problems, logistic regression is used to solve classification problems based on linear regression. We can assue logistic regression puts a sigmoid function outside linear regression's results, then transfer linear regression into 0/1 output classification problem.
**Linear regression is used to fit function, logistic regression is used to predict function.**
**Linear regression uses least square method, logistic regression uses maximum likelyhood method**
> [!IMPORTANT]
> Logistic regression and linear regression both belongs to generalized linear model.
Difference between logistic regression vs linear regression
- Linear regression requires variables accord with normal distribution, logistic regression does not have this requirement
- Linear regression requires predict values are continuous, logistic regression requires predict values are classification types
- Linear regression requires variables and predict values accord with linear relationship, logistic regression does not require this
- Logistic regression analyze the probability of a certain predicted value given a variable, linear regression analyze the relationship between variable and predict values.
The basic principal of logistic regression includes two parts:
- Construct model: We can understand this from the aspect of generalized linear model + logit function, or we can understand this from the aspect of random variables' logistic distribution.
- Solve model's parameters: We can understand this with the help of maximum likelihood estimate, or we can use KL dispersion theory to construct binary cross entropy loss function to solve.

> [!IMPORTANT]
> Generalized linear model. We know there are some limitations due to that linear regression models are too simple. Therefore we introduce some more modeling procedure methods such as Lasso regression, ridge regression, elastic-net regression. Other than that, we can add a function to original linear regression's equation (either left or right hand side). At this time, linear regression models are extended to generalized linear models. The added functions are called link functions.
> By adding functions to equations' either left or right hand side, linear regression models can handle non-linear capabilities. The added function can be called link function. After adding a link function, the model can be called generalized linear model.

# 2 Logit model and logistic regression
## 2.1 logit model
odd(p)=p/(1-p)
logit(p)=ln(p/(1-p))
We assume the above function as a link function, and add this to the left side. 
## 2.2 logistic regression and Sigmoid function
Any function with a shape similar to S can called Sigmoid function.
Is the logistic regression's output represent probability? That depends on the modeling procedure. If you follow statistical analysis procedure to build a model, then the result can represent a probability. If you follow machine learning procedure, the it's very likely that the result is not a probability. 
# 3 Mutiple classification logistic regression
OvO, OvR, MvM
# 4 Logistic regression parameters estimation: Maximum likelyhood estimation, relative entropy and cross entropy loss function
Before we solve parameters, we need to **construct loss function**. Then, for logistic regression, we use **two methods to solve parameter functions**.
- Maximum Likelihood Estimate
- Using Relative Entropy to construct Cross-Entropy loss functions (relative entropy = cross entropy - information entropy)
With either one of the above methods, we can **construct logistic regression's loss function**. This loss function is concave, but we can not solve it with least square method. We should use a more generalized method-Gradient Descent, to solve its parameter functions.
## 4.1 Gradient Descent basic principles and implementation
If we follow SSE method to construct logistic regression's loss function, it will not be concave. Therefore we can not follow SSE, which means we cannot use least squared method to solve its parameter functions either.
At this time we need to know another more generalized method - Gradient Descent
```python
import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt

from ML_basic_function import *
```
The goal of gradient decent is to calculate the minimum result. LSM adopts this goal by solving a equation at one time. Gradient Decent achieve this goal by a iterative method. Approach this goal step by step. It first adopts a initial value randomly, then move this value to the minimum potion to a certain direction step by step.
- Step 1, randomly adopts initial value
- Step 2, determine move direction
- Step 3, determine move step and start move. （Set a parameter called learning rate or step)
- Step 4，do next iteration
```python
def gd(lr=0.02, itera_times=20, w=10):
  """
  lr is learning rates
  """
  results=[w]
  for i in range(itera_times):
    w-=lr*28*(w-2)   #28*(w-2) is for certain loss function's calculation equation
    results.append(w)
  return results
```
```python
def show_trace(res):
  f_line=np.arange(-6,10,0.1)
  plt.plot(f_line, [14*np.power(x-2,2) for x in f_line])
  plt.plot(res, [14*np.power(x-2,2) for x in res], '-o')
  plt.xlabel('x')
  plt.ylabel('Loss(x)')
```
## 4.2 Gradient Decent general modeling procedure and multi gradient decent
- Step 1, determine data and model
- Step 2, Set initial parameter values
- Step 3, get loss function and gradient expression based on loss function (Take linear regression as an example, we can calculate dirvative for the SSE loss function. After this, we can either solve the equations, or we can run gradient decent with the loss function expression)
- Step 4, start gradient decent iteration

gradient decent can be an algorithm cluster, not a sigle algorithm

## 4.3 Two dimensional gradient decent's visulization display
### 4.3.1 Contour map
### 4.3.2 Loss function value change
##4.4 Solve Linear regression problems with gradient decent
- Step 1, determine data and model
- Step 2, Set up initial parameters
- Step 3, construct loss function and gradient decent expression
- Step 4, start gradient loss iteration

# 5 Gradient decent estimation
- Advantages: some cases LSM canot solve (no analytical expression), GD can still works (numerical expression still works).
- Disadvantage: when loss function is not concave, then GD can not find minimum value in the whole domain. Considering that **iteration step=learning rate*gradient**, therefore it may fall in **local minimum trap** (local minimum point's gradient is 0) or **saddle point trap**
Considering the above disadvantage (local minimum trap and saddle point trap), we introduce **random gradient decent and small batch gradient decent**.

# 6 Stochastic Gradient Decent
In order to avoid local minimum trap and saddle point trap, we adjust each training's batch data, try to avoid the trap by different batch data's different features. **When we use different batch data to train, will get different loss functions, therefore the parameters of the model will also be different**
When we use all training data, the features of all the training data are called global feature. When we use a small batch of data, then the feature can be called local feature. The global feature and local feature is different. **Initially, GD's iteration based on global features (use all data), we changed this to iteration based on local features (use batch data)**, which ae stochastic gradient decent (SGD) or mini-batch gradient descent (BGD).
**Stochastic gradient descent** picks one data to construct loss function, the solve procedure of parameters may not be very smooth, but it can avoid local minimum trap. **mini-batch gradient descent** picks a small batch of data to iterate and construct loss function.
> [!NOTE]
> Batch gradient descent computes the gradient of the cost function with respect to the model parameters using the entire training dataset in each iteration. Stochastic gradient descent, on the other hand, computes the gradient using only a single training example or a small subset of examples in each iteration.
> Links
'This site was built using [Difference between Batch Gradient Descent and Stochastic Gradient Descent](https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/#:~:text=Batch%20gradient%20descent%20computes%20the,of%20examples%20in%20each%20iteration.).

when picked batched data=1, then the mini-batch gradient descent = stochastic gradient descent
when picked batched data=whole training data, then the mini-batch gradient descent = batch gradient descent

# 7 Mini-batch Gradient Descent
Not only pick 1 data, use a mini-batch of data to train.
This is a gradually optimization procedure from gradient descent -> stochastic gradient descent -> mini-batch gradient descent

# 8 Gradient descent optimization: data normalization and learning-rate adjust
Other than we select different train batch data, we can also select the above two methods to optimize iteration procedure.
## 8.1 Data normalization
This can be seen as a **optimization method for GD**, and it can also be seen a solution to **avoid problems due to different quantities**.
- 0-1 normalization
- Z-Score normalization
- nonlinear normalization
## 8.2 Learning rate

We use mini-batch GD > schoastic GD > BGD. By default, we use normalization + learning rate decrease + mini-batch GD
# 9 Implement logistic regression using GD
- step 1: construct loss function
- step 2: construct gradient expression based on loss function
- step 3: update parameters using gradient expression
```python
def logit_gd(X,w,y):
  m=X.shape[0]
  grad=X.T.dot(sigmoid(X.dot(w))-y)/m
  return grad
```
Create logistic regression dataset with following codes:
```python
def arrayGenCla(num_examples = 500, num_input = 2, num_class = 3, deg_dispersion = [4,2], bias = False):
  cluster_l = np.empty([num_examples,1])
  mean_ = deg_dispersion[0]
  std_ = deg_dispersion[1]
  lf = []
  ll = []
  k = mean_ * (num_class-1)/2

  for i in range(num_class):
    data_temp=np.random.normal(i*mean_+k, std_, size=(num_examples, num_inputs))
    lf.append(data_temp)
    labels_temp = np.full_like(cluster_l,i)
    l1.append(labels_temp)

  features = np.concatenate(lf)
  labels = np.concatenate(l1)

  if bias == True:
    features = np.concatenate((features, np.ones(l.shape)), 1)
  return features, labels

np.random.seed(24)
Xtrain, Xtest, ytrain, ytest = array_split(f,l)

mean_ = Xtrain[:,:-1].mean(axis=0)
std_ = Xtrain[:,:-1].std(axis=0)

Xtrain[:,:-1] = (Xtrain[:,:-1] - mean_) / std_
Xtest[:,:-1] = (Xtest[:,:-1] - mean_) / std_

np.random.seed(24)
n=f.shape[1]
w=np.random.randn(n,1)

batch_size=50
num_spoch=200
lr_init=0.2

lr_lambad = lambda epoch: 0.95*=eopch

for i in range(num_epoch):
  w=sgd_cal(Xtrain, w, ytrain, logit_gd, batch_size=batch_size, epoch=1, lr=lr_init*lr_lambad(i))

yhat=sigmoid(Xtrain.dot(w))
logit_cla(yhat, thr=0.5)[:10]
(logit_cla(yhat, thr=0.5)==y_train).mean()  #accuracy
```







































