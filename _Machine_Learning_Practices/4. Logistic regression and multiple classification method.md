# 1. Logistic regression vs linear regression
Linear regression is used to solve regression problems, logistic regression is used to solve classification problems based on linear regression. We can assue logistic regression puts a sigmoid function outside linear regression's results, then transfer linear regression into 0/1 output classification problem.
**Linear regression is used to fit function, logistic regression is used to predict function.**
**Linear regression uses least square method, logistic regression uses maximum likelyhood method**
> [!IMPORTANT]
> Logistic regression and linear regression both belongs to generalized linear model.
Difference between logistic regression vs linear regression
- Linear regression requires variables accord with normal distribution, logistic regression does not have this requirement
- Linear regression requires predict values are continuous, logistic regression requires predict values are classification types
- Linear regression requires variables and predict values accord with linear relationship, logistic regression does not require this
- Logistic regression analyze the probability of a certain predicted value given a variable, linear regression analyze the relationship between variable and predict values.
The basic principal of logistic regression includes two parts:
- Construct model: We can understand this from the aspect of generalized linear model + logit function, or we can understand this from the aspect of random variables' logistic distribution.
- Solve model's parameters: We can understand this with the help of maximum likelihood estimate, or we can use KL dispersion theory to construct binary cross entropy loss function to solve.

> [!IMPORTANT]
> Generalized linear model. We know there are some limitations due to that linear regression models are too simple. Therefore we introduce some more modeling procedure methods such as Lasso regression, ridge regression, elastic-net regression. Other than that, we can add a function to original linear regression's equation (either left or right hand side). At this time, linear regression models are extended to generalized linear models. The added functions are called link functions.
> By adding functions to equations' either left or right hand side, linear regression models can handle non-linear capabilities. The added function can be called link function. After adding a link function, the model can be called generalized linear model.

# 2 Logit model and logistic regression
## 2.1 logit model
odd(p)=p/(1-p)
logit(p)=ln(p/(1-p))
We assume the above function as a link function, and add this to the left side. 
## 2.2 logistic regression and Sigmoid function
Any function with a shape similar to S can called Sigmoid function.
Is the logistic regression's output represent probability? That depends on the modeling procedure. If you follow statistical analysis procedure to build a model, then the result can represent a probability. If you follow machine learning procedure, the it's very likely that the result is not a probability. 
# 3 Mutiple classification logistic regression
OvO, OvR, MvM
# 4 Logistic regression parameters estimation: Maximum likelyhood estimation, relative entropy and cross entropy loss function
Before we solve parameters, we need to **construct loss function**. Then, for logistic regression, we use **two methods to solve parameter functions**.
- Maximum Likelihood Estimate
- Using Relative Entropy to construct Cross-Entropy loss functions (relative entropy = cross entropy - information entropy)
With either one of the above methods, we can **construct logistic regression's loss function**. This loss function is concave, but we can not solve it with least square method. We should use a more generalized method-Gradient Descent, to solve its parameter functions.
## 4.1 Gradient Descent basic principles and implementation
If we follow SSE method to construct logistic regression's loss function, it will not be concave. Therefore we can not follow SSE, which means we cannot use least squared method to solve its parameter functions either.
At this time we need to know another more generalized method - Gradient Descent
```python
import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt

from ML_basic_function import *
```
The goal of gradient decent is to calculate the minimum result. LSM adopts this goal by solving a equation at one time. Gradient Decent achieve this goal by a iterative method. Approach this goal step by step. It first adopts a initial value randomly, then move this value to the minimum potion to a certain direction step by step.
- Step 1, randomly adopts initial value
- Step 2, determine move direction
- Step 3, determine move step and start move. （Set a parameter called learning rate or step)
- Step 4，do next iteration
```python
def gd(lr=0.02, itera_times=20, w=10):
  """
  lr is learning rates
  """
  results=[w]
  for i in range(itera_times):
    w-=lr*28*(w-2)   #28*(w-2) is for certain loss function's calculation equation
    results.append(w)
  return results
```
```python
def show_trace(res):
  f_line=np.arange(-6,10,0.1)
  plt.plot(f_line, [14*np.power(x-2,2) for x in f_line])
  plt.plot(res, [14*np.power(x-2,2) for x in res], '-o')
  plt.xlabel('x')
  plt.ylabel('Loss(x)')
```
## 4.2 Gradient Decent general modeling procedure and multi gradient decent
- Step 1, determine data and model
- Step 2, Set initial parameter values
- Step 3, get loss function and gradient expression based on loss function (Take linear regression as an example, we can calculate dirvative for the SSE loss function. After this, we can either solve the equations, or we can run gradient decent with the loss function expression)
- Step 4, start gradient decent iteration

gradient decent can be an algorithm cluster, not a sigle algorithm

## 4.3 Two dimensional gradient decent's visulization display
### 4.3.1 Contour map
### 4.3.2 Loss function value change
### 4.3.3 Solve Linear regression problems with gradient decent
- Step 1, determine data and model
- Step 2, Set up initial parameters
- Step 3, construct loss function and gradient decent expression
- Step 4, start gradient loss iteration

# 5 Gradient decent estimation
- Advantages: some cases LSM canot solve (no analytical expression), GD can still works (numerical expression still works).
- Disadvantage: when loss function is not concave, then GD can not find minimum value in the whole domain. Considering that step=learning rate*gradient, therefore it may fall in **local minimum trap** (local minimum point's gradient is 0) or **saddle point trap**
Considering the above disadvantage (local minimum trap and saddle point trap), we introduce **random gradient decent and small batch gradient decent**.





















