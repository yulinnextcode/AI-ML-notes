# 1. Unsuperivised learning and KNN
## 1.1 Clustering and unsupervised learning
Regression and classification algorithms are both supervised learning algorithms. For unsupervised learning, there are not any labels for sample data. Therefore we can only find any features for the feature matrix.

Two most popular algorithms among unsupervised learning are clustering algorithms and association rule algorithm. Sklearn only implements clustering algorithms.

The difficulity level of clustering algorithms is less than supervised learning algorithms, becaue it does not involves feature selection, and there is no generalization assessment nor parameters fine tuning. Clustering algorithms are also less commonly used than supervised learning. Therefore, we only picked KNN, Mini Batch K-Means and BDSCAN. 

## 1.2 K-Means clustering algorithms
```python
np.random.seed(23)
X,y=arrayGenCla(num_examples=20, num_inputs=2, num_class=2, deg_dispersion=[2,0.5])
plt.scatter(X[:,0],X[:,1],c=y)

np.random.seed(23)
center=np.random.randn(2,2)

res_bool=np.power((X-center[0]),2).sum(1) < np.power((X-center[1]),2).sum(1)
X_1=X[(res_bool)]
X_2=X[(~res_bool)]

X_1.mean(0)
X_2.mean(0)
```
- Step 1. Determin how many categories
- Step 2. Create initial point randomly
- Step 3. Categorize data points based on center point
- Step 4. Re-calculate center point
- Step 5. Re-do Step 3
- Step 6. Re-do Step 4
- If center points are not changed in the last two iterations, then the iteration stops

## 1.3 Implement K-Means clustering in sklearn
```python
from sklearn.cluster import KMeans

km=KMeans(n_clusters=2)
km.fit(X)

km.cluster_centers_
km.inertia_
km.n_iter_
```
# 2. Mini Batch K-Means and DBSCAN density clustering
Mini Batch K-Means algorithm is valid because mini-batch data's feature is believed to be consistent with the total data's features. However, mini-batch data is not total data after all. Therefore Mini Batch algorithm sacrifise accuracy compared to original K-Means.
```python
from sklearn.cluster import MiniBatchKMeans
```
With the increasing size of the datasets being analyzed, the computation time of K-means increases because of its constraint of needing the whole dataset in main memory. For this reason, several methods have been proposed to reduce the temporal and spatial cost of the algorithm. A different approach is the Mini batch K-means algorithm. Mini Batch K-means algorithmâ€˜s main idea is to use small random batches of data of a fixed size, so they can be stored in memory. Each iteration a new random sample from the dataset is obtained and used to update the clusters and this is repeated until convergence. Each mini batch updates the clusters using a convex combination of the values of the prototypes and the data, applying a learning rate that decreases with the number of iterations. This learning rate is the inverse of the number of data assigned to a cluster during the process. As the number of iterations increases, the effect of new data is reduced, so convergence can be detected when no changes in the clusters occur in several consecutive iterations. The empirical results suggest that it can obtain a substantial saving of computational time at the expense of some loss of cluster quality, but not extensive study of the algorithm has been done to measure how the characteristics of the datasets, such as the number of clusters or its size, affect the partition quality.
> [!TIP]
> MiNi Batch K-Means Trade accuracy for efficiency

![Python_File_Operation](https://media.geeksforgeeks.org/wp-content/uploads/PicsArt_11-17-08.07.10-300x300.jpg)





















