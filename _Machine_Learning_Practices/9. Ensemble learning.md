# 1. Ensembleing learning 3 key area
Ensembleing learning will train many base estimators, and output the prediction results with a certain combination of all the base estimators.
Ensemble learning is a machine learning technique that combines the predictions of multiple models to improve accuracy and resilience in forecasting. The underlying concept is to combine the outputs of diverse models to create a more precise prediction.
There are 3 ways to ensenble models:
- Merging models, also knows as model blending or model stacking. It focus on strong estimator, combines multiple strong models to create a single model that can use the strengths of each model to make up for its weaknesses.
- Weak estimator ensemble. Focus on weak estimator. Three main ensemble methods are bagging, boosting and stacking. It includes Randome Forest, XGBoost, LightGBM, etc. Use ensemble algorithms to improve weak estimators to strong estimators.
- Mixture of experts, is a machine learning technique that combines multiple neural networks models into one larger model. MoE involves training experts on specific subtasks of a complex predictive modeling problem. In a typical ensemble scenario, all models are trained on the same dataset and their outputs are combined. In MoE, each expert model is only trained on a subset of the dataset.
# 2. Bagging algorithm
Bagging, also known as bootstrap aggregation, is an ensemble learning method that reduces variance in a noisy dataset. It's one of the three most popular ensemble learning techniques, along with boosting and stacking. Here is how bagging works:
- select a random sample of data from a training set with replacement
- train and create multiple weak estimator independently (+90% cases use decision tree) on random subsets of the data
- Aggregate the models' predictions through voting and averaging. (averaging for regression tasks, and boting for classification tasks)
  
# 3. RandomForest
A random forest classifier and regressor are both machine learning algorithms that can be used for classification and regression problems.
# 4. Incremental learning

# 5. Interview questions

# 6. Randome Forest parameter space and automatic optimization






























