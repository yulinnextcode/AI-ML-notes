# 1. Ensembleing learning 3 key area
Ensembleing learning will train many base estimators, and output the prediction results with a certain combination of all the base estimators.
Ensemble learning is a machine learning technique that combines the predictions of multiple models to improve accuracy and resilience in forecasting. The underlying concept is to combine the outputs of diverse models to create a more precise prediction.
There are 3 ways to ensenble models:
- Merging models, also knows as model blending or model stacking. It focus on strong estimator, combines multiple strong models to create a single model that can use the strengths of each model to make up for its weaknesses.
- Weak estimator ensemble. Focus on weak estimator. Three main ensemble methods are bagging, boosting and stacking. It includes Randome Forest, XGBoost, LightGBM, etc. Use ensemble algorithms to improve weak estimators to strong estimators.
- Mixture of experts, is a machine learning technique that combines multiple neural networks models into one larger model. MoE involves training experts on specific subtasks of a complex predictive modeling problem. In a typical ensemble scenario, all models are trained on the same dataset and their outputs are combined. In MoE, each expert model is only trained on a subset of the dataset.
# 2. Bagging algorithm
Bagging, also known as bootstrap aggregation, is an ensemble learning method that reduces variance in a noisy dataset. It's one of the three most popular ensemble learning techniques, along with boosting and stacking. Here is how bagging works:
- select a random sample of data from a training set with replacement
- train and create multiple weak estimator independently (+90% cases use decision tree) on random subsets of the data
- Aggregate the models' predictions through voting and averaging. (averaging for regression tasks, and boting for classification tasks)
```python
np.argmax(np.bincount(r_clf))
```
Bagging algorithm includes: RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, BaggingClassifier, BaggingRegressor

# 3. RandomForest
A random forest classifier and regressor are both machine learning algorithms that can be used for classification and regression problems.
Random Forest is a widely-used machine learning algorithm developed by Leo Breiman and Adele Cutler, which combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems.
> [!TIP]
> The only difference between Random Forest and Decision Tree is that Rndom Forest train and build weak estimator independently by bootstrapping (i.e.: randomly samples, with replacement). Others are all the same as Decision Trees. Both RF and DT can randomly select only a subset of the total available features to be evaluated at each node as potential splitters.
```python
sklearn.ensemble.RandomForestRegressor
sklearn.ensemble.RandomForestClassifier
```
The above two class parameters are almost the same
## 3.1 Implement RandomForestRegressor
```python
import matplotlib.pyplot as plt
from sklearn.ensemble improt RandomForestRegressor as RFR
from sklearn.tree import DecisionTreeRegressor as DTR
from sklearn.model_selection import cross_validation, KFold

# cross_validate can output training and validation sets' results, cross_val_score can only ouput validation sets' results

X=data.iloc[:,:-1]
y=data.iloc[:,-1]
y.mean()
X.shape

reg_f=RFR()
reg_t=DTR()

cv=KFold(n_split=5, shuffle=True, random_state=1412)

result_t=cross_validate(reg_t
                        ,X,y
                        ,cv=cv
                        ,scoring="neg_mean_squared_error"
                        ,return_train_score=True
                        ,verbose=True
                        ,n_jobs=-1)

result_f=cross_validate(reg_f
                        ,X,y
                        ,cv=cv
                        ,scoring="neg_mean_squared_error"
                        ,return_train_score=True
                        ,verbose=True
                        ,n_jobs=-1)

#result_t shows that reg_t is very overfitted, but result_f results are much better. RF has more generalization capability
```
## 3.2 Random Forest Regressor parameters
| Name  | Description |
| ------------- | ------------- |
| weak estimator number  | **n_estimators** |
| weak estimator training dataset  | **bootstrap, oob_score, max_samples**, max_features, random_state  |
| weak estimator structure  | criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes, min_impurity_decrease  |
| others  | n_jobs, verbose, ccp_alpha  |

Random Forest Regressor weak estimator structure: criterion can select squared_error, absolute_error, poisson.
Weak estimator number: default is 100.
weak estimator training dataset: bootstrap, oob_score, max_samples. **random select sample data to build weak estimarots independently (bootstrap=True), then aggregate the results wity Baggings' method. (averaging for regression tasks, and boting for classification tasks)**

> [!IMPORTANT]
> max_features: random select features. Max_feature is the number of features to consider each time to make the split decision. Let us say the dimension of your data is 50 and the max_feature is 10, each time you need to find the split, you randomly select 10 features and use them to decide which one of the 10 is the best feature to use. When you go to the next node you will select randomly another 10 and so on.
> This mechanism is used to control overfitting. In fact, it is similar to the technique used in random forest, except in random forest we start with sampling also from the data and we generate multiple trees.
> So even if you set the number to 10, if you go deep you will end up using all the features, but each time you limit the set to 10.

> [!TIP]
> **numpy, pandas, random all can set random seeds, different librarie' random seeds follow different rules.**

## 3.3 Ensemble learning algorithm parameter space and grid search
```python
param_grid_simple={"criterion": ["squared_error", "poisson"]
                  ,"n_estimators": [*range(20,100,5)]
                  ,"max_depth": [*range(10,25,2)]
                  ,"max_features": ["log2", "sqrt", 16,32,64,"auto"]
                  ,"min_impurity_decrease": [*np.arange(0,5,10)]
                  }

reg=RFR(random_state=1412, verbose=True, n_jobs=-1)

cv=KFold(n_splits=5, shuffle=True,random_state=1412)
search=GridSearchCV(estimator=reg
                   ,param_grid=param_grid_simple
                   ,scoring="neg_mean_squared_error"
                   ,verbose=True
                   ,cv=cv
                   ,n_jobs=-1)

start=time.time()
search.fit(X,y)
print(time.time()-start)

search.best_estimator_

abs(search.best_score_)**0.5

```

# 4. Incremental learning
Incremental learning is a machine learning technique that uses input data to improve the knowledge of an existing model. It's a dynamic approach that allows AI models to learn new knowledge gradually, while also helping them retain and strengthen their existing understanding.

Incremental learning is similar to human learning patterns. It differs from batch learning, which trains a model on the entire data set at once, while incremental learning learns from new data points as they become available, updating its model parameters incrementally.

Benefits of incremental learning include:
Enhanced retention
By revisiting and building up previously learned material, incremental learning improves retention and helps solidify knowledge over the years.
Adaptability
This iterative approach enables AI models to adapt to changing environments.
Decision-making refinement
This approach enables AI models to refine their decision-making prowess without compromising their existing knowledge base.

Assume we have a very large file and we need to load it and train with the data. The solution is we load the large data by batch, then preprocess and train. Finally load next batch.
```python
trainpath=""
testpath=""

looprange = range(0,10**6,50000)

reg =RFR(n_estimators=10, random_state=1412, warm_start=True, verbose=True, n_jobs=-1)

test=pd.read_csv
Xtest=test.iloc[:,:-1]
Ytest=test.iloc[:,-1]

trainsubset=pd.read_csv(trainpath, header=None, index_col=0, skiprows=950000, nrows=50000)

for line in looprange:
  if line==0:
    header="infer"
    newtree=0
  else:
    header=None
    newtree=10

trainsubset=pd.read_csv(trainpath, header=header, index_col=0, skiprows=line, nrows=50000)
Xtrain=trainsubset.iloc[:,:-1]
Ytrain=trainsubset.iloc[:,-1]
reg.n_estimators += newtree
reg=reg.fit(Xtrain, Ytrain)

reg.score(Xtest,Ytest)
```
# 5. Bagging 6 Interview questions
- Q1. Why bagging algorithms perform better than a single estimator?
- Q2. Why bagging can reduce varianceï¼Ÿ
- Q3. What are the basic conditions for effective bagging? Is the effect of bagging always better than that of weak estimators?
- Q4. Can bagging method integrate algorithms other than decision trees?
- Q5. How to enhance the independence of weak estimators in bagging?
- Q6. In addition to random forests, do you know other bagging algorithms?


# 6. Randome Forest parameter space and automatic optimization






























