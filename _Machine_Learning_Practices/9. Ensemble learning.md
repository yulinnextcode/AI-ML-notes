# 1. Ensembleing learning 3 key area
Ensembleing learning will train many base estimators, and output the prediction results with a certain combination of all the base estimators.
Ensemble learning is a machine learning technique that combines the predictions of multiple models to improve accuracy and resilience in forecasting. The underlying concept is to combine the outputs of diverse models to create a more precise prediction.
There are 3 ways to ensenble models:
- Merging models, also knows as model blending or model stacking. It focus on strong estimator, combines multiple strong models to create a single model that can use the strengths of each model to make up for its weaknesses.
- Weak estimator ensemble. Focus on weak estimator. Three main ensemble methods are bagging, boosting and stacking. It includes Randome Forest, XGBoost, LightGBM, etc. Use ensemble algorithms to improve weak estimators to strong estimators.
- Mixture of experts, is a machine learning technique that combines multiple neural networks models into one larger model. MoE involves training experts on specific subtasks of a complex predictive modeling problem. In a typical ensemble scenario, all models are trained on the same dataset and their outputs are combined. In MoE, each expert model is only trained on a subset of the dataset.
# 2. Bagging algorithm
Bagging, also known as bootstrap aggregation, is an ensemble learning method that reduces variance in a noisy dataset. It's one of the three most popular ensemble learning techniques, along with boosting and stacking. Here is how bagging works:
- select a random sample of data from a training set with replacement
- train and create multiple weak estimator independently (+90% cases use decision tree) on random subsets of the data
- Aggregate the models' predictions through voting and averaging. (averaging for regression tasks, and boting for classification tasks)
```python
np.argmax(np.bincount(r_clf))
```
Bagging algorithm includes: RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, BaggingClassifier, BaggingRegressor

# 3. RandomForest
A random forest classifier and regressor are both machine learning algorithms that can be used for classification and regression problems.
Random Forest is a widely-used machine learning algorithm developed by Leo Breiman and Adele Cutler, which combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems.
> [!TIP]
> The only difference between Random Forest and Decision Tree is that Rndom Forest train and build weak estimator independently by bootstrapping (i.e.: randomly samples, with replacement). Others are all the same as Decision Trees. Both RF and DT can randomly select only a subset of the total available features to be evaluated at each node as potential splitters.
```python
sklearn.ensemble.RandomForestRegressor
sklearn.ensemble.RandomForestClassifier
```
The above two class parameters are almost the same
```python
import matplotlib.pyplot as plt
from sklearn.ensemble improt RandomForestRegressor as RFR
from sklearn.tree import DecisionTreeRegressor as DTR
from sklearn.model_selection import cross_validation, KFold

# cross_validate can output training and validation sets' results, cross_val_score can only ouput validation sets' results

X=data.iloc[:,:-1]
y=data.iloc[:,-1]
y.mean()
X.shape

reg_f=RFR()
reg_t=DTR()

cv=KFold(n_split=5, shuffle=True, random_state=1412)

result_t=cross_validate(reg_t
                        ,X,y
                        ,cv=cv
                        ,scoring="neg_mean_squared_error"
                        ,return_train_score=True
                        ,verbose=True
                        ,n_jobs=-1)

result_f=cross_validate(reg_f
                        ,X,y
                        ,cv=cv
                        ,scoring="neg_mean_squared_error"
                        ,return_train_score=True
                        ,verbose=True
                        ,n_jobs=-1)

#result_t shows that reg_t is very overfitted, but result_f results are much better. RF has more generalization capability
```

# 4. Incremental learning

# 5. Interview questions

# 6. Randome Forest parameter space and automatic optimization






























