# 1. Decision Boundary
We can use experience or theorical method to evaluate a model's capabilities. Additionaly, we can also use draw model's decition boundary to evaluate model capability.
Decision boundary essentially is a visiualization method. We can also use decision boundary to check if a model is overfit or underfit. Drawing model decision boundary as following steps:
- Step 1. Observate sample distribution
- Step 2. Train model with give sample data
- Step 3. Classify each point in the given space with the trained model
- Step 4. Add sample nodes back to the sample space
Figure

> [!NOTE]
> Most models are trying to find a effective decision boundary to classify data. In some cases, models will try to change sample space in Step 1 first (for example transfer 2-D sample space to 3-D with kernel). Then it will figure out a decision surface or hyperplane to classify the data.

Drawing logistic regression's decision bounary as follows
```python
def logit_DB(X, w, y):
  """
  Plot decision boundary for logit model
  """
  x1, x2=np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 1000).reshape(-1,1),
                     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 1000).reshape(-1,1))

  X_temp=np.concatenate([x1.reshape(-1,1), x2.reshape(-1,1) np.ones(shape=(1000000, 1))], )

  y_hat_temp=logit_cla(sigmoid(X_temp.dot(w)))
  yhat=y_hat_temp.reshape(x1.shape)

  from matplotlib.colors import ListedColormap
  custom_cmap=ListedColormap(['#EF9A9A','#90CAF9'])
  plt.contourf(x1, x2, yhat, cmap=custom_cmap)

np.random.seed(24)

f,l=arrayGenCla(num_class=2, deg_dispersion=[6,2], bias=True)

np.random.seed(24)

Xtrain, Xtest, ytrain, ytest = array_split(f,l)

mean_=Xtrain[:,:-1].mean(axis=0)
std_=Xtrain[:,:-1].std(axis=0)

Xtrain[:,:-1]=(Xtrain[:,:-1]-mean_)/std_
Xtest[:,:-1]=(Xtest[:,:-1]-mean_)/std_

plt.scatter(f[:,0], f[:,1], c=1)

np.random.seed(24)

batch_size=50
num_spoch=200
lr_init=0.2

n=f.shape[1]
w=np.rando.randn(n,1)

train_acc=[]
test_acc=[]

lr_lambda=lambda epoch: 0.95**epoch

for i in range(num_epoch):
  w = sgd_cal(Xtrain, w,ytrain, logit_gd, batch_size=batch_size, epoch=1, lr=lr_init*lr_lambda(i))
  train_acc.append(logit_acc(Xtrain, w, ytrain, thr=0.5))
  test_acc.append(logit_acc(Xtest, w, ytest, thr=0.5))

plt.plot(list(range(num_epoch)), np.array(train_acc).flatten(), label='train_acc')
plt.plot(list(range(num_epoch)), np.array(test_acc).flatten(), label='test_acc')
plt.xlabel('epochs')
plt.ylabel('Accuracy')
plt.legend(loc=4)

logit_DB(Xtrain, w, ytrain)

plt.scatter(Xtrain[(ytrain==0).flatten(), 0], Xtrain[(ytrain==0).flatten(),1],color='red')
plt.scatter(Xtrain[(ytrain==1).flatten(), 0], Xtrain[(ytrain==0).flatten(),1],color='blue')
```
# 2. Classification model estimators
Recall, F1-score, ROC-AUC, KS curve.
## 2.1 Limitation of accuracy
Accuracy votes equally. 
In linear regression modeling, we use SSE to both construct loss function and estimate the model. In classification modeling, we use cross-binary entropy to construct the loss function and use accuracy to estimate the model.
## 2.2 Confusion matrix
- Actual condition positive (P): total positive in sample data, total samples with real positive label
- Actual condition negative (N): total negative in sample data, total samples with real negative label
- Predicted condition positive (PP): total predicted positive in sample data, total samples with predicted positive label
- Predicted condition negative (PN): total predicted negative in sample data, total samples with predicted negative label

> [!NOTE]
> **In the following TP, TN, FP, FN, the second character is predicted value.**
- True positive (TP)
- True negative (TN)
- False positive (FP), Type I error, false alarm, underestimation
- False negative (FN), Type II error, miss, overestimation
confusion matrix figure----------------------------------------------------------------

## 2.3 Recall
Recall = TP/(TP+FN), focusing on whether sample 1 is recognized correctly
## 2.4 Precision
Precision = TP/(TP+FP), focusing on how many 1 is predicted correctly in all 1 predicted samples
figure------------------------------------------------------------------------------------
## 2.5 F1-score
Harmonic mean for Recall and Precision.
F1-Score=2*Recall*Precision/(Recall+Precision)
## 2.6 Specificity (TNR)
Specificity = TN/(TN+FP) , <->recall
## 2.7 Negative Predictive Value
NPV = TN/(TN+FN) <->precision
## 2.8 False Positive Rate
FPR=1-specificity=FP/(FP+TN)

> [!NOTE]
> Most important featues should be labeled as 1.

# 3. ROC-AUC


















