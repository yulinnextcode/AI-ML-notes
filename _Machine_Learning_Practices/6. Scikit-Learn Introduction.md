# 1. Scikit-learn introduction
> [!NOTE]
> Algrithm package, algrithm library and algrithm framework.
sklearn is algrithm library, Pandas is a package, NumPy is a library, PyTorch/TensorFlow are algorithm frameworks.
Package may only support one item or one class function. Library may include many classes. Framework may include objects as well.

[Understanding frameworks, libraries, and packages](https://datasciencedojo.com/blog/frameworks-libraries-and-packages/#:~:text=While%20frameworks%20provide%20structure%20and,component%20for%20successful%20software%20development).

[Framework vs library vs package vs module](https://dev.to/hamza/framework-vs-library-vs-package-vs-module-the-debate-3jpp).
> Module
> 
> Is the smallest piece of software. A module is a set of methods or functions ready to be used somewhere else.
> 
> Package
> 
> Is a collection of modules. This may sound funny, but usually what a package does, is gather a number of modules holding in general the same functional purpose. Making it easier to include all the related modules at once.
> 
> Library
> 
> Well library at it's core, is a collections of packages. It's purpose is to offer a set of functionalities ready to use without worrying about the subsequent packages. So a library is what you include when you want to add some functionality to your code. It does not force any coding style on you either.
> 
> Framework
> 
> It's a set of libraries. But this time, the framework does not just offer functionalities, but it also provides an architecture for the development work. In other words you don't include a framework. You integrate you code into it. He is the wire frame of the project. That's why a framework forces its coding style on you.

Sklearn is a completed algorithm library 
> [!NOTE]
> Underlying data structure has an significant effect on algorithm libraries. sklearn is built on NumPy, SciPy and matplotlib, which underlying data strecture is array. Therefore, algorithm library sklearn's underlying data strecture is array. Another example is distributed computation framework Spark, it has two algorithm libraries. One ML package is built on Spark basic data structure RDD, the other one MLLib package is built on Spark advanced data structure DataSet and FataFrame.
- **sklearn core object: estimator.** Most third-party library has its own core object, such as NumPy's Array, Pandas's DataFrame, PyTorch's tensor. Two steps to use the estimator. First instantiate object, then use the object to train model with the data.
```python
np.random.seed(24)
features, labels = arrayGenReg(delta=0.01)


import sklearn                            # Method 1
sklearn.learn_model.LinearRegression

from sklearn import linear_model          # Method 2
linear_model.LinearRegression

from sklearn.linear_model import LinearRegression   # Method 3

model = LinearRegression()

X=features[:, :2]
y=labels

model.fit(X,y)
model.coef_
model.inttercept_

model.predict(X)[:10]
y[:10]
```
- **sklearn functions.** These are not estimators (class), but are single functions. No need to instantiation before being used.
```python
from sklearn.metrics import mean_squared_error

mean_squared_error(model.predict(X),y)

model.get_parameters()
```
```python
class sklearn.linear_model.LinearRegression(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)

LinearRegression(fit_intercept=False)  # modify hyper-parameter example

```
> [!NOTE]
> User can check detailed parameters, attributes and methods using [SupervisedLearning_LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- Methods for solving parameters in training: Ordinary Least Squares

> [!IMPORTANT]
> **sklearn categorize its estimators and functions into 6 categories: Classification, Regression, Clustering, Dimensionality Reduction, Model Selection, Preprocessing**
- Estimators:
  - Supervised Learning: Classification, Regression
  - Unsupervised Learning: Clustering
- Functions:
  - Most of thses are functions, few of them are estimators.
> [!NOTE]
> User can check detailed [sklearn 6 categories](https://scikit-learn.org/stable/)
- User Guide: document sklearn all contents in [User Guide](https://scikit-learn.org/stable/user_guide.html)
- API: User can check [API documents](https://scikit-learn.org/stable/modules/classes.html) to see examples.
- Source codes: some are rewrite with cython.
# 2. Scikit-Learn commonly used features
## 2.1 sklearn read dataset
sklearn not only provides many **built-in datasets**, but also it provides many **functions to create datasets**.
Use [API documents](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) then check sklearn.datasets. This includes all datasets and methods for creating datasets, which are **loaders** and **samples generator**
## 2.2 sklearn data split methods
```python
from sklearn.model_selection import train_test_split

train_test_split(X,y,random_state=24)
```
## 2.3 sklearn data standardization and normalization
- Standardization, includes Z-Score standardization, 0-1 standardization
  - Z-Score standardization function method
```python
from sklearn import preprocessing   # This is a function, not an estimator class

preprocessing.scale    # z-scale
X = np.arange(9).reshape(3,3)
preprocessing.scale(X)
```
  - Z-Score standardization estimator method. (Compared to function mentioned above, estimators can be formed in series to build a ML pipeline. Functions can not formed in series by pipeline.)
> [!NOTE]
> Estimators can be formed in series to build a ML pipeline. Functions can not formed in series by pipeline. (Pipeline supports estimators)

```python
from sklearn.preprocessing import StandardScaler   # This is an estimator class, not function mentioned above

StandardScaler

scaler = StandardScaler()
X = np.arrange(15).reshape(5,3)
X_train, X_test=train_test_split(X)

scaler.fit(X_train)   # calculate statistics based on X_train data
scaler.scale_
scaler.mean_
scaler.var_

scaler.transform(X_train)   # similar like prediction(), standardize X_train
scaler.transform(X_test)    # stanadardize X_test data with statistics calculated based on X_train data

"""
scaler.fit(X_train) and scaler.transform(X_train) can be combined to
scaler.fit_transform(X_train)
statistics are also stored in scaler estimator
"""
```
  - 0-1 standardization function
```python
preprocessing.minmax_scale(X)
```
  - 0-1 standardization estimator
```python
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit_transform(X)
```
- Normalization, specifically targetd on one sample data (one line), divided by either Norm-1 or Norm-2
  - Normalization function
```python
preprocessing.normalize(X, norm='l1')
```
  - Normalization estimator
```python
from sklearn.preprocessing import Normalizer

normlize=Normalizer(norm='l1')
normlize.fit_transform(X)
```

> [!TIP]
> Another concept Regularization. This usually means add L1 norm or L2 norm to loss function, to avoid overfit.

## 2.4 Logistic regression estimator in sklearn
```python
from sklearn.linear_model import LogisticRegression

X,y=load_iris(return_X_y=True)
clf_test = LogisticRegression(max_iter=1000)
clf_test.fit(X,y)

clf_test.coef_
clf_test.predict(X)[:10]

clf_test.predict_proba(X)[:10]

clf_test.score(X,y)

from sklearn.metrics import accuracy_score
accuracy_score(y, clf_test.predict(X))
```
## 2.5 sklearn build ML pipeline
make_pipeline can be used to form a machine learning pipeline.
> [!TIP]
> make_pipeline can only serielize estimators, not functions. The final output is also an estimator.
```python
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))

X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)

pipe.fit(X_train, y_train)

pipe.predict(X_test)

pipe.score(X_test, y_test)

pipe.score(X_train, y_train)
``` 
## 2.6 save model in sklearn
jbolib pakaage to save and read model
```python
import joblib

joblib.dump(pipe, 'pipe.model')

pipe1=joblib.load('pipe.model')

pipe1.score(X_train, y_train)
```
# 3. Regularization and sklearn logistic regression estimator parameters
## 3.1 Regularization
L1 regularization, L2 regularization (by default), Elastic-net regularization are used in sklearn.
Add a regularizer to loss function can be called regularization. Regularizer can also be called a penalty term. The regularizer can be l1-norm or l2-norm or the combination of l1 and l2 norm.
The purpose of adding a regularizer is to avoid overfitting.

> [!TIP]
> Lasso regression = original linear regression loss function + l-1 norm
> Ridge regression = original linear regression loss function + l-2 norm

**[Empirical Risk & Structural Risk](https://alisure.github.io/2018/04/14/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%93%E6%B7%B7%E6%A6%82%E5%BF%B5%E4%B9%8B%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E4%B8%8E%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9/#:~:text=%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9(Empirical%20Risk)%EF%BC%9A,%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6%E8%BE%83%E5%B0%8F%EF%BC%89%E3%80%82)** 
- Empirical rish is a principle in statistical learning theory that defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true "risk") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the "empirical" risk).
- Structural risk is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting â€“ the model becoming too strongly tailored to the particularities of the training set and generalising poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.
## 3.2 Unswefitting, fitting and Overfitting
- A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term.
Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares.

![Python_File_Operation](/_Machine_Learning_Practices/imgs/Ridge.JPG)

- The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. The lasso estimate thus solves the minimization of the least-squares penalty.

![Python_File_Operation](/_Machine_Learning_Practices/imgs/Lasso.JPG)

```python
from sklearn.linear_model import Ridge, Lasso

reg_rid = Ridge(alpha=0.005)

reg_rid.fit(X, y)

reg_las = Lasso(alpha=0.001)

reg_las.fit(X,y)
```
> [!TIP]
> Linear Regression Modeling Stratagies:
> - Data augmentation, when model results is not good, consider to derive more features for data augmentation
> - Features selection, use Lasso l1 regulizer to select key features and skip unimportant features
> - Fix overfitting, use Ridge l2 regulizer to alleviate overfitting (If skip above step, too many features may affect l2's regularization effect)

> [!IMPORTANT]
> Not all models can be overfitting corrected by regularization. Models can be corrected by reguarization methods include linear regression, logistic regression, LDA, SVM and some PCA derived methods. **Tree models can not be overfit corrected with regularization method**.
## 3.3 Logistic regression's parameters in sklearn
- penalty
- dual
- tol
- C
- fit_intercept
- intercept_scaling
- class_weight
- random_state
- solver
- max_iter
- multi_class
- verbose
- warm_start
- l1_ratio
# 4. Logistic regression regularization modeling
```python
import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt

# estimators
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

# functions
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# prepare datasets
np.random.seed(24)
X = np.random.normal(0, 1, size=(1000,2))
y = np.array(X[:,0]+X[:,1]**2<1.5, int)
plt.scatter(X[:,0], X[:,1], c=y)

np.random.seed(24)
for i in range(200):
  y[np.random.randint(1000)]=1
  y[np.random.randint(1000)]=0
plt.scatter(X[:,0], X[:,1], c=y)

X_train, X_test, y_train, t_test=train_test_split(X,y,train_size=0.7,random_state=42)

# simple model can not fit, we need to use PloynomialFeatures to derive more features.
# degree determins the highest order to add model complexity
# penalty and C is to avoid overfitting
def plr(degree=l, penalty='none', C=1.0):
  pipe=make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), StandardScaler(), LogisticRegression(penalty=penalty, tol=1e-4, C=C, max_iter=int(1e6)))
  return pipe
```python
pl1=plr()
pl1.get_params()['polynomialfeatures__include_bias']
pl1.set_params(polynomialfeatures__include_bias=True)
pl1.get_params()['polynomialfeatures__include_bias']

pr1=plr()
pr1.fit(X_train, y_train)
pr1.score(X_train, y_train), pr1.score(X_test, y_test)

plot_decision_boundary(X,y,pr1)  

pr2=plr(degree=2)
pr2.fit(X_train, y_train)
pr2.socre(X_train, y_train), pr2.score(X_test, y_test)
plot_decision_boundary(X, y, pr2)

pr2.named_steps
pr2.named_steps['logisticregression'].coef_

# following to avoid over-fitting
pr3=plr(degree=10)
pr3.fit(X_train, y_train)
pr3.score(X_train, y_train), pr3.score(X_test, y_test)

pr3.fit(X_train, y_train)
pr3.score(X_train, y_train), pr3.score(X_test, y_test)

plot_decision_boundary(X,y,pr3)

score_l=[]
for degree in range(1,21):
  pr_temp=plr(degree=degree)
  pr_temp.fit(X_train, y_train)
  score_temp=[pr_temp.score(X_train, y_train), pr_temp.score(X_test, y_test)]
  score_l.append(score_temp)

np.array(score_l)

pl1=plr(degree=10, penalty='l1', C=1.0)
pl1.fit(X_train, y_train)

pl1.set_params(logisticregression__solver='saga')
pl1.fit(X_train, y_train)
pl1.score(X_train, y_train), pl1.score(X_test, y_test)

pl2=plr(degree=10, penalty='l2', C=1.0).fit(X_train, y_train)
```
# 5. ML fine tuning and grid search
sklearn use GridSearchCV to do fine tuning, and fine tuning is to adjust hpyer-parameters, not parameters.
Two key points for gridsearch. One is to construct parameter space, the other is to use proper estimator. When we construct parameter space, we need to consider control empiricial risk and structural risk at the same time.

## 5.1 Cross validation and estimators selection
For classification models, we usually use ROC-AUC and F1-Score.
Cross validation includes following steps:
- Split training dataset into train and validation data
- Train the model with training data, and validate the model with validating data
- Calculate multiple validation results on many validating data, and calculate its average result as model's final performance under certain hyper-parameter

> [!TIP]
> Since we already have train and validation data, do we still need test data?
> We either keep test data or not. If we keep test data, test data is to determine whether the previous cross-validation results process is effective or not. If we are confident with the cross-validation process, we can skip the test data.

## 5.2 Scikit-Learn grid search
It includes two method GridSearchCV and RandomizedSearchCV.
```python
from sklearn.datasets import load_iris
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=24)

clf = LogisticRegression(max_iter=int(1e6), solver='saga')

clf.get_params()

# create parameter space
param_grid_simple = {'penalty': ['l1','l2'],
                     'C': [1,0.5,0.1,0.05,0.01]}

# list of dict
param_grid_ra = [
  {'penalty': ['l1','l2'], 'C': [1,0.5,0.1,0.05,0.01]},
  {'penalty': ['elasticent'], 'C': [1,0.5,0.1,0.05,0.01], 'l1_ratio':[0.3,0.6,0.9]}
]

search=GridSearchCV(estimator=clf, param_grid=param_grid_simple)

search.fit(X_train, y_train)

```
> [!TIP]
> The smaller C is, the lsee penalty to empirical risk, and the larger penalty to structural risk.
# 6. Multi-classification model estimators' macro and micro process
Micro- and macro-averages (for whatever metric) will compute slightly different things, and thus their interpretation differs. A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).
# 7. GridSearchCV advanced methods
## 7.1 Use ML pipeline to construct all domain parameter space
Pipeline itself is a estimarot, and it also constructed by estimators. 
```python
pipe = make_pipeline(PolynomialFeatures(), StandardScaler(), LogisticRegression(max_iter=int(1e6)))  # pipe is an estimator, and it is constructed by another 3 estimators

pipe.get_params()   # this will return the above 3 estimators' all parameters
```
## 7.2 Optimize estimators selection
```python
scoring = {'AUC': 'roc_auc', 'Accuracy':make_scorer(accuracy_score)}
search = GridSearchCV(estimator=pipe, param_grid=para_grid_simple,scoring=scoring, refit='AUC', n_jobs=5)

search.fit(X_train, y_train)
```







































