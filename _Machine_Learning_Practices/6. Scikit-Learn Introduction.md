# 1. Scikit-learn introduction
> [!NOTE]
> Algrithm package, algrithm library and algrithm framework.
sklearn is algrithm library, Pandas is a package, NumPy is a library, PyTorch/TensorFlow are algorithm frameworks.
Package may only support one item or one class function. Library may include many classes. Framework may include objects as well.

[Understanding frameworks, libraries, and packages](https://datasciencedojo.com/blog/frameworks-libraries-and-packages/#:~:text=While%20frameworks%20provide%20structure%20and,component%20for%20successful%20software%20development).

[Framework vs library vs package vs module](https://dev.to/hamza/framework-vs-library-vs-package-vs-module-the-debate-3jpp).
> Module
> 
> Is the smallest piece of software. A module is a set of methods or functions ready to be used somewhere else.
> 
> Package
> 
> Is a collection of modules. This may sound funny, but usually what a package does, is gather a number of modules holding in general the same functional purpose. Making it easier to include all the related modules at once.
> 
> Library
> 
> Well library at it's core, is a collections of packages. It's purpose is to offer a set of functionalities ready to use without worrying about the subsequent packages. So a library is what you include when you want to add some functionality to your code. It does not force any coding style on you either.
> 
> Framework
> 
> It's a set of libraries. But this time, the framework does not just offer functionalities, but it also provides an architecture for the development work. In other words you don't include a framework. You integrate you code into it. He is the wire frame of the project. That's why a framework forces its coding style on you.

Sklearn is a completed algorithm library 
> [!NOTE]
> Underlying data structure has an significant effect on algorithm libraries. sklearn is built on NumPy, SciPy and matplotlib, which underlying data strecture is array. Therefore, algorithm library sklearn's underlying data strecture is array. Another example is distributed computation framework Spark, it has two algorithm libraries. One ML package is built on Spark basic data structure RDD, the other one MLLib package is built on Spark advanced data structure DataSet and FataFrame.
- **sklearn core object: estimator.** Most third-party library has its own core object, such as NumPy's Array, Pandas's DataFrame, PyTorch's tensor. Two steps to use the estimator. First instantiate object, then use the object to train model with the data.
```python
np.random.seed(24)
features, labels = arrayGenReg(delta=0.01)


import sklearn                            # Method 1
sklearn.learn_model.LinearRegression

from sklearn import linear_model          # Method 2
linear_model.LinearRegression

from sklearn.linear_model import LinearRegression   # Method 3

model = LinearRegression()

X=features[:, :2]
y=labels

model.fit(X,y)
model.coef_
model.inttercept_

model.predict(X)[:10]
y[:10]
```
- **sklearn functions.** These are not estimators (class), but are single functions. No need to instantiation before being used.
```python
from sklearn.metrics import mean_squared_error

mean_squared_error(model.predict(X),y)

model.get_parameters()
```
```python
class sklearn.linear_model.LinearRegression(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)

LinearRegression(fit_intercept=False)  # modify hyper-parameter example

```
> [!NOTE]
> User can check detailed parameters, attributes and methods using [SupervisedLearning_LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- Methods for solving parameters in training: Ordinary Least Squares

> [!IMPORTANT]
> **sklearn categorize its estimators and functions into 6 categories: Classification, Regression, Clustering, Dimensionality Reduction, Model Selection, Preprocessing**
- Estimators:
  - Supervised Learning: Classification, Regression
  - Unsupervised Learning: Clustering
- Functions:
  - Most of thses are functions, few of them are estimators.
> [!NOTE]
> User can check detailed [sklearn 6 categories](https://scikit-learn.org/stable/)
- User Guide: document sklearn all contents in [User Guide](https://scikit-learn.org/stable/user_guide.html)
- API: User can check [API documents](https://scikit-learn.org/stable/modules/classes.html) to see examples.
- Source codes: some are rewrite with cython.
# 2. Scikit-Learn commonly used features
## 2.1 sklearn read dataset
sklearn not only provides many **built-in datasets**, but also it provides many **functions to create datasets**.
Use [API documents](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) then check sklearn.datasets. This includes all datasets and methods for creating datasets, which are **loaders** and **samples generator**
## 2.2 sklearn data split methods
```python
from sklearn.model_selection import train_test_split

train_test_split(X,y,random_state=24)
```
## 2.3 sklearn data standardization and normalization
- Standardization, includes Z-Score standardization, 0-1 standardization
  - Z-Score standardization function method
```python
from sklearn import preprocessing   # This is a function, not an estimator class

preprocessing.scale    # z-scale
X = np.arange(9).reshape(3,3)
preprocessing.scale(X)
```
  - Z-Score standardization estimator method. (Compared to function mentioned above, estimators can be formed in series to build a ML pipeline. Functions can not formed in series by pipeline.)
> [!NOTE]
> Estimators can be formed in series to build a ML pipeline. Functions can not formed in series by pipeline. (Pipeline supports estimators)

```python
from sklearn.preprocessing import StandardScaler   # This is an estimator class, not function mentioned above

StandardScaler

scaler = StandardScaler()
X = np.arrange(15).reshape(5,3)
X_train, X_test=train_test_split(X)

scaler.fit(X_train)   # calculate statistics based on X_train data
scaler.scale_
scaler.mean_
scaler.var_

scaler.transform(X_train)   # similar like prediction(), standardize X_train
scaler.transform(X_test)    # stanadardize X_test data with statistics calculated based on X_train data

"""
scaler.fit(X_train) and scaler.transform(X_train) can be combined to
scaler.fit_transform(X_train)
statistics are also stored in scaler estimator
"""
```
  - 0-1 standardization function
```python
preprocessing.minmax_scale(X)
```
  - 0-1 standardization estimator
```python
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit_transform(X)
```
- Normalization, specifically targetd on one sample data (one line), divided by either Norm-1 or Norm-2
  - Normalization function
```python
preprocessing.normalize(X, norm='l1')
```
  - Normalization estimator
```python
from sklearn.preprocessing import Normalizer

normlize=Normalizer(norm='l1')
normlize.fit_transform(X)
```

> [!TIP]
> Another concept Regularization. This usually means add L1 norm or L2 norm to loss function, to avoid overfit.

# 3. Logistic regression estimator in sklearn
```python
from sklearn.linear_model import LogisticRegression

X,y=load_iris(return_X_y=True)
clf_test = LogisticRegression(max_iter=1000)
clf_test.fit(X,y)

clf_test.coef_
clf_test.predict(X)[:10]

clf_test.predict_proba(X)[:10]

clf_test.score(X,y)

from sklearn.metrics import accuracy_score
accuracy_score(y, clf_test.predict(X))
```
# 4. sklearn build ML pipeline
make_pipeline can be used to form a machine learning pipeline.
> [!TIP]
> make_pipeline can only serielize estimators, not functions. The final output is also an estimator.
```python
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))

X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)

pipe.fit(X_train, y_train)

pipe.predict(X_test)

pipe.score(X_test, y_test)

pipe.score(X_train, y_train)
``` 
# 5. save model in sklearn
jbolib pakaage to save and read model
```python
import joblib

joblib.dump(pipe, 'pipe.model')

pipe1=joblib.load('pipe.model')

pipe1.score(X_train, y_train)
```





























