# 1. Decision Tree Modeling Process
Decision tree modeling is an important supervised learning algorithm. Like previous clustering algorithms, decision tree model is a general caterogy for a type of model.
Decision tree models are very high efficiency, high prediction accuracy, process is simple and clear, and it is a "white-box" model.
**Decision tree models can do both regression and classification prediction**, it also can output feature significance. In essemble learning, the most commonly used basic classifier is tree model.

Decision tree is the basic component of random forest and an intuitive model. We can think of a decision tree as a series of yes/no questions about the data that ultimately lead to an answer to a classification or regression problem. The technical details of decision trees lie in how questions about the data are formed. In the CART algorithm, a decision tree is constructed by determining questions (called node splits) that will cause the Gini coefficient (Gini Impurity) to decay as quickly as possible after being answered. This means that the decision tree is trying to form nodes that classify the data as quickly as possible by finding appropriate thresholds (dichotomous comparison values) among the features that cleanly separate the data into different classes.


## 1.1 Build decision tree with logistic regression
The essentials of building a decision tree is to dig effective classification rules.

## 1.2 Decision tree categories
Decision tree model is not an algorithm, it is a category of algorithms. Followings are most popular ones:
- ID3 (Iterative Dichotomiser 3)、C4.5、 C5.0 decision tree
- CART decisioin tree (classification and regression trees), or C&RT. sklearn decision tree is mostly based on CART model.
- CHAID decition tree (Chi-square automatic interaction detection).

# 2. CART classification trees using sklearn
## 2.1 CART classification process
How do we build the estimator is the key factor. Accuracy is no more suitable for decision trees. We use purity as the key estimator. Here are three key purity estimators for CART:
- Classification error
- Entropy
- Gini
> [!IMPORTANT]
> - ID3、C4.5、C5.0 use entropy
> - Cart use Gini
> - Greedy algorithm use classification error

Another question is to estimator multiple data sets' purity.
## 2.2 Implement CART classification tree with Scikit-Learn
```python
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

DecisionTreeClassifier?
```
| Name  | Description |
| ------------- | ------------- |
| criterion  | estimator or loss function, default is gini coefficient, user can select information entropy {"gini", "entropy"} |
| splitter  | tres model grow method, default is grow with loss function reduce most fastly {"best", "random"}  |
| max_depth  | The maximum depth of the tree. like max_iter  |
| splitter  | tres model grow method, default is grow with loss function reduce most fastly  |
| min_samples_split  | minimal data sample to split  |
| min_samples_leaf  | minimal data sample of leaf nodes  |
| min_weight_fraction_leaf  | minimal weight sum of leaf nodes  |
| max_features  | number of features to consider when looking for the best split, default=None  |
| random_state  | random seed, this seed will determine the "randomness" of the above splitter and max_features. If the random seed is determined, then splitter and max_features are determined. If random_state is not specified, then there is a lot of randomness, and the model will be inconsistent every time.  |
| max_leaf_nodes  | maximum leaf nodes number  |
| min_impurity_decrease  | minimum gini/entropy loss to further grow tree  |
| class_weight  |  different sample weights |
| ccp_alpha  | like regularizer, the larger alpha is, the larger structural risk is  |

> [!TIP]
> max_features is maximum number of randomly selected features for a tree

# 3. CART regression trees using sklearn
CART regression model can solve regression problems. Also, it is an important basic unit for assemble algorithm - GBDT.
> [!IMPORTANT]
> For assemble algorithm, no matter regression or classification problems, CART regression tree is algorithm GBDT's only basic unit.

The basic idea behind CART is to divide the feature space by selecting the best feature and its corresponding split point that maximizes the homogeneity of the target variable within each resulting segment. For classification problems, the homogeneity is typically measured using metrics like Gini impurity or entropy, while for regression problems, it can be measured using mean squared error (MSE) or mean absolute error (MAE).

The CART algorithm starts with the entire dataset and recursively splits it into two subsets based on a selected feature and split point. This process is repeated for each resulting subset until a stopping criterion is met, such as reaching a maximum tree depth, a minimum number of samples in each leaf node, or the inability to further improve the homogeneity. The result is a binary tree where each leaf node represents a specific prediction or class label.

- For classification tasks, once the tree is built, the class label assigned to a new data point is determined by traversing the tree from the root node to a leaf node based on the feature values of the data point. The majority class label in that leaf node is then assigned as the predicted class label for the data point.
- For regression tasks, the predicted value for a new data point is obtained by traversing the tree and taking the average of the target variable values in the leaf node corresponding to the data point.

| Name  | Description |
| ------------- | ------------- |
| criterion  | estimator or loss function, default is gini coefficient, user can select information entropy {"mse", "friedman_mse", "mae"} |
| splitter  | tres model grow method, default is grow with loss function reduce most fastly {"best", "random"}  |
| max_depth  | The maximum depth of the tree. like max_iter  |
| splitter  | tres model grow method, default is grow with loss function reduce most fastly  |
| min_samples_split  | minimal data sample to split  |
| min_samples_leaf  | minimal data sample of leaf nodes  |
| min_weight_fraction_leaf  | minimal weight sum of leaf nodes  |
| max_features  | number of features to consider when looking for the best split, default=None  |
| random_state  | random seed, this seed will determine the "randomness" of the above splitter and max_features. If the random seed is determined, then splitter and max_features are determined. If random_state is not specified, then there is a lot of randomness, and the model will be inconsistent every time.  |
| max_leaf_nodes  | maximum leaf nodes number  |
| min_impurity_decrease  | minimum gini/entropy loss to further grow tree  |
| class_weight  |  different sample weights |
| ccp_alpha  | like regularizer, the larger alpha is, the larger structural risk is  |

To summarize, the primary drawback of decision trees is their tendency to overfit and yield subpar generalization performance, despite employing pre-pruning techniques. As a result, in many cases, ensemble methods are preferred over individual decision trees to address this limitation.

Decision tree is the basic component of random forest and an intuitive model. We can consider a decision tree as a series of yes/no questions about the data that ultimately lead to an answer to a classification or regression problem. Decision Tree's key point is about how to decide criteria to divide the data. In the CART algorithm, a decision tree is constructed by determining questions (called node splits) that will cause the Gini coefficient (Gini Impurity) to decay as quickly as possible after being answered. This means that the decision tree is trying to form nodes that classify the data as quickly as possible by finding appropriate thresholds among the features that clenaly separate the data into different classes.

Random forest is a model composed of many decision trees. Rather than simply averaging the predictions of all trees, RF uses two key concepts, which is where the word random in its name comes from:
- Randomly sample training data points (with replacement) when building the tree
- Consider a random subset of features when splitting nodes (no replacement)


















