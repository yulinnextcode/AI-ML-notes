<details>
<summary><h1>1. Hyperparameter optimization and enumerated grid theory limit</h1></summary>
  
Hpyerparameter optimization method mainly includes 4 categories:
  - Grid search optimization method
  - Baysian optimization method
  - Gradient-based optimization method
  - Searm optimization method (evolutionary algorithm, genetic algorithm)

Disadvantage of grid search: large amount of possible candidates

Following is a benchmark result using grid search.
```python
from sklearn.ensemble import RandomForestRegressor as RFR
from sklearn.model_selection import corss_validate, KFold, GridSearchCV

X=data.iloc[:,:-1]
y=data.iloc[:,-1]

# parameter space
param_grid_simple = {
    "criterion": ["squared_error", "poisson"],
    "n_estimators": [*range(20,100,5)],
    "max_depth": [*range(10,25,2)],
    "max_fatures": ["log2", "sqrt", 16, 32, 64, "auto"],
    "min_impurity_decrease": [*np.arange(0,5,10)]
}

reg=RFR(random_state=1412, verbose=True, n_jobs=-1)
cv=KFold(n_splits=5, shuffle=True, random_state=1412)
search=GridSearchCV(estimator=reg,
                    param_grid=param_grid_simple,
                    scoring="neg_mean_squared_error",
                    verbose=True,
                    cv=cv,
                    n_jobs=-1)

start=time.time()
search.fit(X,y)
print(time.time()-start)

search.best_estimator_
abs(search.best_score_)**0.5

ad_reg=RFR(n_estimators=85, max_depth=23, max_features=16, random_state=1412)

cv=KFold(n_splits=5, shuffle=True, random_state=1412)
result_post_adjusted=cross_validate(ad_reg,
                                    X, y,
                                    cv=cv,
                                    scoring="neg_mean_squared_error",
                                    return_train_score=True,
                                    verbose=True,
                                    n_jobs=-1
                                    )

def RMSE(cvresult,key):
  return (abs(cvresult[key])**0.5).mean()

def count_space(param):
  no_option=1
  for i in param_grid_simple:
    no_option*=len(param_grid_simple[i])
  print(no_option)

def rebuild_on_best_param(ad_reg):
  cv=KFold(n_splits=5, shuffle=True, random_state=1412)
  result_post_adjusted=cross_validated(ad_reg,
                                       X,y,
                                       cv=cv,
                                       scoring="neg_mean_squared_error",
                                       return_train_score=True,
                                       verbose=True,
                                       n_jobs=-1)
  print("Training RMSE:{:.3f}".format(RMSE(result_post_adjusted, "train_score")))
  print("Testing RMSE:{:.3f}".format(RMSE(result_post_adjusted, "test_score")))
```

</details>

<details>
<summary><h1>2. Random grid search RandomizedSearchCV</h1></summary>

## 2.1 Random grid search basic principles
Random search cross-validation is a technique that searches for the optimal hyperparameters of a model by evaluating the model's performance on random combinations of hyperparameter values. The idea is to define a set of hyperparameters and a range of values for each hyperparameter, and then randomly sample values from these ranges to create different combinations of hyperparameters. This process is repeated a specified number of times, and the best combination of hyperparameters that produces the best performance on a validation set is selected.

The random search cross-validation technique can be implemented using the RandomizedSearchCV class from the scikit-learn library in Python. The RandomizedSearchCV class takes as input a machine learning model, a distribution of hyperparameters, and a cross-validation strategy. The distribution of hyperparameters specifies how to sample values from each hyperparameter range.

Random search cross-validation (RandomizedSearchCV) is another powerful technique for optimizing the hyperparameters of a machine learning model. It works in a similar way to grid search cross-validation, but instead of searching over a predefined grid of hyperparameters, it samples them randomly from a distribution. In this blog, we will discuss the concept of random search cross-validation and provide a code example in Python.

## 2.2 Random grid search implementation
```python
from sklearn.model_selection import RandomizedSearchCV

param_grid_simple = {"citerion": ["squared_error","poisson"],
                     "n_estimators": [*range(20,100,5)],
                     "max_depth": [*range(10,25,2)],
                     "max_features": ["log2", "sqrt", 16,32,64,"auto"],
                     "min_impurity_decrease": [*np.arange(0,5,10)]
                    }

reg = RFR(random_state=1412, verbose=True, n_jobs=-1)
cv = KFold(n_split=5, shuffle=True, random_state=1412)

search = RandomizedSearchCV(estimator=reg,
                            param_distributions=param_grid_simple,
                            n_iter=800,    # sub domain is 800
                            scoring="neg_mean_squared_error",
                            verbose=True,
                            cv=cv,
                            random_state=1412,
                            n_jobs=-1)

start=time.time()
search.fit(X,y)
print(time.time()-start)

search.best_estimator_
abs(search.best_score_)**0.5

ad_reg=RFR(max_depth=24, max_features=16, min_impurity_decrease=0,
           n_estimators=85, n_jobs=-1, random_state=1412,
           verbose=True)

rebuild_on_best_param(ad_reg)   #or use refit()

```

```python
from mpl_toolkits.mplot3d import axes3d

p1,p2,MSE=axes3d.get_test_date(0.05)

len(p1) #120
len(p2) #120

import numpy as np

n=10000

unsampled=np.random.randint(0,14400,14400-n)
p1,p2,MSE=axes3d.get_test_data(0.05)

MSE=MSE.ravel()
MSE[unsampled]=np.nan
MSE=MSE.reshape((120,120))

plt.figure(dpi=300)
ax=plt.axes(projection="3d")
ax.plot_wireframe(p1,p2,MSE,rstride=2,cstride=2,linewidth=0.5)
ax.zaxis.set_tick_params(labelsize=7)
ax.xaxis.set_tick_params(labelsize=7)
ax.yaxis.set_tick_params(labelsize=7)

MSE=MSE.ravel().tolist()
MSE=[x for x in MSE if str(x) != 'nan']
print(np.min(MSE))
```

## 2.3 Random grid search theory limitation
- The sampled subspace can reflect the distribution of the global space to a certain extent, and the larger the subspace is (the more parameter combinations it contains), the closer the distribution of the subspace is to the distribution of the global space.
- When the global space itself is dense enough, a very small subspace can also obtain a distribution similar to that of the global space.
- If the global space includes the theoretical minimum value of the loss function, then a subspace that is highly similar to the distribution of the global space is likely to also include the minimum value of the loss function, or include a series of secondary minimum values very close to the minimum value.
</details>

<details>
<summary><h1>3. Halving grid search HalvingSearchCV</h1></summary>
  
To fix the problem that enumeration grid search is too slow, sklearn presents two optimization methods: one is to **adjust the search space**, and the other is to **adjust the data for each training**. The method to adjust the search space is random grid search, and the method to adjust each training data is half grid search.

Part IV: Why I Choose HalvingGridSearch? HalvingGridSearch checks a lot of boxes for me:
- Speed: The principle of ‘successive halving’ rapidly reduces the search space, making it far faster than traditional methods, particularly with large parameter sets.
- Performance: It holds its own against GridSearchCV in terms of performance and, in certain situations, even outdoes it.
- Flexibility: With adjustable options for resource allocation, it allows for greater control over the process.
```python
class sklearn.model_selection.HalvingGridSearchCV(estimator, param_grid, *, factor=3, resource='n_samples',max_resources='auto', min_resources='exhaust', aggressive_elimination=False, cv=5, scoring=None, refit=True, error_score=nan, return_train_score=True, random_state=None, n_jobs=None, verbose=0)
```
> [!TIP]
> According to sklearn, HalvingGridSearch: The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.

## 3.1 Halving grid search procedure

HalvingSearchCV procedure:
- First, a small self-d0 is randomly sampled from the full data set without replacement, and the performance of all parameter combinations is verified on d0. According to the verification results on d0, the half of the parameter combinations with the lowest score 1/2 will be eliminated.
- Then, a subset d1 that is twice as large as d0 is sampled from the full data set without replacement, and the performance of the remaining half of the parameter combinations is verified on d1. According to the verification results on d1, the parameter combinations with the lowest 1/2 score will be eliminated.
- Then sample d2, which is twice as large as d1, from the full data set without replacement, and verify the performance of the remaining 1/4 parameter combination on d2. According to the verification results on d2, the parameter combinations with the lowest 1/2 score will be eliminated.

| Iteration  | Number of Sample Data | Number of Hyperparameter |
| ------------- | ------------- | ------------- |
| 1  | S  | C  |
| 2  | 2S  | 1/2 * C  |
| 3  | 4S  | 1/4 * C  |
| ...  |  |   |


> [!IMPORTANT]
> When **1/n * C <= 1 or nS > total sample data**, then the iteration will stop. Also, S should not be too small, and total amount data must be large enough.

> [!TIP]
> Halving grid search does not perform well on small sample data sets

## 3.2 Halving grid search implementation

```python
from sklearn.experimental import enable_halving_search_cv  
from sklearn.model_selection import HalvingGridSearchCV, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import time

# Create a synthetic dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, random_state=42)

# Define the model and parameter grid
model = RandomForestClassifier(random_state=42)
param_grid = {'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']}

# Set up HalvingGridSearchCV
search_halving = HalvingGridSearchCV(model, param_grid, cv=5, factor=2, resource='n_samples', min_resource=10, aggressive_elimination=True)

# Set up GridSearchCV
search_normal = GridSearchCV(model, param_grid, cv=5)

# Fit the data and measure the time for HalvingGridSearchCV
start = time.time()
search_halving.fit(X, y)
end = time.time()
print("HalvingGridSearchCV Time:", end - start)

# Fit the data and measure the time for GridSearchCV
start = time.time()
search_normal.fit(X, y)
end = time.time()
print("GridSearchCV Time:", end - start)
```

## 3.3 Halving random grid search
The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.

The candidates are sampled at random from the parameter space and the number of sampled candidates is determined by n_candidates.
```python
class sklearn.model_selection.HalvingRandomSearchCV(estimator, param_distributions, *, n_candidates='exhaust', factor=3, resource='n_samples', max_resources='auto', min_resources='smallest', aggressive_elimination=False, cv=5, scoring=None, refit=True, error_score=nan, return_train_score=True, random_state=None, n_jobs=None, verbose=0)
```

</details>

<details>
<summary><h1>4. AutoML prospective</h1></summary>

AutoML, or automated machine learning, is the process of using automation to apply machine learning (ML) models to real-world problems. This can include every stage from starting with a raw dataset to building a machine learning model ready for deployment. AutoML can save time and resources, and often provides faster, more accurate outputs than hand-coded algorithms.

AutoML can be used for the following tasks: Classification, Regression, Forecasting, Computer vision, and NLP.

Automated machine learning, also known as AutoML, is the process of automating the end-to-end process of building machine learning models. This includes tasks such as data preprocessing, feature engineering, model selection, and hyperparameter tuning.

![Procedure](https://dataknowsall.com/hs-fs/hubfs/hyper_00.png?width=800&height=522&name=hyper_00.png)

The goal of AutoML is to make it easier for non-experts to develop machine learning models, by providing a simple, user-friendly interface for training and deploying models. This can help to democratize machine learning and make it more accessible to a wider range of people, including those with little or no experience in data science.

For data scientists and MLOps teams, AutoML can reduce manual labor and simplify routine tasks, while allowing other parts of the organization to participate in the process of creating and deploying machine learning models.

## 4.1 Top AutoML frames
- Google AI Platform (Cloud AutoML)
- Microsoft Azure AutoML
- Amazon SageMaker Autopilot
- Auto-Sklearn

'Please see more Azure AutoML [examples here](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs/automl-forecasting-task-energy-demand/automl-forecasting-task-energy-demand-advanced.ipynb).

'[Azure Machine Learning CLI and Python SDK](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train?view=azureml-api-2&tabs=python).

'[Azure AutoML model](https://medium.com/data-science-at-microsoft/azure-automl-quickly-build-high-quality-ml-models-3b53733da2d).


</details>
