<details>
<summary><h1>1. Hyperparameter optimization and enumerated grid theory limit</h1></summary>
  
Hpyerparameter optimization method mainly includes 4 categories:
  - Grid search optimization method
  - Baysian optimization method
  - Gradient-based optimization method
  - Searm optimization method (evolutionary algorithm, genetic algorithm)

Disadvantage of grid search: large amount of possible candidates

Following is a benchmark result using grid search.
```python
from sklearn.ensemble import RandomForestRegressor as RFR
from sklearn.model_selection import corss_validate, KFold, GridSearchCV

X=data.iloc[:,:-1]
y=data.iloc[:,-1]

# parameter space
param_grid_simple = {
    "criterion": ["squared_error", "poisson"],
    "n_estimators": [*range(20,100,5)],
    "max_depth": [*range(10,25,2)],
    "max_fatures": ["log2", "sqrt", 16, 32, 64, "auto"],
    "min_impurity_decrease": [*np.arange(0,5,10)]
}

reg=RFR(random_state=1412, verbose=True, n_jobs=-1)
cv=KFold(n_splits=5, shuffle=True, random_state=1412)
search=GridSearchCV(estimator=reg,
                    param_grid=param_grid_simple,
                    scoring="neg_mean_squared_error",
                    verbose=True,
                    cv=cv,
                    n_jobs=-1)

start=time.time()
search.fit(X,y)
print(time.time()-start)

search.best_estimator_
abs(search.best_score_)**0.5

ad_reg=RFR(n_estimators=85, max_depth=23, max_features=16, random_state=1412)

cv=KFold(n_splits=5, shuffle=True, random_state=1412)
result_post_adjusted=cross_validate(ad_reg,
                                    X, y,
                                    cv=cv,
                                    scoring="neg_mean_squared_error",
                                    return_train_score=True,
                                    verbose=True,
                                    n_jobs=-1
                                    )

def RMSE(cvresult,key):
  return (abs(cvresult[key])**0.5).mean()

def count_space(param):
  no_option=1
  for i in param_grid_simple:
    no_option*=len(param_grid_simple[i])
  print(no_option)

def rebuild_on_best_param(ad_reg):
  cv=KFold(n_splits=5, shuffle=True, random_state=1412)
  result_post_adjusted=cross_validated(ad_reg,
                                       X,y,
                                       cv=cv,
                                       scoring="neg_mean_squared_error",
                                       return_train_score=True,
                                       verbose=True,
                                       n_jobs=-1)
  print("Training RMSE:{:.3f}".format(RMSE(result_post_adjusted, "train_score")))
  print("Testing RMSE:{:.3f}".format(RMSE(result_post_adjusted, "test_score")))
```

</details>

<details>
<summary><h1>2. Random grid search RandomizedSearchCV</h1></summary>

## 2.1 Random grid search basic principles
Random search cross-validation is a technique that searches for the optimal hyperparameters of a model by evaluating the model's performance on random combinations of hyperparameter values. The idea is to define a set of hyperparameters and a range of values for each hyperparameter, and then randomly sample values from these ranges to create different combinations of hyperparameters. This process is repeated a specified number of times, and the best combination of hyperparameters that produces the best performance on a validation set is selected.

The random search cross-validation technique can be implemented using the RandomizedSearchCV class from the scikit-learn library in Python. The RandomizedSearchCV class takes as input a machine learning model, a distribution of hyperparameters, and a cross-validation strategy. The distribution of hyperparameters specifies how to sample values from each hyperparameter range.

Random search cross-validation (RandomizedSearchCV) is another powerful technique for optimizing the hyperparameters of a machine learning model. It works in a similar way to grid search cross-validation, but instead of searching over a predefined grid of hyperparameters, it samples them randomly from a distribution. In this blog, we will discuss the concept of random search cross-validation and provide a code example in Python.

## 2.2 Random grid search implementation
```python
from sklearn.model_selection import RandomizedSearchCV

param_grid_simple = {"citerion": ["squared_error","poisson"],
                     "n_estimators": [*range(20,100,5)],
                     "max_depth": [*range(10,25,2)],
                     "max_features": ["log2", "sqrt", 16,32,64,"auto"],
                     "min_impurity_decrease": [*np.arange(0,5,10)]
                    }

reg = RFR(random_state=1412, verbose=True, n_jobs=-1)
cv = KFold(n_split=5, shuffle=True, random_state=1412)

search = RandomizedSearchCV(estimator=reg,
                            param_distributions=param_grid_simple,
                            n_iter=800,    # sub domain is 800
                            scoring="neg_mean_squared_error",
                            verbose=True,
                            cv=cv,
                            random_state=1412,
                            n_jobs=-1)

start=time.time()
search.fit(X,y)
print(time.time()-start)

search.best_estimator_
abs(search.best_score_)**0.5

ad_reg=RFR(max_depth=24, max_features=16, min_impurity_decrease=0,
           n_estimators=85, n_jobs=-1, random_state=1412,
           verbose=True)

rebuild_on_best_param(ad_reg)   #or use refit()

```

```python
from mpl_toolkits.mplot3d import axes3d

p1,p2,MSE=axes3d.get_test_date(0.05)

len(p1) #120
len(p2) #120

import numpy as np

n=10000

unsampled=np.random.randint(0,14400,14400-n)
p1,p2,MSE=axes3d.get_test_data(0.05)

MSE=MSE.ravel()
MSE[unsampled]=np.nan
MSE=MSE.reshape((120,120))

plt.figure(dpi=300)
ax=plt.axes(projection="3d")
ax.plot_wireframe(p1,p2,MSE,rstride=2,cstride=2,linewidth=0.5)
ax.zaxis.set_tick_params(labelsize=7)
ax.xaxis.set_tick_params(labelsize=7)
ax.yaxis.set_tick_params(labelsize=7)

MSE=MSE.ravel().tolist()
MSE=[x for x in MSE if str(x) != 'nan']
print(np.min(MSE))
```

## 2.3 Random grid search theory limitation
- The sampled subspace can reflect the distribution of the global space to a certain extent, and the larger the subspace is (the more parameter combinations it contains), the closer the distribution of the subspace is to the distribution of the global space.
- When the global space itself is dense enough, a very small subspace can also obtain a distribution similar to that of the global space.
- If the global space includes the theoretical minimum value of the loss function, then a subspace that is highly similar to the distribution of the global space is likely to also include the minimum value of the loss function, or include a series of secondary minimum values very close to the minimum value.
</details>

<details>
<summary><h1>3. Halving grid search HalvingSearchCV</h1></summary>
  
To fix the problem that enumeration grid search is too slow, sklearn presents two optimization methods: one is to **adjust the search space**, and the other is to **adjust the data for each training**. The method to adjust the search space is random grid search, and the method to adjust each training data is half grid search.

Part IV: Why I Choose HalvingGridSearch? HalvingGridSearch checks a lot of boxes for me:
- Speed: The principle of ‘successive halving’ rapidly reduces the search space, making it far faster than traditional methods, particularly with large parameter sets.
- Performance: It holds its own against GridSearchCV in terms of performance and, in certain situations, even outdoes it.
- Flexibility: With adjustable options for resource allocation, it allows for greater control over the process.
```python
class sklearn.model_selection.HalvingGridSearchCV(estimator, param_grid, *, factor=3, resource='n_samples',max_resources='auto', min_resources='exhaust', aggressive_elimination=False, cv=5, scoring=None, refit=True, error_score=nan, return_train_score=True, random_state=None, n_jobs=None, verbose=0)
```
> [!TIP]
> According to sklearn, HalvingGridSearch: The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.

## 3.1 Halving grid search procedure

HalvingSearchCV procedure:
- First, a small self-d0 is randomly sampled from the full data set without replacement, and the performance of all parameter combinations is verified on d0. According to the verification results on d0, the half of the parameter combinations with the lowest score 1/2 will be eliminated.
- Then, a subset d1 that is twice as large as d0 is sampled from the full data set without replacement, and the performance of the remaining half of the parameter combinations is verified on d1. According to the verification results on d1, the parameter combinations with the lowest 1/2 score will be eliminated.
- Then sample d2, which is twice as large as d1, from the full data set without replacement, and verify the performance of the remaining 1/4 parameter combination on d2. According to the verification results on d2, the parameter combinations with the lowest 1/2 score will be eliminated.

| Iteration  | Number of Sample Data | Number of Hyperparameter |
| ------------- | ------------- | ------------- |
| 1  | S  | C  |
| 2  | 2S  | 1/2 * C  |
| 3  | 4S  | 1/4 * C  |
| ...  |  |   |


> [!IMPORTANT]
> When **1/n * C <= 1 or nS > total sample data**, then the iteration will stop. Also, S should not be too small, and total amount data must be large enough.

> [!TIP]
> Halving grid search does not perform well on small sample data sets

## 3.2 Halving grid search implementation

```python
from sklearn.experimental import enable_halving_search_cv  
from sklearn.model_selection import HalvingGridSearchCV, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import time

# Create a synthetic dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, random_state=42)

# Define the model and parameter grid
model = RandomForestClassifier(random_state=42)
param_grid = {'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']}

# Set up HalvingGridSearchCV
search_halving = HalvingGridSearchCV(model, param_grid, cv=5, factor=2, resource='n_samples', min_resource=10, aggressive_elimination=True)

# Set up GridSearchCV
search_normal = GridSearchCV(model, param_grid, cv=5)

# Fit the data and measure the time for HalvingGridSearchCV
start = time.time()
search_halving.fit(X, y)
end = time.time()
print("HalvingGridSearchCV Time:", end - start)

# Fit the data and measure the time for GridSearchCV
start = time.time()
search_normal.fit(X, y)
end = time.time()
print("GridSearchCV Time:", end - start)
```

## 3.3 Halving random grid search
The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.

The candidates are sampled at random from the parameter space and the number of sampled candidates is determined by n_candidates.
```python
class sklearn.model_selection.HalvingRandomSearchCV(estimator, param_distributions, *, n_candidates='exhaust', factor=3, resource='n_samples', max_resources='auto', min_resources='smallest', aggressive_elimination=False, cv=5, scoring=None, refit=True, error_score=nan, return_train_score=True, random_state=None, n_jobs=None, verbose=0)
```

</details>

<details>
<summary><h1>4. AutoML prospective</h1></summary>

AutoML, or automated machine learning, is the process of using automation to apply machine learning (ML) models to real-world problems. This can include every stage from starting with a raw dataset to building a machine learning model ready for deployment. AutoML can save time and resources, and often provides faster, more accurate outputs than hand-coded algorithms.

AutoML can be used for the following tasks: Classification, Regression, Forecasting, Computer vision, and NLP.

Automated machine learning, also known as AutoML, is the process of automating the end-to-end process of building machine learning models. This includes tasks such as data preprocessing, feature engineering, model selection, and hyperparameter tuning.

![Procedure](https://dataknowsall.com/hs-fs/hubfs/hyper_00.png?width=800&height=522&name=hyper_00.png)

The goal of AutoML is to make it easier for non-experts to develop machine learning models, by providing a simple, user-friendly interface for training and deploying models. This can help to democratize machine learning and make it more accessible to a wider range of people, including those with little or no experience in data science.

For data scientists and MLOps teams, AutoML can reduce manual labor and simplify routine tasks, while allowing other parts of the organization to participate in the process of creating and deploying machine learning models.

## 4.1 Top AutoML frames
- Google AI Platform (Cloud AutoML)
- Microsoft Azure AutoML
- Amazon SageMaker Autopilot
- Auto-Sklearn

'Please see more Azure AutoML [examples here](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs/automl-forecasting-task-energy-demand/automl-forecasting-task-energy-demand-advanced.ipynb).

'[Azure Machine Learning CLI and Python SDK](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train?view=azureml-api-2&tabs=python).

'[Azure AutoML model](https://medium.com/data-science-at-microsoft/azure-automl-quickly-build-high-quality-ml-models-3b53733da2d).

'[AutoML examples](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs/automl-forecasting-task-energy-demand/automl-forecasting-task-energy-demand-advanced.ipynb).

'[AutoML in Azure](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-first-experiment-automated-ml?view=azureml-api-2).

</details>

<details>
<summary><h1>5. Bayesian optimization</h1></summary>

## 5.1 Bayesian optimization fundamentals
Bayesian Optimization has been widely used for the hyperparameter tuning purpose in the Machine Learning world. Despite the fact that there are many terms and math formulas involved, the concept behind turns out to be very simple.

Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. It works by building a probabilistic model of the objective function, called the surrogate function, that is then searched efficiently with an acquisition function before candidate samples are chosen for evaluation on the real objective function.

Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset.

### 5.1.1 Bayesian optimization basic procedure
- **Step 1: Define f(x) which is to be estimated and the domain of x**
- **Step 2: Select n values of x and calculate these x for corresponding f(x). (solve for the observed values)**

![Step_2](https://miro.medium.com/v2/resize:fit:786/format:webp/1*IduzsxNeH3LVFxdklmcVZQ.png)
  
- **Step 3: Based on the limited observations, the function is estimated (this assumption is called prior knowledge in Bayesian optimization), and the target value (maximum or minimum value) on the estimated f is obtained**

![Step_3](https://miro.medium.com/v2/resize:fit:786/format:webp/1*R9lZWgSX3_Fw_rWDXECFvA.png)

![Step_3](https://miro.medium.com/v2/resize:fit:786/format:webp/1*JZCYS8kB_imt9mlaHon3YQ.png)

Surrogate models in Bayesian optimization serve as a proxy for the true objective function. They capture the uncertainty in the function’s behavior and provide predictions or estimates of the function’s values at unobserved points within the search space. By iteratively updating the surrogate model based on observed function evaluations, Bayesian optimization guides the search towards regions of the search space likely to contain the optimal solution.

Gaussian processes (GPs) are commonly used as surrogate models in Bayesian optimization. A Gaussian process defines a distribution over functions, where any finite set of function values follows a multivariate Gaussian distribution. In the context of Bayesian optimization, a GP is used to model the unknown objective function, and it provides a posterior distribution over the function values given the observed data. '[More details](https://medium.com/@okanyenigun/step-by-step-guide-to-bayesian-optimization-a-python-based-approach-3558985c6818)

- **Step 4: Define a rule to determine the next observation point to be calculated**

![Step_4](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4hIYnTL76I5gTxQi-gH7g.png)

How do we choose the next point for the surrogate function?

Acquisition functions determine the next point or set of points to evaluate in the search space. It quantifies the potential utility or desirability of sampling a particular point based on the current state of the optimization process. The purpose of the acquisition function is to balance exploration and exploitation.

It takes into account both the predicted values of the surrogate model and the uncertainty associated with those predictions. It combines these two aspects to identify points that are expected to have high objective function values and (or) high uncertainty, indicating the potential for improvement. '[More details](https://medium.com/@okanyenigun/step-by-step-guide-to-bayesian-optimization-a-python-based-approach-3558985c6818)

There are several acquisition functions used in Bayesian optimization: Expected Improvement, Upper Confidence Bound, Probability of Improvement.

- **Step 5: Iterate above steps 2-4, untile the assumption distribution satisfy our requirements, or all calculation resource are used up (for example, at most m observations, or at most allowed t run-time minutes). Then output current curve's maximum or minimum**

![Procedure](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/GpParBayesAnimationSmall.gif/439px-GpParBayesAnimationSmall.gif)

### 5.1.2 Bayesian optimization for Hyperparameter Optimization

When bayesian is used for hyperparameter optimization, some points needs to be noted:
- When Bayesian optimization is not used for HPO, f(x) can be totally blackbox function (don't know f(x) expression, only know x and f(x) corresponding relations); When Bayesian optimization is used for HPO, f(x) usually is loss function and we know loss function expressions, but we do not know loss functions specific characstics.
- When used in HPO, x are hyperparameter space and are high dimensionals.
- Initial observed number of points n and final maximum observed number of points m are both bayesian optimization parameters. Maximum observed number determines bayesian optimization iteration times.
- In step 3, we use probability **surrogate model** to estimate the function distribution. Surrogate model can predict x's corresponding value fx and the confidence level for this fx value (can be mean and variance). Surrogate model can be Gaussian Processes or Gaussian Mixed Model's TPE processes
- In step 4, The rule to determine next observation point is called **Aquisition Function**. AF estimates how next osbervation will affect current curve. AF can be Probability of improvement, Expectation improvement, Upper confidence bound, Entropy.

## 5.2 Bayesian optimization implementation

Please noted: following three bayesian optimization are not supported in Python parallel or acceleration (like sklearn's n_jobs parameter), but these can be deployed in distributed calculation platforms

- Bayes_opt
```python
#pip install bayesian-optimization
#conda install -c conda-forge bayesian-optimization
```
- Hyperopt
```python
#pip install hyperopt
```
- Optuna
```python
#pip install optuna
```
- Skopt
```python
#pip install scikit-optimize
```

```python
import numpy as np
import pandas as pd
import time
import os

import sklearn
from sklearn.ensemble import RandomForestRegressor as RFR
from sklearn.model_selection import KFold, cross_validate

from bayes_opt import BayesianOptimization

import hyperopt
from hyperopt import hp, fmin, tpe, Trials, partial
from hyperopt.early_stop import no_progress_loss

import optuna

X=data.iloc[:,:-1]
y=data.iloc[:,-1]
```
### 5.2.1 bayesian-optimization based on GP (two-star recommend)

There are following three rules for bayes_opt library which affect target functions:
- The input of the objective function must be specific hyperparameters, not the entire hyperparameter space, nor elements other than data, algorithms and other hyperparameters. Therefore, when defining the objectie function, we need to let the hyperparameters serve as the input of the objective function.
- The input value of the hyperparameter can only be a floating point number, and integers and strings are not supported. Therefore, when the actual parameters of the algorithm require the input of strings, the parameters cannot be adjusted using bayes_opt. When the actual parameters of the algorithm require the input of integers, the type of the parameters needs to be specified in the objective function.
- bayes_opt only supports finding the maximum value of f(x), but does not support finding the minimum value. Therefore, when the objective function we define is some kind of loss, the output of the objective function needs to be negative. When the objective function we define is accuracy, or an indicator such as auc, we can keep the output of the objective function as it is.

- **1. Define target function**
```python
from bayes_opt import BayesianOptimization

def bayesopt_objective(n_estimators, max_depth, max_features, min_impurity_decrease):
  reg = RFR(n_estimators = int(n_estimators)
           ,max_depth = int(max_depth)
           ,max_features = int(max_features)
           ,min_impurity_decrease = min_impurity_decrease
           ,random_state = 1412
           ,verbose = False
           ,n_jobs = -1)

  cv = KFold(n_split=5， shuffle=True, random_state=1412)
  validation_loss = cross_validate(reg,X,y
                                  ,scoring="neg_root_mean_squared_error"
                                  ,cv=cv
                                  ,verbose=False
                                  ,n_jobs=-1
                                  ,error_score="raise"  #if any errors happen in cross validation, return error message
                                  )

  return np.mean(validation_loss["test_score"])
```
- **2. Define parameter space**
```python
param_grid_simple = {"n_estimators": (80,100)  # it can select any float number within closed range
                     , "max_depth": (10,25)
                     , "max_features": (10, 20)
                     , "min_impurity_decrease": (0,1)
                    }
```

- **3. Define procedures to optimize target function**
```python

def param_bayes_opt(init_points,n_iter):

  opt=BayesianOptimization(bayesopt_objective
                          ,param_grid_simple
                          ,random_state=1412   # there is a parameter called random_state but does not work
                          )

  opt.maximize(init_points = init_points
              ,n_iter = iter)

  params_best = opt.max["params"]
  score_best = opt.max["target"]

  print("\n", "best params:", params_best,
        "\n", "best cvscore:", score_best)

  return params_best, score_best
```

- **4. Define validation function (not necessary)**
The optimized results can be reproduced, that is, we can re-verify the optimal parameters given by the optimization algorithm, where the verification function is highly similar to the objective function, input the parameter or hyperparameter space, and output the final loss function result, in When using the optimization algorithm that comes with sklearn, since the optimization algorithm itself performs the steps of splitting the data and cross-validating, the optimal score obtained by the optimization algorithm is often different from the score we verified by ourselves (because the data segmentation during cross-validation is different ), however in the Bayesian optimization process, the cross-validation and data segmentation in the objective function are all specified by ourselves. **So in principle, as long as a random number seed is set in the objective function, Bayesian optimization gives The best score must be the same as the score after our verification**, so when you are familiar with the code of the optimization process, you do not need to consider secondary verification.
```python
def bayes_opt_validation(params_best):
  reg = RFR(n_estimators=int(params_best["n_estimators"])
           ,mat_depth=int(params_best["max_depth"])
           ,max_features=int(params_best["max_features"])
           ,min_impurity_decrease=params_best["min_impurity_decrease"]
           ,random_state=1412
           ,verbose=False
           ,n_jobs=-1)

  cv = KFold(n_splits=5, shuffle=True, random_state=1412)

  validation_loss=cross_validate(reg,X,y,
                                 scoring="neg_root_mean_squared_error",
                                 cv=cv,
                                 verbose=False,
                                 n_jobs=-1)

  return np.mean(validation_loss["test_score"])
```

- **5. Run optimization procedure**

```python
start = time.time()

params_best, socre_best = param_bayes_opt(20,280)
print("It takes %s minutes" % ((time.time()-start)/60))

validation_score=bayes_opt_validation(params_best)
print("validation scores:", validation_score)
```

- **6. Summary**
  - This method has superior in principles.
  - The optimization process cannot be reproduced, but the optimization results can be reproduced
  - Low efficiency. No early stopping mechanism
  - Support user customize capability

### 5.2.2 Implement TPE (Tree-structered Parzen Estimator Approach) optimization based on hyperopt (four-star recommend)

```python
import hyperopt

from hyperopt import hp, fmin, tpe, Trials, partial
from hyperopt.early_stop import no_progress_loss

```

There are following two rules for hyperopt library which affect target functions:
- The input of the target function must be a dictionary that complies with hyperopt regulations.
- Hyperopt only supports finding the minimum value of f(x), but does not support finding the maximum value.



- **1. Define target function**
```python
from bayes_opt import BayesianOptimization

def hyperopt_objective(params):
  reg = RFR(n_estimators = int(params["n_estimators"])
           ,max_depth = int(params["max_depth"])
           ,max_features = int(params["max_features"])
           ,min_impurity_decrease = int(params["min_impurity_decrease"])
           ,random_state = 1412
           ,verbose = False
           ,n_jobs = -1)

  cv = KFold(n_split=5， shuffle=True, random_state=1412)
  validation_loss = cross_validate(reg,X,y
                                  ,scoring="neg_root_mean_squared_error"
                                  ,cv=cv
                                  ,verbose=False
                                  ,n_jobs=-1
                                  ,error_score="raise"  #if any errors happen in cross validation, return error message
                                  )

  return np.mean(abs(validation_loss["test_score"]))
```
- **2. Define parameter space**
```python
param_grid_simple = {"n_estimators": hp.quniform("estimator", 80, 100, 1)
                     , "max_depth": hp.quniform("max_depth", 10,25,1)
                     , "max_features": hp.quniform("max_features", 10,20,1)
                     , "min_impurity_decrease": hp.quniform("min_impurity_decrease",0,5,1)
                    }

```

- **3. Define procedures to optimize target function**
```python

def param_hyperopt(max_evals=100):

  trials = Trials()

  early_stop_fn = no_progress_loss(100)

  params_best = fmin(hyperopt_objective
                    ,space=param_grid_simple
                    ,algo=tpe.suggest  # select surrgate model
                    ,max_evals=max_evals
                    ,verbose=True
                    ,trials=trials
                    ,early_stop_fn=early_stop_fn)

  print("\n best params:", params_best,"\n")

  return params_best, trails
```

> [!TIP]
> [*range(5,50,5)]    #[5,10,15,20,25,30,35,40,45]


- **4. Define validation function (not necessary)**
The optimized results can be reproduced, that is, we can re-verify the optimal parameters given by the optimization algorithm, where the verification function is highly similar to the objective function, input the parameter or hyperparameter space, and output the final loss function result, in When using the optimization algorithm that comes with sklearn, since the optimization algorithm itself performs the steps of splitting the data and cross-validating, the optimal score obtained by the optimization algorithm is often different from the score we verified by ourselves (because the data segmentation during cross-validation is different ), however in the Bayesian optimization process, the cross-validation and data segmentation in the objective function are all specified by ourselves. **So in principle, as long as a random number seed is set in the objective function, Bayesian optimization gives The best score must be the same as the score after our verification**, so when you are familiar with the code of the optimization process, you do not need to consider secondary verification.
```python
def hyperopt_validation(params):
  reg = RFR(n_estimators=int(params["n_estimators"])
           ,mat_depth=int(params["max_depth"])
           ,max_features=int(params["max_features"])
           ,min_impurity_decrease=params["min_impurity_decrease"]
           ,random_state=1412
           ,verbose=False
           ,n_jobs=-1)

  cv = KFold(n_splits=5, shuffle=True, random_state=1412)

  validation_loss=cross_validate(reg,X,y,
                                 scoring="neg_root_mean_squared_error",
                                 cv=cv,
                                 verbose=False,
                                 n_jobs=-1)

  return np.mean(validation_loss["test_score"])
```

- **5. Run optimization procedure**

```python
start = time.time()

params_best, trials = param_hyperopt(30)
print("It takes %s minutes" % ((time.time()-start)/60))

validation_score=hyperopt_validation(params_best)
print("validation scores:", validation_score)
```

The above codes require high precision requirements. A little bit change may cause a lot of errors.

### 5.2.3 optuna (four-star recommend)

Optuna has the best compatibility.
- **1. Define target function and parameter space**

```python
def optuna_objective(trail):

  # define parameter space
  n_estimators = trail.suggest_int("n_estimators", 80, 100, 1)
  max_depth = trail.suggest_int("max_depth", 10,25,1)
  max_features = trail.suggest_int("max_features", 10, 20, 1)
  min_impurity_decrease = trail.suggest_int("min_impurity_decrease",0,5,1)

  # define estimators
  reg = RFR(n_estimators = n_estimators
            ,max_depth = max_depth
            ,max_features = max_features
            ,min_impurity_decrease = min_impurity_decrease
            ,random_state = 1412
            ,verbose = False
            ,n_jobs = -1)



  cv = KFold(n_splits=5, shuffle=True, random_state=1412)
  validation_loss = cross_validate(reg, X, y
                                ,scoring="neg_root_mean_squared_error"
                                ,cv=cv
                                ,verbose=False
                                ,n_jobs=-1
                                ,error_score='raise')
  return np.mean(abs(validation_loss["test_score"]))
```
- **2. Define procedures to optimize target function**
  
```python

def optimizer_optuna(n_trials, algo):

  if algo=="TPE":
    algo = optuna.samplers.TPESampler(n_startup_trials = 10, n_ei_candidates=24)
  elif algo == "GP":
    from optuna.integration import skoptSampler
    import skopt
    algo = skoptSampler(skopt_kwargs={"base_estimator":"GP",
                                      "n_initial_points":10,
                                      "acq_func":"EI"})

  study = optuna.create_study(sampler = algo
                            , direction="minimize")

  study.optimize(optuna_objective
                ,n_trials=n_trials
                ,show_progress_bar=True)

  print("\n", "best params:", study.best_trial.params,
        "\n", "best score:", study.best_trial.values,
        "\n")

  return study.best_trial.params, study.best_trial.values

```
- **3. Run optimization procedure**
```python
import warnings
warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')

best_params, best_score=optimizer_optuna(10, "GP")

optuna.logging.set_verbosity(optuna.logging.ERROR)
best_parame, best_score=optimizer_optuna(300, "TPE")
best_parame, best_score=optimizer_optuna(300, "GP")
```
</details>























